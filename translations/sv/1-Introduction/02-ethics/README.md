<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58860ce9a4b8a564003d2752f7c72851",
  "translation_date": "2025-10-03T16:36:54+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "sv"
}
-->
# Introduktion till Dataetik

|![ Sketchnote av [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| Dataetik inom Data Science - _Sketchnote av [@nitya](https://twitter.com/nitya)_ |

---

Vi √§r alla datamedborgare som lever i en datadriven v√§rld.

Marknadstrender visar att √•r 2022 kommer 1 av 3 stora organisationer att k√∂pa och s√§lja sin data via online [marknadsplatser och b√∂rser](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/). Som **apputvecklare** kommer vi att uppt√§cka att det blir enklare och billigare att integrera datadrivna insikter och algoritmstyrd automation i dagliga anv√§ndarupplevelser. Men n√§r AI blir alltmer utbrett m√•ste vi ocks√• f√∂rst√• de potentiella skador som kan orsakas av [vapenisering](https://www.youtube.com/watch?v=TQHs8SA1qpk) av s√•dana algoritmer i stor skala.

Trender pekar p√• att vi √•r 2025 kommer att generera och konsumera √∂ver [180 zettabyte](https://www.statista.com/statistics/871513/worldwide-data-created/) data. F√∂r **dataforskare** inneb√§r denna explosion av information en o√∂vertr√§ffad tillg√•ng till personlig och beteendem√§ssig data. Med detta kommer m√∂jligheten att bygga detaljerade anv√§ndarprofiler och subtilt p√•verka beslutsfattande‚Äîofta p√• s√§tt som skapar en [illusion av fri vilja](https://www.datasciencecentral.com/the-pareto-set-and-the-paradox-of-choice/). √Ñven om detta kan anv√§ndas f√∂r att styra anv√§ndare mot √∂nskade resultat, v√§cker det ocks√• viktiga fr√•gor om datasekretess, autonomi och de etiska gr√§nserna f√∂r algoritmisk p√•verkan.

Dataetik √§r nu _n√∂dv√§ndiga skyddsr√§cken_ f√∂r dataforskning och ingenj√∂rskonst, som hj√§lper oss att minimera potentiella skador och oavsiktliga konsekvenser av v√•ra datadrivna handlingar. [Gartners Hype Cycle f√∂r AI](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifierar relevanta trender inom digital etik, ansvarsfull AI och AI-styrning som nyckeldrivkrafter f√∂r st√∂rre megatrender kring _demokratisering_ och _industrialisering_ av AI.

![Gartners Hype Cycle f√∂r AI - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

I denna lektion kommer vi att utforska det fascinerande omr√•det dataetik - fr√•n grundl√§ggande begrepp och utmaningar till fallstudier och till√§mpade AI-koncept som styrning - som hj√§lper till att etablera en etisk kultur i team och organisationer som arbetar med data och AI.




## [Quiz f√∂re f√∂rel√§sningen](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Grundl√§ggande definitioner

L√•t oss b√∂rja med att f√∂rst√• den grundl√§ggande terminologin.

Ordet "etik" kommer fr√•n det [grekiska ordet "ethikos"](https://en.wikipedia.org/wiki/Ethics) (och dess rot "ethos") som betyder _karakt√§r eller moralisk natur_. 

**Etik** handlar om de gemensamma v√§rderingar och moraliska principer som styr v√•rt beteende i samh√§llet. Etik baseras inte p√• lagar utan p√• allm√§nt accepterade normer f√∂r vad som √§r "r√§tt kontra fel". Etiska √∂verv√§ganden kan dock p√•verka f√∂retagsstyrningsinitiativ och statliga regleringar som skapar fler incitament f√∂r efterlevnad.

**Dataetik** √§r en [ny gren av etiken](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) som "studerar och utv√§rderar moraliska problem relaterade till _data, algoritmer och motsvarande praxis_". H√§r fokuserar **"data"** p√• handlingar relaterade till generering, registrering, kuratering, bearbetning, spridning, delning och anv√§ndning, **"algoritmer"** fokuserar p√• AI, agenter, maskininl√§rning och robotar, och **"praxis"** fokuserar p√• √§mnen som ansvarsfull innovation, programmering, hacking och etiska koder.

**Till√§mpad etik** √§r den [praktiska till√§mpningen av moraliska √∂verv√§ganden](https://en.wikipedia.org/wiki/Applied_ethics). Det √§r processen att aktivt unders√∂ka etiska fr√•gor i samband med _verkliga handlingar, produkter och processer_, och vidta korrigerande √•tg√§rder f√∂r att s√§kerst√§lla att dessa f√∂rblir i linje med v√•ra definierade etiska v√§rderingar.

**Etisk kultur** handlar om [_operationalisering_ av till√§mpad etik](https://hbr.org/2019/05/how-to-design-an-ethical-organization) f√∂r att s√§kerst√§lla att v√•ra etiska principer och praxis antas p√• ett konsekvent och skalbart s√§tt i hela organisationen. Framg√•ngsrika etiska kulturer definierar organisationens etiska principer, erbjuder meningsfulla incitament f√∂r efterlevnad och f√∂rst√§rker etiska normer genom att uppmuntra och f√∂rst√§rka √∂nskade beteenden p√• alla niv√•er i organisationen.


## Etiska begrepp

I denna sektion kommer vi att diskutera begrepp som **gemensamma v√§rderingar** (principer) och **etiska utmaningar** (problem) f√∂r dataetik - och utforska **fallstudier** som hj√§lper dig att f√∂rst√• dessa begrepp i verkliga sammanhang.

### 1. Etiska principer

Varje strategi f√∂r dataetik b√∂rjar med att definiera _etiska principer_ - de "gemensamma v√§rderingar" som beskriver acceptabla beteenden och v√§gleder efterlevnad i v√•ra data- och AI-projekt. Du kan definiera dessa p√• individuell eller teamniv√•. Men de flesta stora organisationer beskriver dessa i ett _etiskt AI_-uppdrag eller ramverk som definieras p√• f√∂retagsniv√• och till√§mpas konsekvent √∂ver alla team.

**Exempel:** Microsofts [Ansvarsfull AI](https://www.microsoft.com/en-us/ai/responsible-ai) uppdrag lyder: _"Vi √§r engagerade i att fr√§mja AI som drivs av etiska principer som s√§tter m√§nniskor f√∂rst"_ - och identifierar 6 etiska principer i ramverket nedan:

![Ansvarsfull AI hos Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

L√•t oss kort utforska dessa principer. _Transparens_ och _ansvarsskyldighet_ √§r grundl√§ggande v√§rderingar som andra principer bygger p√• - s√• l√•t oss b√∂rja d√§r:

* [**Ansvarsskyldighet**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) g√∂r ut√∂vare _ansvariga_ f√∂r sina data- och AI-operationer och efterlevnad av dessa etiska principer.
* [**Transparens**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) s√§kerst√§ller att data- och AI-√•tg√§rder √§r _f√∂rst√•eliga_ (tolkbara) f√∂r anv√§ndare, och f√∂rklarar vad och varf√∂r bakom besluten.
* [**R√§ttvisa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - fokuserar p√• att s√§kerst√§lla att AI behandlar _alla m√§nniskor_ r√§ttvist, och adresserar eventuella systemiska eller implicita socio-tekniska f√∂rdomar i data och system.
* [**Tillf√∂rlitlighet och s√§kerhet**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - s√§kerst√§ller att AI beter sig _konsekvent_ med definierade v√§rderingar, och minimerar potentiella skador eller oavsiktliga konsekvenser.
* [**Sekretess och s√§kerhet**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - handlar om att f√∂rst√• dataursprung och ge _datasekretess och relaterade skydd_ till anv√§ndare.
* [**Inkludering**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - handlar om att designa AI-l√∂sningar med avsikt, och anpassa dem f√∂r att m√∂ta ett _brett spektrum av m√§nskliga behov_ och f√∂rm√•gor.

> üö® Fundera p√• vad ditt uppdrag f√∂r dataetik skulle kunna vara. Utforska etiska AI-ramverk fr√•n andra organisationer - h√§r √§r exempel fr√•n [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles), och [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Vilka gemensamma v√§rderingar har de? Hur relaterar dessa principer till AI-produkten eller industrin de verkar inom?

### 2. Etiska utmaningar

N√§r vi har definierat etiska principer √§r n√§sta steg att utv√§rdera v√•ra data- och AI-√•tg√§rder f√∂r att se om de √§r i linje med dessa gemensamma v√§rderingar. T√§nk p√• dina √•tg√§rder i tv√• kategorier: _datainsamling_ och _algoritmdesign_. 

Vid datainsamling kommer √•tg√§rder sannolikt att involvera **personlig data** eller personligt identifierbar information (PII) f√∂r identifierbara levande individer. Detta inkluderar [olika typer av icke-personlig data](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) som _tillsammans_ identifierar en individ. Etiska utmaningar kan relatera till _datasekretess_, _data√§gande_ och relaterade √§mnen som _informerat samtycke_ och _immateriella r√§ttigheter_ f√∂r anv√§ndare.

Vid algoritmdesign kommer √•tg√§rder att involvera insamling och kuratering av **datam√§ngder**, och sedan anv√§nda dem f√∂r att tr√§na och implementera **datamodeller** som f√∂rutsp√•r resultat eller automatiserar beslut i verkliga sammanhang. Etiska utmaningar kan uppst√• fr√•n _datam√§ngdsf√∂rdomar_, _datakvalitetsproblem_, _or√§ttvisa_ och _missrepresentation_ i algoritmer - inklusive vissa problem som √§r systemiska till sin natur.

I b√•da fallen belyser etiska utmaningar omr√•den d√§r v√•ra √•tg√§rder kan komma i konflikt med v√•ra gemensamma v√§rderingar. F√∂r att uppt√§cka, mildra, minimera eller eliminera dessa bekymmer m√•ste vi st√§lla moraliska "ja/nej"-fr√•gor relaterade till v√•ra √•tg√§rder och vidta korrigerande √•tg√§rder vid behov. L√•t oss titta p√• n√•gra etiska utmaningar och de moraliska fr√•gor de v√§cker:


#### 2.1 Data√§gande

Datainsamling involverar ofta personlig data som kan identifiera datasubjekten. [Data√§gande](https://permission.io/blog/data-ownership) handlar om _kontroll_ och [_anv√§ndarr√§ttigheter_](https://permission.io/blog/data-ownership) relaterade till skapande, bearbetning och spridning av data. 

De moraliska fr√•gor vi beh√∂ver st√§lla √§r: 
 * Vem √§ger datan? (anv√§ndare eller organisation)
 * Vilka r√§ttigheter har datasubjekten? (ex: √•tkomst, radering, portabilitet)
 * Vilka r√§ttigheter har organisationer? (ex: r√§tta illvilliga anv√§ndarrecensioner)

#### 2.2 Informerat samtycke

[Informerat samtycke](https://legaldictionary.net/informed-consent/) definierar handlingen d√§r anv√§ndare samtycker till en √•tg√§rd (som datainsamling) med en _full f√∂rst√•else_ av relevanta fakta inklusive syfte, potentiella risker och alternativ. 

Fr√•gor att utforska h√§r √§r:
 * Gav anv√§ndaren (datasubjektet) tillst√•nd f√∂r datainsamling och anv√§ndning?
 * F√∂rstod anv√§ndaren syftet med att datan samlades in?
 * F√∂rstod anv√§ndaren de potentiella riskerna med sitt deltagande?

#### 2.3 Immateriella r√§ttigheter

[Immateriella r√§ttigheter](https://en.wikipedia.org/wiki/Intellectual_property) avser immateriella skapelser som h√§rr√∂r fr√•n m√§nskligt initiativ, som kan _ha ekonomiskt v√§rde_ f√∂r individer eller f√∂retag. 

Fr√•gor att utforska h√§r √§r:
 * Hade den insamlade datan ekonomiskt v√§rde f√∂r en anv√§ndare eller ett f√∂retag?
 * Har **anv√§ndaren** immateriella r√§ttigheter h√§r?
 * Har **organisationen** immateriella r√§ttigheter h√§r?
 * Om dessa r√§ttigheter finns, hur skyddar vi dem?

#### 2.4 Datasekretess

[Datasekretess](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) eller informationssekretess avser bevarandet av anv√§ndarsekretess och skydd av anv√§ndaridentitet med avseende p√• personligt identifierbar information. 

Fr√•gor att utforska h√§r √§r:
 * √Ñr anv√§ndarnas (personliga) data s√§krad mot hack och l√§ckor?
 * √Ñr anv√§ndarnas data endast tillg√§nglig f√∂r auktoriserade anv√§ndare och sammanhang?
 * Bevaras anv√§ndarnas anonymitet n√§r data delas eller sprids?
 * Kan en anv√§ndare avidentifieras fr√•n anonymiserade datam√§ngder?


#### 2.5 R√§tten att bli gl√∂md

[R√§tten att bli gl√∂md](https://en.wikipedia.org/wiki/Right_to_be_forgotten) eller [R√§tten till radering](https://www.gdpreu.org/right-to-be-forgotten/) ger ytterligare skydd f√∂r personlig data till anv√§ndare. Specifikt ger det anv√§ndare r√§tt att beg√§ra radering eller borttagning av personlig data fr√•n internets√∂kningar och andra platser, _under specifika omst√§ndigheter_ - vilket ger dem en ny start online utan att tidigare handlingar h√•lls emot dem.

Fr√•gor att utforska h√§r √§r:
 * Till√•ter systemet datasubjekt att beg√§ra radering?
 * B√∂r √•terkallande av anv√§ndarsamtycke utl√∂sa automatisk radering?
 * Samlades data in utan samtycke eller p√• olagliga s√§tt?
 * √Ñr vi i √∂verensst√§mmelse med statliga regleringar f√∂r datasekretess?


#### 2.6 Datam√§ngdsf√∂rdomar

Datam√§ngds- eller [insamlingf√∂rdomar](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) handlar om att v√§lja en _icke-representativ_ delm√§ngd av data f√∂r algoritmutveckling, vilket skapar potentiell or√§ttvisa i resultat f√∂r olika grupper. Typer av f√∂rdomar inkluderar urvals- eller samplingsf√∂rdomar, frivilligf√∂rdomar och instrumentf√∂rdomar. 

Fr√•gor att utforska h√§r √§r:
 * Rekryterade vi en representativ upps√§ttning datasubjekt?
 * Testade vi v√•r insamlade eller kuraterade datam√§ngd f√∂r olika f√∂rdomar?
 * Kan vi mildra eller ta bort uppt√§ckta f√∂rdomar?

#### 2.7 Datakvalitet

[Datakvalitet](https://lakefs.io/data-quality-testing/) handlar om giltigheten av den kuraterade datam√§ngden som anv√§nds f√∂r att utveckla v√•ra algoritmer, och kontrollerar om funktioner och poster uppfyller kraven f√∂r den niv√• av noggrannhet och konsekvens som beh√∂vs f√∂r v√•rt AI-syfte.

Fr√•gor att utforska h√§r √§r:
 * F√•ngade vi giltiga _funktioner_ f√∂r v√•rt anv√§ndningsfall?
 * Samlades data in _konsekvent_ √∂ver olika datak√§llor?
 * √Ñr datam√§ngden _komplett_ f√∂r olika f√∂rh√•llanden eller scenarier?
* F√•ngas information _korrekt_ och speglar verkligheten?

#### 2.8 Algoritmisk r√§ttvisa

[Algoritmisk r√§ttvisa](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) handlar om att unders√∂ka om algoritmdesignen systematiskt diskriminerar specifika undergrupper av datamottagare, vilket kan leda till [potentiella skador](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) inom _resursf√∂rdelning_ (d√§r resurser nekas eller undanh√•lls fr√•n en grupp) och _servicekvalitet_ (d√§r AI √§r mindre exakt f√∂r vissa undergrupper √§n f√∂r andra).

Fr√•gor att utforska h√§r √§r:
* Har vi utv√§rderat modellens noggrannhet f√∂r olika undergrupper och f√∂rh√•llanden?
* Har vi granskat systemet f√∂r potentiella skador (t.ex. stereotyper)?
* Kan vi revidera data eller tr√§na om modeller f√∂r att minska identifierade skador?

Utforska resurser som [AI Fairness checklists](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) f√∂r att l√§ra dig mer.

#### 2.9 Missvisande representation

[Missvisande datarepresentation](https://www.sciencedirect.com/topics/computer-science/misrepresentation) handlar om att fr√•ga om vi kommunicerar insikter fr√•n √§rligt rapporterad data p√• ett vilseledande s√§tt f√∂r att st√∂dja en √∂nskad ber√§ttelse.

Fr√•gor att utforska h√§r √§r:
* Rapporterar vi ofullst√§ndig eller felaktig data?
* Visualiserar vi data p√• ett s√§tt som leder till vilseledande slutsatser?
* Anv√§nder vi selektiva statistiska tekniker f√∂r att manipulera resultat?
* Finns det alternativa f√∂rklaringar som kan ge en annan slutsats?

#### 2.10 Fri vilja
[Illusionen av fri vilja](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) uppst√•r n√§r systemets "valarkitekturer" anv√§nder beslutsalgoritmer f√∂r att p√•verka m√§nniskor att ta ett f√∂redraget beslut samtidigt som de verkar ge dem alternativ och kontroll. Dessa [m√∂rka m√∂nster](https://www.darkpatterns.org/) kan orsaka social och ekonomisk skada f√∂r anv√§ndare. Eftersom anv√§ndarens beslut p√•verkar beteendeprofiler kan dessa handlingar potentiellt driva framtida val som f√∂rst√§rker eller f√∂rl√§nger effekten av dessa skador.

Fr√•gor att utforska h√§r √§r:
* F√∂rstod anv√§ndaren konsekvenserna av att g√∂ra det valet?
* Var anv√§ndaren medveten om (alternativa) val och f√∂r- och nackdelar med varje?
* Kan anv√§ndaren senare √§ndra ett automatiserat eller p√•verkat val?

### 3. Fallstudier

F√∂r att s√§tta dessa etiska utmaningar i verkliga sammanhang √§r det hj√§lpsamt att titta p√• fallstudier som belyser potentiella skador och konsekvenser f√∂r individer och samh√§llet n√§r s√•dana etiska √∂vertr√§delser f√∂rbises.

H√§r √§r n√•gra exempel:

| Etisk utmaning | Fallstudie | 
|--- |--- |
| **Informerat samtycke** | 1972 - [Tuskegee Syfilisstudien](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Afroamerikanska m√§n som deltog i studien lovades gratis medicinsk v√•rd _men blev lurade_ av forskare som inte informerade deltagarna om deras diagnos eller om tillg√§nglig behandling. M√•nga deltagare dog och deras partners eller barn p√•verkades; studien p√•gick i 40 √•r. | 
| **Datasekretess** | 2007 - [Netflix datapriser](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) gav forskare _10M anonymiserade filmrankningar fr√•n 50K kunder_ f√∂r att f√∂rb√§ttra rekommendationsalgoritmer. Forskare kunde dock korrelera anonymiserad data med personligt identifierbar data i _externa dataset_ (t.ex. IMDb-kommentarer) - vilket effektivt "de-anonymiserade" vissa Netflix-abonnenter. |
| **Insamlingsbias** | 2013 - Staden Boston [utvecklade Street Bump](https://www.boston.gov/transportation/street-bump), en app som l√§t medborgare rapportera potth√•l, vilket gav staden b√§ttre v√§gdata f√∂r att hitta och √•tg√§rda problem. Men [personer i l√•ginkomstgrupper hade mindre tillg√•ng till bilar och telefoner](https://hbr.org/2013/04/the-hidden-biases-in-big-data), vilket gjorde deras v√§gproblem osynliga i denna app. Utvecklare samarbetade med akademiker f√∂r att hantera _r√§ttvis tillg√•ng och digitala klyftor_. |
| **Algoritmisk r√§ttvisa** | 2018 - MIT:s [Gender Shades Study](http://gendershades.org/overview.html) utv√§rderade noggrannheten hos AI-produkter f√∂r k√∂nsklassificering och avsl√∂jade brister i noggrannhet f√∂r kvinnor och personer med f√§rgad hud. Ett [2019 Apple Card](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) verkade erbjuda mindre kredit till kvinnor √§n m√§n. B√•da illustrerade problem med algoritmisk bias som leder till socioekonomiska skador. |
| **Missvisande representation** | 2020 - [Georgia Department of Public Health sl√§ppte COVID-19-diagram](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) som verkade vilseleda medborgare om trender i bekr√§ftade fall med icke-kronologisk ordning p√• x-axeln. Detta illustrerar missvisande representation genom visualiseringstrick. |
| **Illusionen av fri vilja** | 2020 - L√§roappen [ABCmouse betalade $10M f√∂r att l√∂sa en FTC-klagan](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) d√§r f√∂r√§ldrar fastnade i att betala f√∂r abonnemang de inte kunde avbryta. Detta illustrerar m√∂rka m√∂nster i valarkitekturer, d√§r anv√§ndare p√•verkades att g√∂ra potentiellt skadliga val. |
| **Datasekretess & anv√§ndarr√§ttigheter** | 2021 - Facebook [Data Breach](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) exponerade data fr√•n 530M anv√§ndare, vilket resulterade i en $5B-uppg√∂relse med FTC. Det v√§grade dock att informera anv√§ndare om dataintr√•nget, vilket br√∂t mot anv√§ndarr√§ttigheter kring datatransparens och √•tkomst. |

Vill du utforska fler fallstudier? Kolla in dessa resurser:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - etiska dilemman inom olika branscher.
* [Data Science Ethics course](https://www.coursera.org/learn/data-science-ethics#syllabus) - banbrytande fallstudier utforskade.
* [Where things have gone wrong](https://deon.drivendata.org/examples/) - deon-checklista med exempel.

> üö® T√§nk p√• de fallstudier du har sett - har du upplevt eller blivit p√•verkad av en liknande etisk utmaning i ditt liv? Kan du komma p√• minst en annan fallstudie som illustrerar en av de etiska utmaningar vi har diskuterat i detta avsnitt?

## Till√§mpad etik

Vi har pratat om etiska koncept, utmaningar och fallstudier i verkliga sammanhang. Men hur b√∂rjar vi _till√§mpa_ etiska principer och praxis i v√•ra projekt? Och hur _operationaliserar_ vi dessa praxis f√∂r b√§ttre styrning? L√•t oss utforska n√•gra verkliga l√∂sningar:

### 1. Professionella koder

Professionella koder erbjuder ett alternativ f√∂r organisationer att "motivera" medlemmar att st√∂dja deras etiska principer och mission statement. Koder √§r _moraliska riktlinjer_ f√∂r professionellt beteende, som hj√§lper anst√§llda eller medlemmar att fatta beslut som √∂verensst√§mmer med organisationens principer. De √§r bara s√• bra som den frivilliga efterlevnaden fr√•n medlemmarna; dock erbjuder m√•nga organisationer ytterligare bel√∂ningar och straff f√∂r att motivera efterlevnad.

Exempel inkluderar:

* [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) Code of Ethics
* [Data Science Association](http://datascienceassn.org/code-of-conduct.html) Code of Conduct (skapad 2013)
* [ACM Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics) (sedan 1993)

> üö® Tillh√∂r du en professionell ingenj√∂rs- eller datavetenskapsorganisation? Utforska deras webbplats f√∂r att se om de definierar en professionell etisk kod. Vad s√§ger detta om deras etiska principer? Hur "motiverar" de medlemmar att f√∂lja koden?

### 2. Etiska checklistor

Medan professionella koder definierar n√∂dv√§ndigt _etiskt beteende_ fr√•n praktiker, har de [k√§nda begr√§nsningar](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) i efterlevnad, s√§rskilt i storskaliga projekt. Ist√§llet f√∂respr√•kar m√•nga datavetenskapsexperter [checklistor](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) som kan **koppla principer till praxis** p√• mer deterministiska och handlingsbara s√§tt.

Checklistor omvandlar fr√•gor till "ja/nej"-uppgifter som kan operationaliseras, vilket g√∂r att de kan sp√•ras som en del av standardarbetsfl√∂den f√∂r produktlansering.

Exempel inkluderar:
* [Deon](https://deon.drivendata.org/) - en allm√§n datetikchecklista skapad fr√•n [branschrekommendationer](https://deon.drivendata.org/#checklist-citations) med ett kommandoradsverktyg f√∂r enkel integration.
* [Privacy Audit Checklist](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - ger allm√§n v√§gledning f√∂r informationshanteringspraxis ur juridiska och sociala exponeringsperspektiv.
* [AI Fairness Checklist](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - skapad av AI-praktiker f√∂r att st√∂dja adoption och integration av r√§ttvisekontroller i AI-utvecklingscykler.
* [22 fr√•gor f√∂r etik inom data och AI](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - en mer √∂ppen ram, strukturerad f√∂r initial utforskning av etiska fr√•gor i design-, implementerings- och organisationssammanhang.

### 3. Etiska regleringar

Etik handlar om att definiera gemensamma v√§rderingar och g√∂ra r√§tt saker _frivilligt_. **Efterlevnad** handlar om _att f√∂lja lagen_ d√§r den √§r definierad. **Styrning** omfattar brett alla s√§tt p√• vilka organisationer arbetar f√∂r att uppr√§tth√•lla etiska principer och f√∂lja etablerade lagar.

Idag tar styrning tv√• former inom organisationer. F√∂r det f√∂rsta handlar det om att definiera **etiska AI**-principer och etablera praxis f√∂r att operationalisera adoption √∂ver alla AI-relaterade projekt i organisationen. F√∂r det andra handlar det om att f√∂lja alla statligt f√∂reskrivna **dataskyddsregleringar** f√∂r regioner d√§r organisationen verkar.

Exempel p√• dataskydds- och sekretessregleringar:

* `1974`, [US Privacy Act](https://www.justice.gov/opcl/privacy-act-1974) - reglerar _federala myndigheters_ insamling, anv√§ndning och avsl√∂jande av personlig information.
* `1996`, [US Health Insurance Portability & Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - skyddar personlig h√§lsodata.
* `1998`, [US Children's Online Privacy Protection Act (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - skyddar datasekretess f√∂r barn under 13 √•r.
* `2018`, [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) - ger anv√§ndarr√§ttigheter, dataskydd och sekretess.
* `2018`, [California Consumer Privacy Act (CCPA)](https://www.oag.ca.gov/privacy/ccpa) ger konsumenter fler _r√§ttigheter_ √∂ver deras (personliga) data.
* `2021`, Kinas [Personal Information Protection Law](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) antogs nyligen och skapar en av de starkaste regleringarna f√∂r datasekretess online i v√§rlden.

> üö® Europeiska unionens GDPR (General Data Protection Regulation) √§r fortfarande en av de mest inflytelserika datasekretessregleringarna idag. Visste du att den ocks√• definierar [8 anv√§ndarr√§ttigheter](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) f√∂r att skydda medborgarnas digitala sekretess och personliga data? L√§r dig vad dessa √§r och varf√∂r de √§r viktiga.

### 4. Etisk kultur

Observera att det fortfarande finns en immateriell klyfta mellan _efterlevnad_ (att g√∂ra tillr√§ckligt f√∂r att uppfylla "lagens bokstav") och att hantera [systemiska problem](https://www.coursera.org/learn/data-science-ethics/home/week/4) (som stelhet, informationsasymmetri och f√∂rdelningsor√§ttvisa) som kan p√•skynda AI:s vapenisering.

Det senare kr√§ver [samarbetsmetoder f√∂r att definiera etiska kulturer](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) som bygger k√§nslom√§ssiga kopplingar och konsekventa gemensamma v√§rderingar _√∂ver organisationer_ inom branschen. Detta kr√§ver mer [formaliserade datetikskulturer](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) i organisationer - vilket g√∂r det m√∂jligt f√∂r _vem som helst_ att [dra Andon-sn√∂ret](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (f√∂r att lyfta etiska fr√•gor tidigt i processen) och g√∂ra _etiska bed√∂mningar_ (t.ex. vid rekrytering) till ett k√§rnkriterium f√∂r teamformation i AI-projekt.

---
## [Efterf√∂rel√§sningsquiz](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## Granskning & Sj√§lvstudier

Kurser och b√∂cker hj√§lper till att f√∂rst√• grundl√§ggande etiska koncept och utmaningar, medan fallstudier och verktyg hj√§lper till med till√§mpad etik i verkliga sammanhang. H√§r √§r n√•gra resurser att b√∂rja med.
* [Maskininl√§rning f√∂r nyb√∂rjare](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - lektion om r√§ttvisa, fr√•n Microsoft.  
* [Principer f√∂r ansvarsfull AI](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - gratis utbildningsv√§g fr√•n Microsoft Learn.  
* [Etik och datavetenskap](https://resources.oreilly.com/examples/0636920203964) - O'Reilly EBook (M. Loukides, H. Mason m.fl.)  
* [Datavetenskapens etik](https://www.coursera.org/learn/data-science-ethics#syllabus) - onlinekurs fr√•n University of Michigan.  
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - fallstudier fr√•n University of Texas.  

# Uppgift  

[Skriv en fallstudie om dataetik](assignment.md)  

---

**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• dess originalspr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.