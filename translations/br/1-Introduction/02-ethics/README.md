<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1341f6da63d434f5ba31b08ea951b02c",
  "translation_date": "2025-09-06T08:37:30+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "br"
}
-->
# Introdu√ß√£o √† √âtica de Dados

|![ Sketchnote por [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| √âtica em Ci√™ncia de Dados - _Sketchnote por [@nitya](https://twitter.com/nitya)_ |

---

Somos todos cidad√£os de dados vivendo em um mundo dataficado.

Tend√™ncias de mercado indicam que, at√© 2022, 1 em cada 3 grandes organiza√ß√µes comprar√° e vender√° seus dados por meio de [Mercados e Exchanges](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) online. Como **Desenvolvedores de Aplicativos**, ser√° mais f√°cil e barato integrar insights baseados em dados e automa√ß√£o orientada por algoritmos nas experi√™ncias di√°rias dos usu√°rios. Mas, √† medida que a IA se torna onipresente, tamb√©m precisaremos entender os potenciais danos causados pela [armazeniza√ß√£o](https://www.youtube.com/watch?v=TQHs8SA1qpk) desses algoritmos em larga escala.

As tend√™ncias tamb√©m indicam que criaremos e consumiremos mais de [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) de dados at√© 2025. Como **Cientistas de Dados**, isso nos d√° n√≠veis sem precedentes de acesso a dados pessoais. Isso significa que podemos construir perfis comportamentais de usu√°rios e influenciar a tomada de decis√µes de maneiras que criam uma [ilus√£o de escolha livre](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice), enquanto potencialmente direcionamos os usu√°rios para resultados que preferimos. Isso tamb√©m levanta quest√µes mais amplas sobre privacidade de dados e prote√ß√£o dos usu√°rios.

A √©tica de dados agora √© um _guia necess√°rio_ para ci√™ncia e engenharia de dados, ajudando-nos a minimizar danos potenciais e consequ√™ncias n√£o intencionais de nossas a√ß√µes baseadas em dados. O [Ciclo de Hype da Gartner para IA](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifica tend√™ncias relevantes em √©tica digital, IA respons√°vel e governan√ßa de IA como fatores-chave para megatend√™ncias maiores em torno da _democratiza√ß√£o_ e _industrializa√ß√£o_ da IA.

![Ciclo de Hype da Gartner para IA - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

Nesta li√ß√£o, exploraremos a fascinante √°rea da √©tica de dados - desde conceitos e desafios fundamentais at√© estudos de caso e conceitos aplicados de IA, como governan√ßa - que ajudam a estabelecer uma cultura √©tica em equipes e organiza√ß√µes que trabalham com dados e IA.

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Defini√ß√µes B√°sicas

Vamos come√ßar entendendo a terminologia b√°sica.

A palavra "√©tica" vem da [palavra grega "ethikos"](https://en.wikipedia.org/wiki/Ethics) (e sua raiz "ethos"), que significa _car√°ter ou natureza moral_.

**√âtica** trata dos valores compartilhados e princ√≠pios morais que governam nosso comportamento na sociedade. A √©tica n√£o se baseia em leis, mas em normas amplamente aceitas do que √© "certo versus errado". No entanto, considera√ß√µes √©ticas podem influenciar iniciativas de governan√ßa corporativa e regulamenta√ß√µes governamentais que criam mais incentivos para conformidade.

**√âtica de Dados** √© um [novo ramo da √©tica](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) que "estuda e avalia problemas morais relacionados a _dados, algoritmos e pr√°ticas correspondentes_". Aqui, **"dados"** se concentra em a√ß√µes relacionadas √† gera√ß√£o, registro, curadoria, processamento, dissemina√ß√£o, compartilhamento e uso; **"algoritmos"** se concentra em IA, agentes, aprendizado de m√°quina e rob√¥s; e **"pr√°ticas"** se concentra em t√≥picos como inova√ß√£o respons√°vel, programa√ß√£o, hacking e c√≥digos de √©tica.

**√âtica Aplicada** √© a [aplica√ß√£o pr√°tica de considera√ß√µes morais](https://en.wikipedia.org/wiki/Applied_ethics). √â o processo de investigar ativamente quest√µes √©ticas no contexto de _a√ß√µes, produtos e processos do mundo real_ e tomar medidas corretivas para garantir que permane√ßam alinhados com nossos valores √©ticos definidos.

**Cultura √âtica** trata de [_operacionalizar_ a √©tica aplicada](https://hbr.org/2019/05/how-to-design-an-ethical-organization) para garantir que nossos princ√≠pios e pr√°ticas √©ticas sejam adotados de maneira consistente e escal√°vel em toda a organiza√ß√£o. Culturas √©ticas bem-sucedidas definem princ√≠pios √©ticos em toda a organiza√ß√£o, fornecem incentivos significativos para conformidade e refor√ßam normas √©ticas, incentivando e amplificando comportamentos desejados em todos os n√≠veis da organiza√ß√£o.

## Conceitos de √âtica

Nesta se√ß√£o, discutiremos conceitos como **valores compartilhados** (princ√≠pios) e **desafios √©ticos** (problemas) para √©tica de dados - e exploraremos **estudos de caso** que ajudam a entender esses conceitos em contextos do mundo real.

### 1. Princ√≠pios √âticos

Toda estrat√©gia de √©tica de dados come√ßa definindo _princ√≠pios √©ticos_ - os "valores compartilhados" que descrevem comportamentos aceit√°veis e orientam a√ß√µes em conformidade em nossos projetos de dados e IA. Voc√™ pode defini-los em n√≠vel individual ou de equipe. No entanto, a maioria das grandes organiza√ß√µes os descreve em uma declara√ß√£o de miss√£o ou estrutura de _IA √©tica_ definida em n√≠veis corporativos e aplicada de forma consistente em todas as equipes.

**Exemplo:** A declara√ß√£o de miss√£o de [IA Respons√°vel](https://www.microsoft.com/en-us/ai/responsible-ai) da Microsoft diz: _"Estamos comprometidos com o avan√ßo da IA orientada por princ√≠pios √©ticos que colocam as pessoas em primeiro lugar"_ - identificando 6 princ√≠pios √©ticos na estrutura abaixo:

![IA Respons√°vel na Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Vamos explorar brevemente esses princ√≠pios. _Transpar√™ncia_ e _responsabilidade_ s√£o valores fundamentais sobre os quais outros princ√≠pios s√£o constru√≠dos - ent√£o vamos come√ßar por a√≠:

* [**Responsabilidade**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) torna os profissionais _respons√°veis_ por suas opera√ß√µes de dados e IA e pela conformidade com esses princ√≠pios √©ticos.
* [**Transpar√™ncia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) garante que as a√ß√µes de dados e IA sejam _compreens√≠veis_ (interpret√°veis) para os usu√°rios, explicando o que e por que por tr√°s das decis√µes.
* [**Justi√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - foca em garantir que a IA trate _todas as pessoas_ de forma justa, abordando quaisquer preconceitos sociot√©cnicos sist√™micos ou impl√≠citos em dados e sistemas.
* [**Confiabilidade e Seguran√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - garante que a IA se comporte _consistentemente_ com os valores definidos, minimizando danos potenciais ou consequ√™ncias n√£o intencionais.
* [**Privacidade e Seguran√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de entender a linhagem dos dados e fornecer _privacidade de dados e prote√ß√µes relacionadas_ aos usu√°rios.
* [**Inclusividade**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de projetar solu√ß√µes de IA com inten√ß√£o, adaptando-as para atender a uma _ampla gama de necessidades e capacidades humanas_.

> üö® Pense em qual poderia ser sua declara√ß√£o de miss√£o de √©tica de dados. Explore estruturas de IA √©tica de outras organiza√ß√µes - aqui est√£o exemplos da [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) e [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Quais valores compartilhados eles t√™m em comum? Como esses princ√≠pios se relacionam com o produto ou setor de IA em que operam?

### 2. Desafios √âticos

Depois de definir os princ√≠pios √©ticos, o pr√≥ximo passo √© avaliar nossas a√ß√µes de dados e IA para ver se est√£o alinhadas com esses valores compartilhados. Pense em suas a√ß√µes em duas categorias: _coleta de dados_ e _design de algoritmos_.

Na coleta de dados, as a√ß√µes provavelmente envolver√£o **dados pessoais** ou informa√ß√µes pessoalmente identific√°veis (PII) de indiv√≠duos identific√°veis. Isso inclui [diversos itens de dados n√£o pessoais](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) que, _coletivamente_, identificam um indiv√≠duo. Os desafios √©ticos podem estar relacionados √† _privacidade de dados_, _propriedade de dados_ e t√≥picos relacionados, como _consentimento informado_ e _direitos de propriedade intelectual_ dos usu√°rios.

No design de algoritmos, as a√ß√µes envolver√£o a coleta e curadoria de **conjuntos de dados**, e o uso deles para treinar e implantar **modelos de dados** que preveem resultados ou automatizam decis√µes em contextos do mundo real. Os desafios √©ticos podem surgir de _vi√©s nos conjuntos de dados_, problemas de _qualidade dos dados_, _injusti√ßas_ e _distor√ß√µes_ nos algoritmos - incluindo algumas quest√µes que s√£o sist√™micas por natureza.

Em ambos os casos, os desafios √©ticos destacam √°reas onde nossas a√ß√µes podem entrar em conflito com nossos valores compartilhados. Para detectar, mitigar, minimizar ou eliminar essas preocupa√ß√µes, precisamos fazer perguntas morais de "sim/n√£o" relacionadas √†s nossas a√ß√µes e tomar medidas corretivas conforme necess√°rio. Vamos dar uma olhada em alguns desafios √©ticos e nas quest√µes morais que eles levantam:

#### 2.1 Propriedade de Dados

A coleta de dados muitas vezes envolve dados pessoais que podem identificar os sujeitos dos dados. [Propriedade de dados](https://permission.io/blog/data-ownership) trata do _controle_ e [_direitos dos usu√°rios_](https://permission.io/blog/data-ownership) relacionados √† cria√ß√£o, processamento e dissemina√ß√£o de dados.

As quest√µes morais que precisamos fazer s√£o:
 * Quem √© o propriet√°rio dos dados? (usu√°rio ou organiza√ß√£o)
 * Quais direitos os sujeitos dos dados possuem? (ex: acesso, exclus√£o, portabilidade)
 * Quais direitos as organiza√ß√µes possuem? (ex: retificar avalia√ß√µes maliciosas de usu√°rios)

#### 2.2 Consentimento Informado

[Consentimento informado](https://legaldictionary.net/informed-consent/) define o ato de os usu√°rios concordarem com uma a√ß√£o (como coleta de dados) com um _entendimento completo_ dos fatos relevantes, incluindo o prop√≥sito, os riscos potenciais e as alternativas.

Quest√µes a explorar aqui s√£o:
 * O usu√°rio (sujeito dos dados) deu permiss√£o para a captura e uso dos dados?
 * O usu√°rio entendeu o prop√≥sito para o qual os dados foram capturados?
 * O usu√°rio compreendeu os riscos potenciais de sua participa√ß√£o?

#### 2.3 Propriedade Intelectual

[Propriedade intelectual](https://en.wikipedia.org/wiki/Intellectual_property) refere-se a cria√ß√µes intang√≠veis resultantes da iniciativa humana, que podem _ter valor econ√¥mico_ para indiv√≠duos ou empresas.

Quest√µes a explorar aqui s√£o:
 * Os dados coletados t√™m valor econ√¥mico para um usu√°rio ou empresa?
 * O **usu√°rio** possui propriedade intelectual aqui?
 * A **organiza√ß√£o** possui propriedade intelectual aqui?
 * Se esses direitos existem, como estamos protegendo-os?

#### 2.4 Privacidade de Dados

[Privacidade de dados](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) ou privacidade da informa√ß√£o refere-se √† preserva√ß√£o da privacidade do usu√°rio e prote√ß√£o da identidade do usu√°rio em rela√ß√£o a informa√ß√µes pessoalmente identific√°veis.

Quest√µes a explorar aqui s√£o:
 * Os dados (pessoais) dos usu√°rios est√£o protegidos contra invas√µes e vazamentos?
 * Os dados dos usu√°rios est√£o acess√≠veis apenas a usu√°rios e contextos autorizados?
 * O anonimato dos usu√°rios √© preservado quando os dados s√£o compartilhados ou disseminados?
 * Um usu√°rio pode ser desidentificado de conjuntos de dados anonimizados?

#### 2.5 Direito ao Esquecimento

O [Direito ao Esquecimento](https://en.wikipedia.org/wiki/Right_to_be_forgotten) ou [Direito √† Exclus√£o](https://www.gdpreu.org/right-to-be-forgotten/) fornece prote√ß√£o adicional de dados pessoais aos usu√°rios. Especificamente, d√° aos usu√°rios o direito de solicitar a exclus√£o ou remo√ß√£o de dados pessoais de buscas na Internet e outros locais, _sob circunst√¢ncias espec√≠ficas_ - permitindo-lhes um novo come√ßo online sem que a√ß√µes passadas sejam usadas contra eles.

Quest√µes a explorar aqui s√£o:
 * O sistema permite que os sujeitos dos dados solicitem a exclus√£o?
 * A retirada do consentimento do usu√°rio deve acionar a exclus√£o autom√°tica?
 * Os dados foram coletados sem consentimento ou por meios ilegais?
 * Estamos em conformidade com as regulamenta√ß√µes governamentais para privacidade de dados?

#### 2.6 Vi√©s nos Conjuntos de Dados

Vi√©s nos conjuntos de dados ou [Vi√©s de Coleta](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) trata da sele√ß√£o de um subconjunto _n√£o representativo_ de dados para o desenvolvimento de algoritmos, criando potencial injusti√ßa nos resultados para grupos diversos. Tipos de vi√©s incluem vi√©s de sele√ß√£o ou amostragem, vi√©s de voluntariado e vi√©s de instrumento.

Quest√µes a explorar aqui s√£o:
 * Recrutamos um conjunto representativo de sujeitos dos dados?
 * Testamos nosso conjunto de dados coletado ou curado para diversos vieses?
 * Podemos mitigar ou remover quaisquer vieses descobertos?

#### 2.7 Qualidade dos Dados

[Qualidade dos Dados](https://lakefs.io/data-quality-testing/) analisa a validade do conjunto de dados curado usado para desenvolver nossos algoritmos, verificando se os recursos e registros atendem aos requisitos para o n√≠vel de precis√£o e consist√™ncia necess√°rios para nosso prop√≥sito de IA.

Quest√µes a explorar aqui s√£o:
 * Capturamos _recursos_ v√°lidos para nosso caso de uso?
 * Os dados foram capturados _consistentemente_ em diversas fontes de dados?
 * O conjunto de dados est√° _completo_ para diversas condi√ß√µes ou cen√°rios?
 * As informa√ß√µes capturadas refletem a realidade com _precis√£o_?

#### 2.8 Justi√ßa nos Algoritmos
[Verifica√ß√£o de Justi√ßa Algor√≠tmica](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) analisa se o design do algoritmo discrimina sistematicamente subgrupos espec√≠ficos de sujeitos de dados, levando a [potenciais danos](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) na _aloca√ß√£o_ (onde recursos s√£o negados ou retidos desse grupo) e na _qualidade do servi√ßo_ (onde a IA n√£o √© t√£o precisa para alguns subgrupos quanto √© para outros).

Perguntas para explorar aqui s√£o:
 * Avaliamos a precis√£o do modelo para diversos subgrupos e condi√ß√µes?
 * Examinamos o sistema para potenciais danos (por exemplo, estereotipagem)?
 * Podemos revisar os dados ou re-treinar os modelos para mitigar os danos identificados?

Explore recursos como [checklists de Justi√ßa em IA](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) para aprender mais.

#### 2.9 Representa√ß√£o Errada

[Representa√ß√£o Errada de Dados](https://www.sciencedirect.com/topics/computer-science/misrepresentation) trata de perguntar se estamos comunicando insights de dados relatados honestamente de maneira enganosa para apoiar uma narrativa desejada.

Perguntas para explorar aqui s√£o:
 * Estamos relatando dados incompletos ou imprecisos?
 * Estamos visualizando dados de maneira que induza conclus√µes enganosas?
 * Estamos usando t√©cnicas estat√≠sticas seletivas para manipular resultados?
 * Existem explica√ß√µes alternativas que podem oferecer uma conclus√£o diferente?

#### 2.10 Livre Escolha
A [Ilus√£o de Livre Escolha](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) ocorre quando "arquiteturas de escolha" do sistema usam algoritmos de tomada de decis√£o para influenciar pessoas a tomarem um resultado preferido, enquanto aparentam dar op√ß√µes e controle. Esses [padr√µes obscuros](https://www.darkpatterns.org/) podem causar danos sociais e econ√¥micos aos usu√°rios. Como as decis√µes dos usu√°rios impactam perfis de comportamento, essas a√ß√µes potencialmente impulsionam escolhas futuras que podem amplificar ou estender o impacto desses danos.

Perguntas para explorar aqui s√£o:
 * O usu√°rio entendeu as implica√ß√µes de fazer essa escolha?
 * O usu√°rio estava ciente das escolhas (alternativas) e dos pr√≥s e contras de cada uma?
 * O usu√°rio pode reverter uma escolha automatizada ou influenciada posteriormente?

### 3. Estudos de Caso

Para colocar esses desafios √©ticos em contextos do mundo real, √© √∫til olhar para estudos de caso que destacam os potenciais danos e consequ√™ncias para indiv√≠duos e a sociedade, quando essas viola√ß√µes √©ticas s√£o ignoradas.

Aqui est√£o alguns exemplos:

| Desafio √âtico | Estudo de Caso  | 
|--- |--- |
| **Consentimento Informado** | 1972 - [Estudo de S√≠filis de Tuskegee](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Homens afro-americanos que participaram do estudo foram prometidos cuidados m√©dicos gratuitos, _mas foram enganados_ por pesquisadores que n√£o informaram os sujeitos sobre seu diagn√≥stico ou sobre a disponibilidade de tratamento. Muitos morreram e parceiros ou filhos foram afetados; o estudo durou 40 anos. | 
| **Privacidade de Dados** |  2007 - O [pr√™mio de dados da Netflix](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) forneceu aos pesquisadores _10 milh√µes de classifica√ß√µes de filmes anonimizadas de 50 mil clientes_ para ajudar a melhorar algoritmos de recomenda√ß√£o. No entanto, os pesquisadores conseguiram correlacionar dados anonimizados com dados pessoalmente identific√°veis em _conjuntos de dados externos_ (por exemplo, coment√°rios no IMDb) - efetivamente "desanonimizando" alguns assinantes da Netflix.|
| **Vi√©s na Coleta**  | 2013 - A cidade de Boston [desenvolveu o Street Bump](https://www.boston.gov/transportation/street-bump), um aplicativo que permitia aos cidad√£os reportar buracos, fornecendo √† cidade melhores dados sobre as estradas para encontrar e corrigir problemas. No entanto, [pessoas de grupos de baixa renda tinham menos acesso a carros e telefones](https://hbr.org/2013/04/the-hidden-biases-in-big-data), tornando seus problemas nas estradas invis√≠veis no aplicativo. Os desenvolvedores trabalharam com acad√™micos para abordar quest√µes de _acesso equitativo e divis√µes digitais_ para justi√ßa. |
| **Justi√ßa Algor√≠tmica**  | 2018 - O MIT [Estudo Gender Shades](http://gendershades.org/overview.html) avaliou a precis√£o de produtos de IA de classifica√ß√£o de g√™nero, expondo lacunas na precis√£o para mulheres e pessoas de cor. Um [Apple Card de 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) parecia oferecer menos cr√©dito para mulheres do que para homens. Ambos ilustraram problemas de vi√©s algor√≠tmico levando a danos socioecon√¥micos.|
| **Representa√ß√£o Errada de Dados** | 2020 - O [Departamento de Sa√∫de P√∫blica da Ge√≥rgia divulgou gr√°ficos de COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) que pareciam enganar os cidad√£os sobre tend√™ncias de casos confirmados com ordena√ß√£o n√£o cronol√≥gica no eixo x. Isso ilustra representa√ß√£o errada por meio de truques de visualiza√ß√£o. |
| **Ilus√£o de Livre Escolha** | 2020 - O aplicativo de aprendizado [ABCmouse pagou $10 milh√µes para resolver uma reclama√ß√£o da FTC](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) onde os pais foram presos em assinaturas que n√£o podiam cancelar. Isso ilustra padr√µes obscuros em arquiteturas de escolha, onde os usu√°rios foram influenciados a fazer escolhas potencialmente prejudiciais. |
| **Privacidade de Dados e Direitos do Usu√°rio** | 2021 - O [Vazamento de Dados do Facebook](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) exp√¥s dados de 530 milh√µes de usu√°rios, resultando em um acordo de $5 bilh√µes com a FTC. No entanto, a empresa se recusou a notificar os usu√°rios sobre o vazamento, violando os direitos dos usu√°rios em rela√ß√£o √† transpar√™ncia e acesso aos dados. |

Quer explorar mais estudos de caso? Confira esses recursos:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - dilemas √©ticos em diversas ind√∫strias. 
* [Curso de √âtica em Ci√™ncia de Dados](https://www.coursera.org/learn/data-science-ethics#syllabus) - estudos de caso marcantes explorados.
* [Onde as coisas deram errado](https://deon.drivendata.org/examples/) - checklist de Deon com exemplos.

> üö® Pense nos estudos de caso que voc√™ viu - voc√™ j√° experimentou ou foi afetado por um desafio √©tico semelhante em sua vida? Consegue pensar em pelo menos um outro estudo de caso que ilustre um dos desafios √©ticos discutidos nesta se√ß√£o?

## √âtica Aplicada

Falamos sobre conceitos √©ticos, desafios e estudos de caso em contextos do mundo real. Mas como come√ßar a _aplicar_ princ√≠pios e pr√°ticas √©ticas em nossos projetos? E como _operacionalizar_ essas pr√°ticas para uma melhor governan√ßa? Vamos explorar algumas solu√ß√µes do mundo real:

### 1. C√≥digos Profissionais

C√≥digos Profissionais oferecem uma op√ß√£o para organiza√ß√µes "incentivarem" membros a apoiar seus princ√≠pios √©ticos e declara√ß√£o de miss√£o. C√≥digos s√£o _diretrizes morais_ para comportamento profissional, ajudando funcion√°rios ou membros a tomar decis√µes que estejam alinhadas com os princ√≠pios da organiza√ß√£o. Eles s√£o t√£o bons quanto a conformidade volunt√°ria dos membros; no entanto, muitas organiza√ß√µes oferecem recompensas e penalidades adicionais para motivar a conformidade.

Exemplos incluem:

 * [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) C√≥digo de √âtica
 * [Data Science Association](http://datascienceassn.org/code-of-conduct.html) C√≥digo de Conduta (criado em 2013)
 * [ACM C√≥digo de √âtica e Conduta Profissional](https://www.acm.org/code-of-ethics) (desde 1993)

> üö® Voc√™ pertence a uma organiza√ß√£o profissional de engenharia ou ci√™ncia de dados? Explore o site deles para ver se definem um c√≥digo de √©tica profissional. O que isso diz sobre seus princ√≠pios √©ticos? Como est√£o "incentivando" os membros a seguir o c√≥digo?

### 2. Checklists de √âtica

Enquanto os c√≥digos profissionais definem o _comportamento √©tico_ exigido dos profissionais, eles [t√™m limita√ß√µes conhecidas](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) na aplica√ß√£o, particularmente em projetos de grande escala. Em vez disso, muitos especialistas em ci√™ncia de dados [defendem checklists](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), que podem **conectar princ√≠pios a pr√°ticas** de maneiras mais determin√≠sticas e acion√°veis.

Checklists convertem perguntas em tarefas "sim/n√£o" que podem ser operacionalizadas, permitindo que sejam rastreadas como parte dos fluxos de trabalho padr√£o de lan√ßamento de produtos.

Exemplos incluem:
 * [Deon](https://deon.drivendata.org/) - um checklist de √©tica em dados de uso geral criado a partir de [recomenda√ß√µes da ind√∫stria](https://deon.drivendata.org/#checklist-citations) com uma ferramenta de linha de comando para f√°cil integra√ß√£o.
 * [Checklist de Auditoria de Privacidade](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - fornece orienta√ß√£o geral para pr√°ticas de manuseio de informa√ß√µes sob perspectivas legais e sociais.
 * [Checklist de Justi√ßa em IA](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - criado por profissionais de IA para apoiar a ado√ß√£o e integra√ß√£o de verifica√ß√µes de justi√ßa nos ciclos de desenvolvimento de IA.
 * [22 perguntas para √©tica em dados e IA](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - estrutura mais aberta, estruturada para explora√ß√£o inicial de quest√µes √©ticas em design, implementa√ß√£o e contextos organizacionais.

### 3. Regulamenta√ß√µes √âticas

√âtica trata de definir valores compartilhados e fazer a coisa certa _voluntariamente_. **Conformidade** trata de _seguir a lei_ onde definida. **Governan√ßa** cobre amplamente todas as formas pelas quais as organiza√ß√µes operam para aplicar princ√≠pios √©ticos e cumprir leis estabelecidas.

Hoje, a governan√ßa assume duas formas dentro das organiza√ß√µes. Primeiro, trata-se de definir princ√≠pios de **IA √©tica** e estabelecer pr√°ticas para operacionalizar a ado√ß√£o em todos os projetos relacionados √† IA na organiza√ß√£o. Segundo, trata-se de cumprir todas as regulamenta√ß√µes de **prote√ß√£o de dados** exigidas pelo governo nas regi√µes em que opera.

Exemplos de regulamenta√ß√µes de prote√ß√£o e privacidade de dados:

 * `1974`, [Lei de Privacidade dos EUA](https://www.justice.gov/opcl/privacy-act-1974) - regula a coleta, uso e divulga√ß√£o de informa√ß√µes pessoais pelo _governo federal_.
 * `1996`, [Lei de Portabilidade e Responsabilidade de Seguro de Sa√∫de dos EUA (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - protege dados pessoais de sa√∫de.
 * `1998`, [Lei de Prote√ß√£o √† Privacidade Online das Crian√ßas dos EUA (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - protege a privacidade de dados de crian√ßas menores de 13 anos.
 * `2018`, [Regulamento Geral de Prote√ß√£o de Dados (GDPR)](https://gdpr-info.eu/) - fornece direitos aos usu√°rios, prote√ß√£o de dados e privacidade.
 * `2018`, [Lei de Privacidade do Consumidor da Calif√≥rnia (CCPA)](https://www.oag.ca.gov/privacy/ccpa) - d√° aos consumidores mais _direitos_ sobre seus dados pessoais.
 * `2021`, [Lei de Prote√ß√£o de Informa√ß√µes Pessoais da China](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) - recentemente aprovada, criando uma das regulamenta√ß√µes de privacidade de dados online mais fortes do mundo.

> üö® A Uni√£o Europeia definiu o GDPR (Regulamento Geral de Prote√ß√£o de Dados), que continua sendo uma das regulamenta√ß√µes de privacidade de dados mais influentes hoje. Voc√™ sabia que ele tamb√©m define [8 direitos dos usu√°rios](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) para proteger a privacidade digital e os dados pessoais dos cidad√£os? Aprenda quais s√£o esses direitos e por que eles s√£o importantes.

### 4. Cultura √âtica

Note que ainda existe uma lacuna intang√≠vel entre _conformidade_ (fazer o suficiente para atender "√† letra da lei") e abordar [quest√µes sist√™micas](https://www.coursera.org/learn/data-science-ethics/home/week/4) (como ossifica√ß√£o, assimetria de informa√ß√µes e injusti√ßa distributiva) que podem acelerar a arma√ß√£o da IA.

O √∫ltimo requer [abordagens colaborativas para definir culturas √©ticas](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) que construam conex√µes emocionais e valores compartilhados consistentes _entre organiza√ß√µes_ na ind√∫stria. Isso exige mais [culturas √©ticas formalizadas de dados](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) nas organiza√ß√µes - permitindo que _qualquer pessoa_ [puxe o cord√£o Andon](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (para levantar preocupa√ß√µes √©ticas cedo no processo) e tornando _avalia√ß√µes √©ticas_ (por exemplo, na contrata√ß√£o) um crit√©rio central na forma√ß√£o de equipes em projetos de IA.

---
## [Quiz p√≥s-aula](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## Revis√£o e Autoestudo 

Cursos e livros ajudam a entender conceitos √©ticos fundamentais e desafios, enquanto estudos de caso e ferramentas ajudam com pr√°ticas √©ticas aplicadas em contextos do mundo real. Aqui est√£o alguns recursos para come√ßar:

* [Machine Learning For Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - li√ß√£o sobre Justi√ßa, da Microsoft.
* [Princ√≠pios de IA Respons√°vel](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - trilha de aprendizado gratuita do Microsoft Learn.  
* [√âtica e Ci√™ncia de Dados](https://resources.oreilly.com/examples/0636920203964) - EBook da O'Reilly (M. Loukides, H. Mason et. al)  
* [√âtica na Ci√™ncia de Dados](https://www.coursera.org/learn/data-science-ethics#syllabus) - curso online da Universidade de Michigan.  
* [√âtica Desvendada](https://ethicsunwrapped.utexas.edu/case-studies) - estudos de caso da Universidade do Texas.  

# Tarefa  

[Escreva um Estudo de Caso sobre √âtica de Dados](assignment.md)  

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes equivocadas decorrentes do uso desta tradu√ß√£o.