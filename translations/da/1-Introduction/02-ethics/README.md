<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58860ce9a4b8a564003d2752f7c72851",
  "translation_date": "2025-10-03T16:38:15+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "da"
}
-->
# Introduktion til Dataetik

|![ Sketchnote af [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| Data Science Ethics - _Sketchnote af [@nitya](https://twitter.com/nitya)_ |

---

Vi er alle databorgere, der lever i en verden pr√¶get af data.

Markedstendenser viser, at inden 2022 vil 1 ud af 3 store organisationer k√∏be og s√¶lge deres data via online [markedspladser og b√∏rser](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/). Som **appudviklere** vil vi finde det nemmere og billigere at integrere datadrevne indsigter og algoritmebaseret automatisering i daglige brugeroplevelser. Men efterh√•nden som AI bliver mere udbredt, skal vi ogs√• forst√• de potentielle skader for√•rsaget af [v√•benisering](https://www.youtube.com/watch?v=TQHs8SA1qpk) af s√•danne algoritmer i stor skala.

Tendenser antyder, at vi inden 2025 vil generere og forbruge over [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) data. For **dataspecialister** giver denne eksplosion af information en enest√•ende adgang til personlige og adf√¶rdsm√¶ssige data. Med denne adgang f√∏lger muligheden for at opbygge detaljerede brugerprofiler og subtilt p√•virke beslutningstagning‚Äîofte p√• m√•der, der skaber en [illusion af fri vilje](https://www.datasciencecentral.com/the-pareto-set-and-the-paradox-of-choice/). Selvom dette kan bruges til at skubbe brugere mod √∏nskede resultater, rejser det ogs√• kritiske sp√∏rgsm√•l om databeskyttelse, autonomi og de etiske gr√¶nser for algoritmisk indflydelse.

Dataetik er nu _n√∏dvendige retningslinjer_ for datavidenskab og ingeni√∏rarbejde, der hj√¶lper os med at minimere potentielle skader og utilsigtede konsekvenser af vores datadrevne handlinger. [Gartner Hype Cycle for AI](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identificerer relevante tendenser inden for digital etik, ansvarlig AI og AI-styring som n√∏glefaktorer for st√∏rre megatrends omkring _demokratisering_ og _industrialisering_ af AI.

![Gartner's Hype Cycle for AI - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

I denne lektion vil vi udforske det fascinerende omr√•de inden for dataetik - fra kernekoncepter og udfordringer til casestudier og anvendte AI-koncepter som styring - der hj√¶lper med at etablere en etik-kultur i teams og organisationer, der arbejder med data og AI.

## [Quiz f√∏r lektionen](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Grundl√¶ggende Definitioner

Lad os starte med at forst√• den grundl√¶ggende terminologi.

Ordet "etik" stammer fra det [gr√¶ske ord "ethikos"](https://en.wikipedia.org/wiki/Ethics) (og dets rod "ethos"), som betyder _karakter eller moralsk natur_. 

**Etik** handler om de f√¶lles v√¶rdier og moralske principper, der styrer vores adf√¶rd i samfundet. Etik er ikke baseret p√• love, men p√• bredt accepterede normer for, hvad der er "rigtigt vs. forkert". Dog kan etiske overvejelser p√•virke initiativer inden for virksomhedsledelse og regeringsreguleringer, der skaber flere incitamenter til overholdelse.

**Dataetik** er en [ny gren af etik](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1), der "unders√∏ger og evaluerer moralske problemer relateret til _data, algoritmer og tilsvarende praksis_". Her fokuserer **"data"** p√• handlinger relateret til generering, registrering, kuratering, behandling, formidling, deling og brug, **"algoritmer"** fokuserer p√• AI, agenter, maskinl√¶ring og robotter, og **"praksis"** fokuserer p√• emner som ansvarlig innovation, programmering, hacking og etiske kodeks.

**Anvendt etik** er den [praktiske anvendelse af moralske overvejelser](https://en.wikipedia.org/wiki/Applied_ethics). Det er processen med aktivt at unders√∏ge etiske sp√∏rgsm√•l i konteksten af _virkelige handlinger, produkter og processer_ og tage korrigerende foranstaltninger for at sikre, at disse forbliver i overensstemmelse med vores definerede etiske v√¶rdier.

**Etik-kultur** handler om [_operationelt at anvende_ anvendt etik](https://hbr.org/2019/05/how-to-design-an-ethical-organization) for at sikre, at vores etiske principper og praksis bliver vedtaget p√• en konsekvent og skalerbar m√•de i hele organisationen. Succesfulde etik-kulturer definerer organisationens etiske principper, giver meningsfulde incitamenter til overholdelse og forst√¶rker etiske normer ved at opmuntre og fremh√¶ve √∏nsket adf√¶rd p√• alle niveauer i organisationen.

## Etiske Koncepter

I denne sektion vil vi diskutere koncepter som **f√¶lles v√¶rdier** (principper) og **etiske udfordringer** (problemer) inden for dataetik - og udforske **casestudier**, der hj√¶lper dig med at forst√• disse koncepter i virkelige kontekster.

### 1. Etiske Principper

Enhver dataetik-strategi begynder med at definere _etiske principper_ - de "f√¶lles v√¶rdier", der beskriver acceptable adf√¶rd og guider overholdelse i vores data- og AI-projekter. Du kan definere disse p√• individuelt eller teamniveau. Dog beskriver de fleste store organisationer disse i en _etisk AI_-missionserkl√¶ring eller rammev√¶rk, der er defineret p√• virksomhedsniveau og konsekvent h√•ndh√¶vet p√• tv√¶rs af alle teams.

**Eksempel:** Microsofts [Ansvarlig AI](https://www.microsoft.com/en-us/ai/responsible-ai)-missionserkl√¶ring lyder: _"Vi er forpligtet til at fremme AI drevet af etiske principper, der s√¶tter mennesker f√∏rst"_ - og identificerer 6 etiske principper i rammev√¶rket nedenfor:

![Ansvarlig AI hos Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Lad os kort udforske disse principper. _Transparens_ og _ansvarlighed_ er grundl√¶ggende v√¶rdier, som de andre principper bygger p√• - s√• lad os starte der:

* [**Ansvarlighed**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) g√∏r praktikere _ansvarlige_ for deres data- og AI-operationer og overholdelse af disse etiske principper.
* [**Transparens**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) sikrer, at data- og AI-handlinger er _forst√•elige_ (fortolkelige) for brugere, og forklarer hvad og hvorfor bag beslutninger.
* [**Fairness**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - fokuserer p√• at sikre, at AI behandler _alle mennesker_ retf√¶rdigt og adresserer eventuelle systemiske eller implicitte socio-tekniske bias i data og systemer.
* [**P√•lidelighed & Sikkerhed**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - sikrer, at AI opf√∏rer sig _konsekvent_ med definerede v√¶rdier og minimerer potentielle skader eller utilsigtede konsekvenser.
* [**Privatliv & Sikkerhed**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - handler om at forst√• dataens oprindelse og give _databeskyttelse og relaterede sikkerhedsforanstaltninger_ til brugere.
* [**Inklusion**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - handler om at designe AI-l√∏sninger med intention og tilpasse dem til at im√∏dekomme et _bredt spektrum af menneskelige behov_ og kapaciteter.

> üö® T√¶nk over, hvad din dataetik-missionserkl√¶ring kunne v√¶re. Udforsk etiske AI-rammev√¶rk fra andre organisationer - her er eksempler fra [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) og [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Hvilke f√¶lles v√¶rdier har de? Hvordan relaterer disse principper sig til AI-produktet eller industrien, de opererer i?

### 2. Etiske Udfordringer

N√•r vi har defineret etiske principper, er n√¶ste skridt at evaluere vores data- og AI-handlinger for at se, om de stemmer overens med disse f√¶lles v√¶rdier. T√¶nk p√• dine handlinger i to kategorier: _datainnsamling_ og _algoritmedesign_. 

Ved datainnsamling vil handlinger sandsynligvis involvere **personlige data** eller personligt identificerbare oplysninger (PII) for identificerbare levende individer. Dette inkluderer [forskellige typer af ikke-personlige data](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en), der _samlet set_ identificerer en person. Etiske udfordringer kan relateres til _databeskyttelse_, _dataejerskab_ og relaterede emner som _informeret samtykke_ og _intellektuelle ejendomsrettigheder_ for brugere.

Ved algoritmedesign vil handlinger involvere indsamling og kuratering af **datas√¶t**, som derefter bruges til at tr√¶ne og implementere **datamodeller**, der forudsiger resultater eller automatiserer beslutninger i virkelige kontekster. Etiske udfordringer kan opst√• fra _datas√¶tbias_, _datakvalitetsproblemer_, _uretf√¶rdighed_ og _fejlrepr√¶sentation_ i algoritmer - inklusive nogle problemer, der er systemiske af natur.

I begge tilf√¶lde fremh√¶ver etiske udfordringer omr√•der, hvor vores handlinger kan komme i konflikt med vores f√¶lles v√¶rdier. For at opdage, afb√∏de, minimere eller eliminere disse bekymringer skal vi stille moralske "ja/nej"-sp√∏rgsm√•l relateret til vores handlinger og derefter tage korrigerende handlinger efter behov. Lad os se p√• nogle etiske udfordringer og de moralske sp√∏rgsm√•l, de rejser:

#### 2.1 Dataejerskab

Datainnsamling involverer ofte personlige data, der kan identificere datasubjekterne. [Dataejerskab](https://permission.io/blog/data-ownership) handler om _kontrol_ og [_brugernes rettigheder_](https://permission.io/blog/data-ownership) relateret til oprettelse, behandling og formidling af data. 

De moralske sp√∏rgsm√•l, vi skal stille, er: 
 * Hvem ejer dataene? (bruger eller organisation)
 * Hvilke rettigheder har datasubjekterne? (fx adgang, sletning, portabilitet)
 * Hvilke rettigheder har organisationer? (fx rette skadelige brugeranmeldelser)

#### 2.2 Informeret Samtykke

[Informeret samtykke](https://legaldictionary.net/informed-consent/) definerer handlingen, hvor brugere accepterer en handling (som datainnsamling) med en _fuld forst√•else_ af relevante fakta, herunder form√•l, potentielle risici og alternativer. 

Sp√∏rgsm√•l at udforske her er:
 * Gav brugeren (datasubjektet) tilladelse til datainnsamling og brug?
 * Forstod brugeren form√•let med, hvorfor dataene blev indsamlet?
 * Forstod brugeren de potentielle risici ved deres deltagelse?

#### 2.3 Intellektuel Ejendom

[Intellektuel ejendom](https://en.wikipedia.org/wiki/Intellectual_property) refererer til immaterielle skabelser, der er resultatet af menneskelig initiativ, og som kan _have √∏konomisk v√¶rdi_ for individer eller virksomheder. 

Sp√∏rgsm√•l at udforske her er:
 * Havde de indsamlede data √∏konomisk v√¶rdi for en bruger eller virksomhed?
 * Har **brugeren** intellektuel ejendom her?
 * Har **organisationen** intellektuel ejendom her?
 * Hvis disse rettigheder eksisterer, hvordan beskytter vi dem?

#### 2.4 Databeskyttelse

[Databeskyttelse](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) eller informationsbeskyttelse refererer til bevarelse af brugerens privatliv og beskyttelse af brugerens identitet med hensyn til personligt identificerbare oplysninger. 

Sp√∏rgsm√•l at udforske her er:
 * Er brugernes (personlige) data sikret mod hacking og l√¶kager?
 * Er brugernes data kun tilg√¶ngelige for autoriserede brugere og kontekster?
 * Bevares brugernes anonymitet, n√•r data deles eller formidles?
 * Kan en bruger blive de-identificeret fra anonymiserede datas√¶t?

#### 2.5 Retten til at Blive Glemt

[Retten til at blive glemt](https://en.wikipedia.org/wiki/Right_to_be_forgotten) eller [Retten til Sletning](https://www.gdpreu.org/right-to-be-forgotten/) giver yderligere beskyttelse af personlige data til brugere. Specifikt giver det brugere ret til at anmode om sletning eller fjernelse af personlige data fra internets√∏gninger og andre steder, _under specifikke omst√¶ndigheder_ - hvilket giver dem en ny start online uden tidligere handlinger, der holdes imod dem.

Sp√∏rgsm√•l at udforske her er:
 * Tillader systemet datasubjekter at anmode om sletning?
 * Skal tilbagetr√¶kning af brugerens samtykke udl√∏se automatisk sletning?
 * Blev data indsamlet uden samtykke eller p√• ulovlig vis?
 * Er vi i overensstemmelse med regeringsreguleringer for databeskyttelse?

#### 2.6 Datas√¶tbias

Datas√¶t eller [indsamlingsbias](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) handler om at v√¶lge et _ikke-repr√¶sentativt_ datas√¶t til algoritmeudvikling, hvilket skaber potentiel uretf√¶rdighed i resultatet for forskellige grupper. Typer af bias inkluderer udv√¶lgelses- eller pr√∏vebias, frivillig bias og instrumentbias. 

Sp√∏rgsm√•l at udforske her er:
 * Rekrutterede vi et repr√¶sentativt datas√¶t af datasubjekter?
 * Testede vi vores indsamlede eller kuraterede datas√¶t for forskellige bias?
 * Kan vi afb√∏de eller fjerne eventuelle opdagede bias?

#### 2.7 Datakvalitet

[Datakvalitet](https://lakefs.io/data-quality-testing/) ser p√• gyldigheden af det kuraterede datas√¶t, der bruges til at udvikle vores algoritmer, og kontrollerer, om funktioner og poster opfylder kravene til det niveau af n√∏jagtighed og konsistens, der er n√∏dvendigt for vores AI-form√•l.

Sp√∏rgsm√•l at udforske her er:
 * Indsamlede vi gyldige _funktioner_ til vores brugssag?
 * Blev data indsamlet _konsekvent_ p√• tv√¶rs af forskellige datakilder?
 * Er datas√¶ttet _komplet_ for forskellige forhold eller scenarier?
* Bliver information indfanget _pr√¶cist_ og afspejler virkeligheden?

#### 2.8 Algoritmisk Retf√¶rdighed

[Algoritmisk Retf√¶rdighed](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) unders√∏ger, om algoritmens design systematisk diskriminerer specifikke undergrupper af datasubjekter, hvilket kan f√∏re til [potentielle skader](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) i _ressourcefordeling_ (hvor ressourcer n√¶gtes eller tilbageholdes fra den gruppe) og _servicekvalitet_ (hvor AI ikke er lige s√• pr√¶cis for nogle undergrupper som for andre).

Sp√∏rgsm√•l, der kan udforskes her, er:
* Evaluerede vi modelpr√¶cision for forskellige undergrupper og forhold?
* Unders√∏gte vi systemet for potentielle skader (f.eks. stereotyper)?
* Kan vi revidere data eller genoptr√¶ne modeller for at afhj√¶lpe identificerede skader?

Udforsk ressourcer som [AI Fairness-tjeklister](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) for at l√¶re mere.

#### 2.9 Fejlrepr√¶sentation

[Data Fejlrepr√¶sentation](https://www.sciencedirect.com/topics/computer-science/misrepresentation) handler om at sp√∏rge, om vi kommunikerer indsigt fra √¶rligt rapporterede data p√• en vildledende m√•de for at underst√∏tte en √∏nsket fort√¶lling.

Sp√∏rgsm√•l, der kan udforskes her, er:
* Rapporterer vi ufuldst√¶ndige eller un√∏jagtige data?
* Visualiserer vi data p√• en m√•de, der f√∏rer til vildledende konklusioner?
* Bruger vi selektive statistiske teknikker til at manipulere resultater?
* Er der alternative forklaringer, der kan give en anden konklusion?

#### 2.10 Fri Valg
[Illusionen af Fri Valg](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) opst√•r, n√•r systemets "valgarkitekturer" bruger beslutningsalgoritmer til at skubbe folk mod at tage et foretrukket resultat, mens det ser ud som om, de har muligheder og kontrol. Disse [m√∏rke m√∏nstre](https://www.darkpatterns.org/) kan for√•rsage sociale og √∏konomiske skader for brugere. Fordi brugerbeslutninger p√•virker adf√¶rdsprofiler, kan disse handlinger potentielt drive fremtidige valg, der kan forst√¶rke eller udvide virkningen af disse skader.

Sp√∏rgsm√•l, der kan udforskes her, er:
* Forstod brugeren konsekvenserne af at tr√¶ffe det valg?
* Var brugeren opm√¶rksom p√• (alternative) valg og fordele & ulemper ved hver?
* Kan brugeren senere fortryde et automatiseret eller p√•virket valg?

### 3. Case Studier

For at s√¶tte disse etiske udfordringer i en virkelighedsn√¶r kontekst hj√¶lper det at se p√• case studier, der fremh√¶ver de potentielle skader og konsekvenser for individer og samfund, n√•r s√•danne etiske overtr√¶delser overses.

Her er nogle eksempler:

| Etisk Udfordring | Case Studie | 
|--- |--- |
| **Informeret Samtykke** | 1972 - [Tuskegee Syfilis Studie](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Afroamerikanske m√¶nd, der deltog i studiet, blev lovet gratis medicinsk behandling _men blev bedraget_ af forskere, der undlod at informere deltagerne om deres diagnose eller om tilg√¶ngelig behandling. Mange deltagere d√∏de, og partnere eller b√∏rn blev p√•virket; studiet varede i 40 √•r. | 
| **Databeskyttelse** | 2007 - [Netflix data-pris](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) gav forskere _10M anonymiserede filmvurderinger fra 50K kunder_ for at hj√¶lpe med at forbedre anbefalingsalgoritmer. Forskere var dog i stand til at korrelere anonymiserede data med personligt identificerbare data i _eksterne datas√¶t_ (f.eks. IMDb-kommentarer) - effektivt "de-anonymiserende" nogle Netflix-abonnenter. |
| **Indsamlingsbias** | 2013 - Boston City [udviklede Street Bump](https://www.boston.gov/transportation/street-bump), en app, der lod borgere rapportere huller i vejen, hvilket gav byen bedre data til at finde og l√∏se problemer. Dog havde [folk i lavindkomstgrupper mindre adgang til biler og telefoner](https://hbr.org/2013/04/the-hidden-biases-in-big-data), hvilket gjorde deres vejproblemer usynlige i denne app. Udviklere arbejdede med akademikere for at l√∏se _lighed i adgang og digitale skel_ for retf√¶rdighed. |
| **Algoritmisk Retf√¶rdighed** | 2018 - MIT [Gender Shades Studie](http://gendershades.org/overview.html) evaluerede pr√¶cisionen af AI-produkter til k√∏nsidentifikation og afsl√∏rede mangler i pr√¶cision for kvinder og farvede personer. Et [2019 Apple Card](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) syntes at tilbyde mindre kredit til kvinder end m√¶nd. Begge illustrerede problemer med algoritmisk bias, der f√∏rte til socio√∏konomiske skader. |
| **Data Fejlrepr√¶sentation** | 2020 - [Georgia Department of Public Health udgav COVID-19 diagrammer](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening), der syntes at vildlede borgere om tendenser i bekr√¶ftede tilf√¶lde med ikke-kronologisk r√¶kkef√∏lge p√• x-aksen. Dette illustrerer fejlrepr√¶sentation gennem visualiseringstricks. |
| **Illusionen af fri valg** | 2020 - L√¶ringsappen [ABCmouse betalte $10M for at afvikle en FTC-klage](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/), hvor for√¶ldre blev fanget i at betale for abonnementer, de ikke kunne annullere. Dette illustrerer m√∏rke m√∏nstre i valgarkitekturer, hvor brugere blev skubbet mod potentielt skadelige valg. |
| **Databeskyttelse & Brugerrettigheder** | 2021 - Facebook [Data Brud](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) afsl√∏rede data fra 530M brugere, hvilket resulterede i en $5B afvikling til FTC. Det n√¶gtede dog at informere brugere om bruddet, hvilket overtr√•dte brugerrettigheder omkring datatransparens og adgang. |

Vil du udforske flere case studier? Tjek disse ressourcer:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - etiske dilemmaer p√• tv√¶rs af forskellige industrier. 
* [Data Science Ethics kursus](https://www.coursera.org/learn/data-science-ethics#syllabus) - landmark case studier udforsket.
* [Hvor tingene er g√•et galt](https://deon.drivendata.org/examples/) - deon-tjekliste med eksempler.

> üö® T√¶nk over de case studier, du har set - har du oplevet eller v√¶ret p√•virket af en lignende etisk udfordring i dit liv? Kan du t√¶nke p√• mindst √©n anden case studie, der illustrerer en af de etiske udfordringer, vi har diskuteret i dette afsnit?

## Anvendt Etik

Vi har talt om etiske begreber, udfordringer og case studier i virkelighedsn√¶re kontekster. Men hvordan kommer vi i gang med at _anvende_ etiske principper og praksisser i vores projekter? Og hvordan _operationaliserer_ vi disse praksisser for bedre styring? Lad os udforske nogle virkelighedsn√¶re l√∏sninger:

### 1. Professionelle Koder

Professionelle koder tilbyder en mulighed for organisationer til at "incitamentere" medlemmer til at st√∏tte deres etiske principper og mission. Koder er _moralske retningslinjer_ for professionel adf√¶rd, der hj√¶lper medarbejdere eller medlemmer med at tr√¶ffe beslutninger, der stemmer overens med organisationens principper. De er kun s√• gode som den frivillige overholdelse fra medlemmer; dog tilbyder mange organisationer yderligere bel√∏nninger og sanktioner for at motivere overholdelse.

Eksempler inkluderer:

* [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) Etisk kodeks
* [Data Science Association](http://datascienceassn.org/code-of-conduct.html) Adf√¶rdskodeks (oprettet 2013)
* [ACM Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics) (siden 1993)

> üö® Tilh√∏rer du en professionel ingeni√∏r- eller data science-organisation? Udforsk deres hjemmeside for at se, om de definerer en professionel etisk kodeks. Hvad siger dette om deres etiske principper? Hvordan "incitamenterer" de medlemmer til at f√∏lge koden?

### 2. Etiske Tjeklister

Mens professionelle koder definerer kr√¶vet _etisk adf√¶rd_ fra praktikere, har de [kendte begr√¶nsninger](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) i h√•ndh√¶velse, is√¶r i storskala projekter. I stedet anbefaler mange data science-eksperter [tjeklister](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), der kan **forbinde principper med praksis** p√• mere deterministiske og handlingsorienterede m√•der.

Tjeklister konverterer sp√∏rgsm√•l til "ja/nej"-opgaver, der kan operationaliseres, hvilket g√∏r det muligt at spore dem som en del af standard produktudgivelsesarbejdsgange.

Eksempler inkluderer:
* [Deon](https://deon.drivendata.org/) - en generel dataetik-tjekliste oprettet fra [industriens anbefalinger](https://deon.drivendata.org/#checklist-citations) med et kommandolinjev√¶rkt√∏j for nem integration.
* [Privacy Audit Checklist](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - giver generel vejledning til informationsh√•ndteringspraksis fra juridiske og sociale eksponeringsperspektiver.
* [AI Fairness Checklist](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - oprettet af AI-praktikere for at st√∏tte adoption og integration af fairness-tjek i AI-udviklingscyklusser.
* [22 sp√∏rgsm√•l om etik i data og AI](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - en mere √•ben ramme, struktureret til indledende udforskning af etiske sp√∏rgsm√•l i design, implementering og organisatoriske kontekster.

### 3. Etiske Reguleringer

Etik handler om at definere f√¶lles v√¶rdier og g√∏re det rigtige _frivilligt_. **Overholdelse** handler om _at f√∏lge loven_, hvis og hvor den er defineret. **Styring** d√¶kker bredt alle de m√•der, hvorp√• organisationer opererer for at h√•ndh√¶ve etiske principper og overholde etablerede love.

I dag tager styring to former inden for organisationer. For det f√∏rste handler det om at definere **etiske AI**-principper og etablere praksisser for at operationalisere adoption p√• tv√¶rs af alle AI-relaterede projekter i organisationen. For det andet handler det om at overholde alle regeringsmandaterede **databeskyttelsesreguleringer** for de regioner, den opererer i.

Eksempler p√• databeskyttelses- og privatlivsreguleringer:

* `1974`, [US Privacy Act](https://www.justice.gov/opcl/privacy-act-1974) - regulerer _f√∏deral regeringens_ indsamling, brug og offentligg√∏relse af personlige oplysninger.
* `1996`, [US Health Insurance Portability & Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - beskytter personlige sundhedsdata.
* `1998`, [US Children's Online Privacy Protection Act (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - beskytter databeskyttelse for b√∏rn under 13.
* `2018`, [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) - giver brugerrettigheder, databeskyttelse og privatliv.
* `2018`, [California Consumer Privacy Act (CCPA)](https://www.oag.ca.gov/privacy/ccpa) giver forbrugere flere _rettigheder_ over deres (personlige) data.
* `2021`, Kinas [Personal Information Protection Law](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) blev netop vedtaget og skaber en af de st√¶rkeste online databeskyttelsesreguleringer i verden.

> üö® Den Europ√¶iske Union definerede GDPR (General Data Protection Regulation) forbliver en af de mest indflydelsesrige databeskyttelsesreguleringer i dag. Vidste du, at den ogs√• definerer [8 brugerrettigheder](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) for at beskytte borgernes digitale privatliv og personlige data? L√¶r om, hvad disse er, og hvorfor de betyder noget.

### 4. Etisk Kultur

Bem√¶rk, at der stadig er en immateriel kl√∏ft mellem _overholdelse_ (at g√∏re nok for at opfylde "lovens bogstav") og adressering af [systemiske problemer](https://www.coursera.org/learn/data-science-ethics/home/week/4) (som stivhed, informationsasymmetri og fordelingsm√¶ssig uretf√¶rdighed), der kan fremskynde v√•beniseringen af AI.

Sidstn√¶vnte kr√¶ver [samarbejdsmetoder til at definere etiske kulturer](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f), der bygger f√∏lelsesm√¶ssige forbindelser og konsistente f√¶lles v√¶rdier _p√• tv√¶rs af organisationer_ i branchen. Dette kr√¶ver mere [formaliserede dataetiske kulturer](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) i organisationer - hvilket giver _enhver_ mulighed for at [tr√¶kke Andon-snoren](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (for at rejse etiske bekymringer tidligt i processen) og g√∏re _etiske vurderinger_ (f.eks. ved ans√¶ttelse) til et kernekriterium for teamdannelse i AI-projekter.

---
## [Quiz efter forel√¶sning](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## Gennemgang & Selvstudie

Kurser og b√∏ger hj√¶lper med at forst√• kerneetikbegreber og udfordringer, mens case studier og v√¶rkt√∏jer hj√¶lper med anvendte etiske praksisser i virkelighedsn√¶re kontekster. Her er nogle ressourcer at starte med.
* [Maskinl√¶ring for begyndere](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - lektion om retf√¶rdighed fra Microsoft.
* [Principper for ansvarlig AI](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - gratis l√¶ringsforl√∏b fra Microsoft Learn.
* [Etik og datavidenskab](https://resources.oreilly.com/examples/0636920203964) - O'Reilly EBook (M. Loukides, H. Mason m.fl.)
* [Data Science Ethics](https://www.coursera.org/learn/data-science-ethics#syllabus) - online kursus fra University of Michigan.
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - casestudier fra University of Texas.

# Opgave

[Skriv en case study om dataetik](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• n√∏jagtighed, skal det bem√¶rkes, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for misforst√•elser eller fejltolkninger, der m√•tte opst√• som f√∏lge af brugen af denne overs√¶ttelse.