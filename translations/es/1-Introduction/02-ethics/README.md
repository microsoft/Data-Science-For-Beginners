<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "3a34157cc63516eba97c89a0b2f8c3e6",
  "translation_date": "2025-09-03T20:50:17+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "es"
}
-->
# Introducci贸n a la tica de los Datos

|![ Sketchnote por [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| tica en Ciencia de Datos - _Sketchnote por [@nitya](https://twitter.com/nitya)_ |

---

Todos somos ciudadanos de datos viviendo en un mundo dataficado.

Las tendencias del mercado nos indican que para 2022, 1 de cada 3 grandes organizaciones comprar谩 y vender谩 sus datos a trav茅s de [mercados y plataformas de intercambio](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) en l铆nea. Como **desarrolladores de aplicaciones**, encontraremos m谩s f谩cil y econ贸mico integrar conocimientos basados en datos y automatizaci贸n impulsada por algoritmos en las experiencias diarias de los usuarios. Pero a medida que la IA se vuelve omnipresente, tambi茅n necesitaremos comprender los posibles da帽os causados por la [utilizaci贸n indebida](https://www.youtube.com/watch?v=TQHs8SA1qpk) de estos algoritmos a gran escala.

Las tendencias tambi茅n indican que crearemos y consumiremos m谩s de [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) de datos para 2025. Como **cient铆ficos de datos**, esto nos da niveles sin precedentes de acceso a datos personales. Esto significa que podemos construir perfiles de comportamiento de los usuarios e influir en la toma de decisiones de maneras que crean una [ilusi贸n de libre elecci贸n](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) mientras potencialmente dirigimos a los usuarios hacia resultados que preferimos. Tambi茅n plantea preguntas m谩s amplias sobre la privacidad de los datos y la protecci贸n de los usuarios.

La 茅tica de los datos ahora son _l铆mites necesarios_ para la ciencia y la ingenier铆a de datos, ayud谩ndonos a minimizar los posibles da帽os y las consecuencias no deseadas de nuestras acciones impulsadas por datos. El [Ciclo de Hype de Gartner para IA](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifica tendencias relevantes en 茅tica digital, IA responsable y gobernanza de IA como impulsores clave de megatendencias m谩s amplias en torno a la _democratizaci贸n_ y _industrializaci贸n_ de la IA.

![Ciclo de Hype de Gartner para IA - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

En esta lecci贸n, exploraremos el fascinante 谩mbito de la 茅tica de los datos: desde conceptos y desaf铆os fundamentales, hasta estudios de caso y conceptos aplicados de IA como la gobernanza, que ayudan a establecer una cultura 茅tica en equipos y organizaciones que trabajan con datos e IA.

## [Cuestionario previo a la clase](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/2) 

## Definiciones B谩sicas

Comencemos entendiendo la terminolog铆a b谩sica.

La palabra "茅tica" proviene de la [palabra griega "ethikos"](https://en.wikipedia.org/wiki/Ethics) (y su ra铆z "ethos") que significa _car谩cter o naturaleza moral_. 

**tica** trata sobre los valores compartidos y los principios morales que gobiernan nuestro comportamiento en la sociedad. La 茅tica no se basa en leyes, sino en normas ampliamente aceptadas de lo que es "correcto vs. incorrecto". Sin embargo, las consideraciones 茅ticas pueden influir en iniciativas de gobernanza corporativa y regulaciones gubernamentales que crean m谩s incentivos para el cumplimiento.

**tica de los Datos** es una [nueva rama de la 茅tica](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) que "estudia y eval煤a problemas morales relacionados con _datos, algoritmos y pr谩cticas correspondientes_". Aqu铆, **"datos"** se centra en acciones relacionadas con la generaci贸n, registro, curaci贸n, procesamiento, difusi贸n, intercambio y uso; **"algoritmos"** se centra en IA, agentes, aprendizaje autom谩tico y robots; y **"pr谩cticas"** se centra en temas como innovaci贸n responsable, programaci贸n, hacking y c贸digos 茅ticos.

**tica Aplicada** es la [aplicaci贸n pr谩ctica de consideraciones morales](https://en.wikipedia.org/wiki/Applied_ethics). Es el proceso de investigar activamente cuestiones 茅ticas en el contexto de _acciones, productos y procesos del mundo real_, y tomar medidas correctivas para garantizar que estos permanezcan alineados con nuestros valores 茅ticos definidos.

**Cultura tica** trata sobre [_operacionalizar_ la 茅tica aplicada](https://hbr.org/2019/05/how-to-design-an-ethical-organization) para garantizar que nuestros principios y pr谩cticas 茅ticas sean adoptados de manera consistente y escalable en toda la organizaci贸n. Las culturas 茅ticas exitosas definen principios 茅ticos a nivel organizacional, proporcionan incentivos significativos para el cumplimiento y refuerzan las normas 茅ticas alentando y amplificando los comportamientos deseados en todos los niveles de la organizaci贸n.

## Conceptos de tica

En esta secci贸n, discutiremos conceptos como **valores compartidos** (principios) y **desaf铆os 茅ticos** (problemas) para la 茅tica de los datos, y exploraremos **estudios de caso** que te ayudar谩n a entender estos conceptos en contextos del mundo real.

### 1. Principios ticos

Toda estrategia de 茅tica de los datos comienza definiendo _principios 茅ticos_ - los "valores compartidos" que describen comportamientos aceptables y gu铆an acciones conformes en nuestros proyectos de datos e IA. Puedes definirlos a nivel individual o de equipo. Sin embargo, la mayor铆a de las grandes organizaciones los describen en una declaraci贸n de misi贸n o marco de _IA 茅tica_ que se define a nivel corporativo y se aplica de manera consistente en todos los equipos.

**Ejemplo:** La declaraci贸n de misi贸n de [IA Responsable](https://www.microsoft.com/en-us/ai/responsible-ai) de Microsoft dice: _"Estamos comprometidos con el avance de la IA impulsada por principios 茅ticos que ponen a las personas primero"_ - identificando 6 principios 茅ticos en el marco a continuaci贸n:

![IA Responsable en Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Exploremos brevemente estos principios. _Transparencia_ y _responsabilidad_ son valores fundamentales sobre los que se construyen otros principios, as铆 que comencemos all铆:

* [**Responsabilidad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) hace que los practicantes sean _responsables_ de sus operaciones de datos e IA y del cumplimiento de estos principios 茅ticos.
* [**Transparencia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) asegura que las acciones de datos e IA sean _comprensibles_ (interpretables) para los usuarios, explicando el qu茅 y el porqu茅 detr谩s de las decisiones.
* [**Equidad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - se centra en garantizar que la IA trate a _todas las personas_ de manera justa, abordando cualquier sesgo socio-t茅cnico sist茅mico o impl铆cito en los datos y sistemas.
* [**Fiabilidad y Seguridad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - asegura que la IA se comporte de manera _consistente_ con los valores definidos, minimizando posibles da帽os o consecuencias no deseadas.
* [**Privacidad y Seguridad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de comprender el linaje de los datos y proporcionar _privacidad de datos y protecciones relacionadas_ a los usuarios.
* [**Inclusi贸n**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de dise帽ar soluciones de IA con intenci贸n, adapt谩ndolas para satisfacer una _amplia gama de necesidades y capacidades humanas_.

>  Piensa en cu谩l podr铆a ser tu declaraci贸n de misi贸n de 茅tica de los datos. Explora marcos de IA 茅tica de otras organizaciones: aqu铆 hay ejemplos de [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) y [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). 驴Qu茅 valores compartidos tienen en com煤n? 驴C贸mo se relacionan estos principios con el producto de IA o la industria en la que operan?

### 2. Desaf铆os ticos

Una vez que hemos definido los principios 茅ticos, el siguiente paso es evaluar nuestras acciones de datos e IA para ver si se alinean con esos valores compartidos. Piensa en tus acciones en dos categor铆as: _recolecci贸n de datos_ y _dise帽o de algoritmos_. 

En la recolecci贸n de datos, las acciones probablemente involucrar谩n **datos personales** o informaci贸n personalmente identificable (PII) de individuos identificables. Esto incluye [diversos elementos de datos no personales](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) que _colectivamente_ identifican a un individuo. Los desaf铆os 茅ticos pueden relacionarse con _privacidad de datos_, _propiedad de datos_ y temas relacionados como _consentimiento informado_ y _derechos de propiedad intelectual_ para los usuarios.

En el dise帽o de algoritmos, las acciones involucrar谩n la recolecci贸n y curaci贸n de **conjuntos de datos**, luego usarlos para entrenar y desplegar **modelos de datos** que predigan resultados o automaticen decisiones en contextos del mundo real. Los desaf铆os 茅ticos pueden surgir de _sesgos en los conjuntos de datos_, problemas de _calidad de datos_, _injusticia_ y _representaci贸n err贸nea_ en los algoritmos, incluyendo algunos problemas que son sist茅micos por naturaleza.

En ambos casos, los desaf铆os 茅ticos destacan 谩reas donde nuestras acciones pueden entrar en conflicto con nuestros valores compartidos. Para detectar, mitigar, minimizar o eliminar estas preocupaciones, necesitamos hacer preguntas morales de "s铆/no" relacionadas con nuestras acciones y luego tomar medidas correctivas seg煤n sea necesario. Veamos algunos desaf铆os 茅ticos y las preguntas morales que plantean:

#### 2.1 Propiedad de los Datos

La recolecci贸n de datos a menudo involucra datos personales que pueden identificar a los sujetos de los datos. [Propiedad de los datos](https://permission.io/blog/data-ownership) trata sobre el _control_ y los [_derechos de los usuarios_](https://permission.io/blog/data-ownership) relacionados con la creaci贸n, procesamiento y difusi贸n de datos. 

Las preguntas morales que debemos hacer son: 
 * 驴Qui茅n posee los datos? (usuario u organizaci贸n)
 * 驴Qu茅 derechos tienen los sujetos de los datos? (ej.: acceso, eliminaci贸n, portabilidad)
 * 驴Qu茅 derechos tienen las organizaciones? (ej.: rectificar rese帽as maliciosas de usuarios)

#### 2.2 Consentimiento Informado

[Consentimiento informado](https://legaldictionary.net/informed-consent/) define el acto de que los usuarios acepten una acci贸n (como la recolecci贸n de datos) con un _entendimiento completo_ de los hechos relevantes, incluyendo el prop贸sito, los riesgos potenciales y las alternativas. 

Preguntas para explorar aqu铆 son:
 * 驴El usuario (sujeto de los datos) dio permiso para la captura y uso de datos?
 * 驴El usuario entendi贸 el prop贸sito para el cual se capturaron esos datos?
 * 驴El usuario entendi贸 los riesgos potenciales de su participaci贸n?

#### 2.3 Propiedad Intelectual

[Propiedad intelectual](https://en.wikipedia.org/wiki/Intellectual_property) se refiere a creaciones intangibles resultantes de la iniciativa humana, que pueden _tener valor econ贸mico_ para individuos o empresas. 

Preguntas para explorar aqu铆 son:
 * 驴Los datos recolectados tienen valor econ贸mico para un usuario o empresa?
 * 驴El **usuario** tiene propiedad intelectual aqu铆?
 * 驴La **organizaci贸n** tiene propiedad intelectual aqu铆?
 * Si existen estos derechos, 驴c贸mo los estamos protegiendo?

#### 2.4 Privacidad de los Datos

[Privacidad de los datos](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) o privacidad de la informaci贸n se refiere a la preservaci贸n de la privacidad del usuario y la protecci贸n de su identidad con respecto a informaci贸n personalmente identificable. 

Preguntas para explorar aqu铆 son:
 * 驴Los datos (personales) de los usuarios est谩n protegidos contra hackeos y filtraciones?
 * 驴Los datos de los usuarios son accesibles solo para usuarios y contextos autorizados?
 * 驴Se preserva el anonimato de los usuarios cuando los datos se comparten o difunden?
 * 驴Se puede desidentificar a un usuario de conjuntos de datos anonimizados?

#### 2.5 Derecho al Olvido

El [Derecho al Olvido](https://en.wikipedia.org/wiki/Right_to_be_forgotten) o [Derecho a la Eliminaci贸n](https://www.gdpreu.org/right-to-be-forgotten/) proporciona protecci贸n adicional de datos personales a los usuarios. Espec铆ficamente, da a los usuarios el derecho a solicitar la eliminaci贸n o eliminaci贸n de datos personales de b煤squedas en Internet y otros lugares, _bajo circunstancias espec铆ficas_, permiti茅ndoles un nuevo comienzo en l铆nea sin que se les juzgue por acciones pasadas.

Preguntas para explorar aqu铆 son:
 * 驴El sistema permite a los sujetos de los datos solicitar la eliminaci贸n?
 * 驴Deber铆a el retiro del consentimiento del usuario activar la eliminaci贸n autom谩tica?
 * 驴Se recolectaron datos sin consentimiento o por medios ilegales?
 * 驴Cumplimos con las regulaciones gubernamentales para la privacidad de los datos?

#### 2.6 Sesgo en los Conjuntos de Datos

El sesgo en los conjuntos de datos o [sesgo de recolecci贸n](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) trata sobre seleccionar un subconjunto _no representativo_ de datos para el desarrollo de algoritmos, creando potencialmente injusticia en los resultados para diversos grupos. Tipos de sesgo incluyen sesgo de selecci贸n o muestreo, sesgo de voluntarios y sesgo de instrumentos. 

Preguntas para explorar aqu铆 son:
 * 驴Reclutamos un conjunto representativo de sujetos de datos?
 * 驴Probamos nuestro conjunto de datos recolectado o curado para diversos sesgos?
 * 驴Podemos mitigar o eliminar los sesgos descubiertos?

#### 2.7 Calidad de los Datos

[Calidad de los datos](https://lakefs.io/data-quality-testing/) analiza la validez del conjunto de datos curado utilizado para desarrollar nuestros algoritmos, verificando si las caracter铆sticas y los registros cumplen con los requisitos para el nivel de precisi贸n y consistencia necesario para nuestro prop贸sito de IA.

Preguntas para explorar aqu铆 son:
 * 驴Capturamos caracter铆sticas v谩lidas para nuestro caso de uso?
 * 驴Se capturaron datos de manera consistente en diversas fuentes de datos?
 * 驴El conjunto de datos est谩 completo para diversas condiciones o escenarios?
 * 驴La informaci贸n capturada refleja la realidad con precisi贸n?
[Algorithm Fairness](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) verifica si el dise帽o del algoritmo discrimina sistem谩ticamente contra subgrupos espec铆ficos de sujetos de datos, lo que puede llevar a [da帽os potenciales](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) en _asignaci贸n_ (donde se niegan o retienen recursos para ese grupo) y _calidad del servicio_ (donde la IA no es tan precisa para algunos subgrupos como lo es para otros).

Preguntas para explorar aqu铆 son:
 * 驴Evaluamos la precisi贸n del modelo para diversos subgrupos y condiciones?
 * 驴Examinamos el sistema en busca de posibles da帽os (por ejemplo, estereotipos)?
 * 驴Podemos revisar los datos o reentrenar los modelos para mitigar los da帽os identificados?

Explora recursos como [listas de verificaci贸n de equidad en IA](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) para aprender m谩s.

#### 2.9 Representaci贸n Err贸nea

[Representaci贸n err贸nea de datos](https://www.sciencedirect.com/topics/computer-science/misrepresentation) se trata de preguntarse si estamos comunicando ideas a partir de datos reportados honestamente de manera enga帽osa para apoyar una narrativa deseada.

Preguntas para explorar aqu铆 son:
 * 驴Estamos reportando datos incompletos o inexactos?
 * 驴Estamos visualizando datos de una manera que lleva a conclusiones enga帽osas?
 * 驴Estamos utilizando t茅cnicas estad铆sticas selectivas para manipular resultados?
 * 驴Existen explicaciones alternativas que puedan ofrecer una conclusi贸n diferente?

#### 2.10 Libre Elecci贸n
La [ilusi贸n de libre elecci贸n](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) ocurre cuando las "arquitecturas de elecci贸n" del sistema utilizan algoritmos de toma de decisiones para influir a las personas hacia un resultado preferido mientras aparentan darles opciones y control. Estos [patrones oscuros](https://www.darkpatterns.org/) pueden causar da帽os sociales y econ贸micos a los usuarios. Dado que las decisiones de los usuarios impactan los perfiles de comportamiento, estas acciones potencialmente impulsan elecciones futuras que pueden amplificar o extender el impacto de estos da帽os.

Preguntas para explorar aqu铆 son:
 * 驴El usuario entendi贸 las implicaciones de tomar esa decisi贸n?
 * 驴El usuario estaba consciente de las opciones (alternativas) y los pros y contras de cada una?
 * 驴Puede el usuario revertir una decisi贸n automatizada o influenciada m谩s tarde?

### 3. Estudios de Caso

Para contextualizar estos desaf铆os 茅ticos en el mundo real, es 煤til observar estudios de caso que destacan los posibles da帽os y consecuencias para individuos y la sociedad cuando se pasan por alto estas violaciones 茅ticas.

Aqu铆 hay algunos ejemplos:

| Desaf铆o tico | Estudio de Caso  | 
|--- |--- |
| **Consentimiento Informado** | 1972 - [Estudio de S铆filis de Tuskegee](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - A los hombres afroamericanos que participaron en el estudio se les prometi贸 atenci贸n m茅dica gratuita _pero fueron enga帽ados_ por investigadores que no informaron a los sujetos sobre su diagn贸stico ni sobre la disponibilidad de tratamiento. Muchos sujetos murieron y sus parejas o hijos fueron afectados; el estudio dur贸 40 a帽os. | 
| **Privacidad de Datos** |  2007 - El [premio de datos de Netflix](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) proporcion贸 a los investigadores _10M clasificaciones de pel铆culas anonimizadas de 50K clientes_ para ayudar a mejorar los algoritmos de recomendaci贸n. Sin embargo, los investigadores pudieron correlacionar datos anonimizados con datos personalmente identificables en _conjuntos de datos externos_ (por ejemplo, comentarios de IMDb), efectivamente "desanonimizando" a algunos suscriptores de Netflix.|
| **Sesgo en la Recolecci贸n**  | 2013 - La ciudad de Boston [desarroll贸 Street Bump](https://www.boston.gov/transportation/street-bump), una aplicaci贸n que permit铆a a los ciudadanos reportar baches, proporcionando mejores datos sobre carreteras para encontrar y solucionar problemas. Sin embargo, [las personas de grupos de ingresos m谩s bajos ten铆an menos acceso a autos y tel茅fonos](https://hbr.org/2013/04/the-hidden-biases-in-big-data), haciendo que sus problemas de carreteras fueran invisibles en esta aplicaci贸n. Los desarrolladores trabajaron con acad茅micos para abordar problemas de _acceso equitativo y brechas digitales_ para mayor equidad. |
| **Equidad Algor铆tmica**  | 2018 - El MIT [Estudio Gender Shades](http://gendershades.org/overview.html) evalu贸 la precisi贸n de productos de clasificaci贸n de g茅nero por IA, exponiendo brechas en la precisi贸n para mujeres y personas de color. Una [tarjeta de cr茅dito de Apple en 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) parec铆a ofrecer menos cr茅dito a mujeres que a hombres. Ambos casos ilustraron problemas de sesgo algor铆tmico que llevaron a da帽os socioecon贸micos.|
| **Representaci贸n Err贸nea de Datos** | 2020 - El [Departamento de Salud P煤blica de Georgia public贸 gr谩ficos de COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) que parec铆an enga帽ar a los ciudadanos sobre las tendencias de casos confirmados con un orden no cronol贸gico en el eje x. Esto ilustra la representaci贸n err贸nea mediante trucos de visualizaci贸n. |
| **Ilusi贸n de Libre Elecci贸n** | 2020 - La aplicaci贸n de aprendizaje [ABCmouse pag贸 $10M para resolver una queja de la FTC](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) donde los padres quedaron atrapados pagando suscripciones que no pod铆an cancelar. Esto ilustra patrones oscuros en arquitecturas de elecci贸n, donde los usuarios fueron influenciados hacia decisiones potencialmente da帽inas. |
| **Privacidad de Datos y Derechos de Usuarios** | 2021 - La [brecha de datos de Facebook](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) expuso datos de 530M usuarios, resultando en un acuerdo de $5B con la FTC. Sin embargo, se neg贸 a notificar a los usuarios sobre la brecha, violando los derechos de los usuarios en torno a la transparencia y el acceso a los datos. |

驴Quieres explorar m谩s estudios de caso? Consulta estos recursos:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - dilemas 茅ticos en diversas industrias. 
* [Curso de tica en Ciencia de Datos](https://www.coursera.org/learn/data-science-ethics#syllabus) - estudios de caso emblem谩ticos explorados.
* [Donde las cosas han salido mal](https://deon.drivendata.org/examples/) - lista de verificaci贸n de Deon con ejemplos.


>  Piensa en los estudios de caso que has visto: 驴has experimentado o te ha afectado un desaf铆o 茅tico similar en tu vida? 驴Puedes pensar en al menos un estudio de caso adicional que ilustre uno de los desaf铆os 茅ticos que hemos discutido en esta secci贸n?

## tica Aplicada

Hemos hablado sobre conceptos 茅ticos, desaf铆os y estudios de caso en contextos del mundo real. Pero, 驴c贸mo comenzamos a _aplicar_ principios y pr谩cticas 茅ticas en nuestros proyectos? 驴Y c贸mo _operacionalizamos_ estas pr谩cticas para una mejor gobernanza? Exploremos algunas soluciones del mundo real:

### 1. C贸digos Profesionales

Los c贸digos profesionales ofrecen una opci贸n para que las organizaciones "incentiven" a los miembros a apoyar sus principios 茅ticos y declaraci贸n de misi贸n. Los c贸digos son _directrices morales_ para el comportamiento profesional, ayudando a los empleados o miembros a tomar decisiones que se alineen con los principios de su organizaci贸n. Solo son efectivos si los miembros cumplen voluntariamente; sin embargo, muchas organizaciones ofrecen recompensas y sanciones adicionales para motivar el cumplimiento.

Ejemplos incluyen:

 * [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) C贸digo de tica
 * [Data Science Association](http://datascienceassn.org/code-of-conduct.html) C贸digo de Conducta (creado en 2013)
 * [ACM C贸digo de tica y Conducta Profesional](https://www.acm.org/code-of-ethics) (desde 1993)

>  驴Perteneces a una organizaci贸n profesional de ingenier铆a o ciencia de datos? Explora su sitio para ver si definen un c贸digo profesional de 茅tica. 驴Qu茅 dice esto sobre sus principios 茅ticos? 驴C贸mo est谩n "incentivando" a los miembros para seguir el c贸digo?

### 2. Listas de Verificaci贸n ticas

Mientras que los c贸digos profesionales definen el _comportamiento 茅tico_ requerido de los practicantes, [tienen limitaciones conocidas](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) en su aplicaci贸n, particularmente en proyectos a gran escala. En cambio, muchos expertos en ciencia de datos [abogan por listas de verificaci贸n](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), que pueden **conectar principios con pr谩cticas** de manera m谩s determinista y accionable.

Las listas de verificaci贸n convierten preguntas en tareas de "s铆/no" que pueden ser operacionalizadas, permitiendo que se rastreen como parte de los flujos de trabajo est谩ndar de lanzamiento de productos.

Ejemplos incluyen:
 * [Deon](https://deon.drivendata.org/) - una lista de verificaci贸n de 茅tica en datos de prop贸sito general creada a partir de [recomendaciones de la industria](https://deon.drivendata.org/#checklist-citations) con una herramienta de l铆nea de comandos para f谩cil integraci贸n.
 * [Lista de Verificaci贸n de Auditor铆a de Privacidad](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - proporciona orientaci贸n general para pr谩cticas de manejo de informaci贸n desde perspectivas legales y sociales.
 * [Lista de Verificaci贸n de Equidad en IA](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - creada por practicantes de IA para apoyar la adopci贸n e integraci贸n de verificaciones de equidad en los ciclos de desarrollo de IA.
 * [22 preguntas para 茅tica en datos e IA](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - marco m谩s abierto, estructurado para la exploraci贸n inicial de problemas 茅ticos en dise帽o, implementaci贸n y contextos organizacionales.

### 3. Regulaciones ticas

La 茅tica se trata de definir valores compartidos y hacer lo correcto _voluntariamente_. **Cumplimiento** se trata de _seguir la ley_ si est谩 definida. **Gobernanza** abarca todas las formas en que las organizaciones operan para hacer cumplir principios 茅ticos y cumplir con las leyes establecidas.

Hoy en d铆a, la gobernanza toma dos formas dentro de las organizaciones. Primero, se trata de definir principios de **IA 茅tica** y establecer pr谩cticas para operacionalizar la adopci贸n en todos los proyectos relacionados con IA en la organizaci贸n. Segundo, se trata de cumplir con todas las regulaciones de **protecci贸n de datos** definidas por el gobierno para las regiones en las que opera.

Ejemplos de regulaciones de protecci贸n de datos y privacidad:

 * `1974`, [Ley de Privacidad de EE.UU.](https://www.justice.gov/opcl/privacy-act-1974) - regula la recolecci贸n, uso y divulgaci贸n de informaci贸n personal por parte del _gobierno federal_.
 * `1996`, [Ley de Portabilidad y Responsabilidad de Seguros de Salud de EE.UU. (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - protege datos personales de salud.
 * `1998`, [Ley de Protecci贸n de Privacidad en L铆nea de los Ni帽os de EE.UU. (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - protege la privacidad de datos de ni帽os menores de 13 a帽os.
 * `2018`, [Reglamento General de Protecci贸n de Datos (GDPR)](https://gdpr-info.eu/) - proporciona derechos de usuario, protecci贸n de datos y privacidad.
 * `2018`, [Ley de Privacidad del Consumidor de California (CCPA)](https://www.oag.ca.gov/privacy/ccpa) - otorga a los consumidores m谩s _derechos_ sobre sus datos personales.
 * `2021`, [Ley de Protecci贸n de Informaci贸n Personal de China](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) - una de las regulaciones de privacidad de datos en l铆nea m谩s fuertes a nivel mundial.

>  La Uni贸n Europea defini贸 el GDPR (Reglamento General de Protecci贸n de Datos), que sigue siendo una de las regulaciones de privacidad de datos m谩s influyentes hoy en d铆a. 驴Sab铆as que tambi茅n define [8 derechos de usuario](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) para proteger la privacidad digital y los datos personales de los ciudadanos? Aprende cu谩les son y por qu茅 son importantes.

### 4. Cultura tica

Nota que existe una brecha intangible entre el _cumplimiento_ (hacer lo suficiente para cumplir "la letra de la ley") y abordar [problemas sist茅micos](https://www.coursera.org/learn/data-science-ethics/home/week/4) (como la osificaci贸n, la asimetr铆a de informaci贸n y la desigualdad en la distribuci贸n) que pueden acelerar la instrumentalizaci贸n de la IA.

Esto 煤ltimo requiere [enfoques colaborativos para definir culturas 茅ticas](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) que construyan conexiones emocionales y valores compartidos consistentes _entre organizaciones_ en la industria. Esto exige m谩s [culturas 茅ticas formalizadas en datos](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) en las organizaciones, permitiendo que _cualquiera_ [active el cord贸n Andon](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (para plantear preocupaciones 茅ticas temprano en el proceso) y haciendo que las _evaluaciones 茅ticas_ (por ejemplo, en la contrataci贸n) sean un criterio central en la formaci贸n de equipos para proyectos de IA.

---
## [Cuestionario post-lectura](https://ff-quizzes.netlify.app/en/ds/) 
## Revisi贸n y Autoestudio 

Los cursos y libros ayudan a comprender los conceptos 茅ticos fundamentales y los desaf铆os, mientras que los estudios de caso y herramientas ayudan con las pr谩cticas 茅ticas aplicadas en contextos del mundo real. Aqu铆 hay algunos recursos para comenzar.

* [Machine Learning For Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - lecci贸n sobre equidad, de Microsoft.
* [Principios de IA Responsable](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - ruta de aprendizaje gratuita de Microsoft Learn.
* [tica y Ciencia de Datos](https://resources.oreilly.com/examples/0636920203964) - EBook de O'Reilly (M. Loukides, H. Mason et. al)
* [tica en Ciencia de Datos](https://www.coursera.org/learn/data-science-ethics#syllabus) - curso en l铆nea de la Universidad de Michigan.
* [tica Desenredada](https://ethicsunwrapped.utexas.edu/case-studies) - estudios de caso de la Universidad de Texas.

# Tarea

[Escribe un Estudio de Caso sobre tica de Datos](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci贸n autom谩tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por garantizar la precisi贸n, tenga en cuenta que las traducciones automatizadas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci贸n cr铆tica, se recomienda una traducci贸n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err贸neas que puedan surgir del uso de esta traducci贸n.