<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1341f6da63d434f5ba31b08ea951b02c",
  "translation_date": "2025-09-05T13:45:56+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "es"
}
-->
# Introducci√≥n a la √âtica de los Datos

|![ Sketchnote por [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| √âtica en Ciencia de Datos - _Sketchnote por [@nitya](https://twitter.com/nitya)_ |

---

Todos somos ciudadanos de datos viviendo en un mundo dataficado.

Las tendencias del mercado nos indican que para 2022, 1 de cada 3 grandes organizaciones comprar√° y vender√° sus datos a trav√©s de [mercados y plataformas de intercambio](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) en l√≠nea. Como **desarrolladores de aplicaciones**, encontraremos m√°s f√°cil y econ√≥mico integrar conocimientos basados en datos y automatizaci√≥n impulsada por algoritmos en las experiencias diarias de los usuarios. Pero a medida que la IA se vuelve omnipresente, tambi√©n necesitaremos comprender los posibles da√±os causados por la [utilizaci√≥n indebida](https://www.youtube.com/watch?v=TQHs8SA1qpk) de estos algoritmos a gran escala.

Las tendencias tambi√©n indican que crearemos y consumiremos m√°s de [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) de datos para 2025. Como **cient√≠ficos de datos**, esto nos da niveles sin precedentes de acceso a datos personales. Esto significa que podemos construir perfiles de comportamiento de los usuarios e influir en la toma de decisiones de maneras que crean una [ilusi√≥n de libre elecci√≥n](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice), mientras potencialmente dirigimos a los usuarios hacia resultados que preferimos. Tambi√©n plantea preguntas m√°s amplias sobre la privacidad de los datos y la protecci√≥n de los usuarios.

La √©tica de los datos ahora son _l√≠mites necesarios_ para la ciencia y la ingenier√≠a de datos, ayud√°ndonos a minimizar los posibles da√±os y las consecuencias no deseadas de nuestras acciones impulsadas por datos. El [Ciclo de Hype de Gartner para IA](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifica tendencias relevantes en √©tica digital, IA responsable y gobernanza de IA como impulsores clave de megatendencias m√°s amplias en torno a la _democratizaci√≥n_ y _industrializaci√≥n_ de la IA.

![Ciclo de Hype de Gartner para IA - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

En esta lecci√≥n, exploraremos el fascinante √°mbito de la √©tica de los datos: desde conceptos y desaf√≠os fundamentales, hasta estudios de caso y conceptos aplicados de IA como la gobernanza, que ayudan a establecer una cultura √©tica en equipos y organizaciones que trabajan con datos e IA.

## [Cuestionario previo a la clase](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Definiciones B√°sicas

Comencemos entendiendo la terminolog√≠a b√°sica.

La palabra "√©tica" proviene de la [palabra griega "ethikos"](https://en.wikipedia.org/wiki/Ethics) (y su ra√≠z "ethos") que significa _car√°cter o naturaleza moral_. 

**√âtica** trata de los valores compartidos y principios morales que gobiernan nuestro comportamiento en la sociedad. La √©tica no se basa en leyes, sino en normas ampliamente aceptadas de lo que es "correcto vs. incorrecto". Sin embargo, las consideraciones √©ticas pueden influir en iniciativas de gobernanza corporativa y regulaciones gubernamentales que crean m√°s incentivos para el cumplimiento.

**√âtica de los Datos** es una [nueva rama de la √©tica](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) que "estudia y eval√∫a problemas morales relacionados con _datos, algoritmos y pr√°cticas correspondientes_". Aqu√≠, **"datos"** se centra en acciones relacionadas con la generaci√≥n, registro, curaci√≥n, procesamiento, difusi√≥n, intercambio y uso; **"algoritmos"** se centra en IA, agentes, aprendizaje autom√°tico y robots; y **"pr√°cticas"** se centra en temas como innovaci√≥n responsable, programaci√≥n, hacking y c√≥digos √©ticos.

**√âtica Aplicada** es la [aplicaci√≥n pr√°ctica de consideraciones morales](https://en.wikipedia.org/wiki/Applied_ethics). Es el proceso de investigar activamente cuestiones √©ticas en el contexto de _acciones, productos y procesos del mundo real_, y tomar medidas correctivas para garantizar que estos permanezcan alineados con nuestros valores √©ticos definidos.

**Cultura √âtica** trata de [_operacionalizar_ la √©tica aplicada](https://hbr.org/2019/05/how-to-design-an-ethical-organization) para garantizar que nuestros principios y pr√°cticas √©ticas sean adoptados de manera consistente y escalable en toda la organizaci√≥n. Las culturas √©ticas exitosas definen principios √©ticos a nivel organizacional, proporcionan incentivos significativos para el cumplimiento y refuerzan las normas √©ticas alentando y amplificando los comportamientos deseados en todos los niveles de la organizaci√≥n.

## Conceptos de √âtica

En esta secci√≥n, discutiremos conceptos como **valores compartidos** (principios) y **desaf√≠os √©ticos** (problemas) para la √©tica de los datos, y exploraremos **estudios de caso** que te ayudar√°n a entender estos conceptos en contextos del mundo real.

### 1. Principios √âticos

Toda estrategia de √©tica de los datos comienza definiendo _principios √©ticos_ - los "valores compartidos" que describen comportamientos aceptables y gu√≠an acciones conformes en nuestros proyectos de datos e IA. Puedes definirlos a nivel individual o de equipo. Sin embargo, la mayor√≠a de las grandes organizaciones los describen en una declaraci√≥n de misi√≥n o marco de _IA √©tica_ que se define a nivel corporativo y se aplica de manera consistente en todos los equipos.

**Ejemplo:** La declaraci√≥n de misi√≥n de [IA Responsable](https://www.microsoft.com/en-us/ai/responsible-ai) de Microsoft dice: _"Estamos comprometidos con el avance de la IA impulsada por principios √©ticos que ponen a las personas primero"_ - identificando 6 principios √©ticos en el marco a continuaci√≥n:

![IA Responsable en Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Exploremos brevemente estos principios. _Transparencia_ y _responsabilidad_ son valores fundamentales sobre los que se construyen otros principios, as√≠ que comencemos all√≠:

* [**Responsabilidad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) hace que los practicantes sean _responsables_ de sus operaciones de datos e IA, y del cumplimiento de estos principios √©ticos.
* [**Transparencia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) asegura que las acciones de datos e IA sean _comprensibles_ (interpretables) para los usuarios, explicando el qu√© y el porqu√© detr√°s de las decisiones.
* [**Equidad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - se centra en garantizar que la IA trate a _todas las personas_ de manera justa, abordando cualquier sesgo socio-t√©cnico sist√©mico o impl√≠cito en los datos y sistemas.
* [**Fiabilidad y Seguridad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - asegura que la IA se comporte de manera _consistente_ con los valores definidos, minimizando posibles da√±os o consecuencias no deseadas.
* [**Privacidad y Seguridad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de entender el linaje de los datos y proporcionar _privacidad y protecciones relacionadas_ a los usuarios.
* [**Inclusi√≥n**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de dise√±ar soluciones de IA con intenci√≥n, adapt√°ndolas para satisfacer una _amplia gama de necesidades y capacidades humanas_.

> üö® Piensa en cu√°l podr√≠a ser tu declaraci√≥n de misi√≥n de √©tica de los datos. Explora marcos de IA √©tica de otras organizaciones - aqu√≠ hay ejemplos de [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles), y [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). ¬øQu√© valores compartidos tienen en com√∫n? ¬øC√≥mo se relacionan estos principios con el producto de IA o la industria en la que operan?

### 2. Desaf√≠os √âticos

Una vez que hemos definido los principios √©ticos, el siguiente paso es evaluar nuestras acciones de datos e IA para ver si se alinean con esos valores compartidos. Piensa en tus acciones en dos categor√≠as: _recolecci√≥n de datos_ y _dise√±o de algoritmos_. 

En la recolecci√≥n de datos, las acciones probablemente involucrar√°n **datos personales** o informaci√≥n personalmente identificable (PII) de individuos identificables. Esto incluye [diversos elementos de datos no personales](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) que _colectivamente_ identifican a un individuo. Los desaf√≠os √©ticos pueden relacionarse con _privacidad de datos_, _propiedad de datos_, y temas relacionados como _consentimiento informado_ y _derechos de propiedad intelectual_ para los usuarios.

En el dise√±o de algoritmos, las acciones involucrar√°n la recolecci√≥n y curaci√≥n de **conjuntos de datos**, luego utilizarlos para entrenar y desplegar **modelos de datos** que predigan resultados o automaticen decisiones en contextos del mundo real. Los desaf√≠os √©ticos pueden surgir de _sesgos en los conjuntos de datos_, problemas de _calidad de datos_, _injusticia_, y _representaci√≥n err√≥nea_ en los algoritmos, incluyendo algunos problemas que son sist√©micos por naturaleza.

En ambos casos, los desaf√≠os √©ticos destacan √°reas donde nuestras acciones pueden entrar en conflicto con nuestros valores compartidos. Para detectar, mitigar, minimizar o eliminar estas preocupaciones, necesitamos hacer preguntas morales de "s√≠/no" relacionadas con nuestras acciones y luego tomar medidas correctivas seg√∫n sea necesario. Veamos algunos desaf√≠os √©ticos y las preguntas morales que plantean:

#### 2.1 Propiedad de los Datos

La recolecci√≥n de datos a menudo involucra datos personales que pueden identificar a los sujetos de los datos. [La propiedad de los datos](https://permission.io/blog/data-ownership) trata del _control_ y los [_derechos de los usuarios_](https://permission.io/blog/data-ownership) relacionados con la creaci√≥n, procesamiento y difusi√≥n de datos. 

Las preguntas morales que debemos hacer son: 
 * ¬øQui√©n posee los datos? (usuario u organizaci√≥n)
 * ¬øQu√© derechos tienen los sujetos de los datos? (ej.: acceso, eliminaci√≥n, portabilidad)
 * ¬øQu√© derechos tienen las organizaciones? (ej.: rectificar rese√±as maliciosas de usuarios)

#### 2.2 Consentimiento Informado

[El consentimiento informado](https://legaldictionary.net/informed-consent/) define el acto de que los usuarios acepten una acci√≥n (como la recolecci√≥n de datos) con un _entendimiento completo_ de los hechos relevantes, incluyendo el prop√≥sito, los riesgos potenciales y las alternativas. 

Preguntas para explorar aqu√≠ son:
 * ¬øEl usuario (sujeto de los datos) dio permiso para la captura y uso de datos?
 * ¬øEl usuario entendi√≥ el prop√≥sito para el cual se capturaron esos datos?
 * ¬øEl usuario entendi√≥ los riesgos potenciales de su participaci√≥n?

#### 2.3 Propiedad Intelectual

[La propiedad intelectual](https://en.wikipedia.org/wiki/Intellectual_property) se refiere a creaciones intangibles resultantes de la iniciativa humana, que pueden _tener valor econ√≥mico_ para individuos o empresas. 

Preguntas para explorar aqu√≠ son:
 * ¬øLos datos recolectados tienen valor econ√≥mico para un usuario o empresa?
 * ¬øEl **usuario** tiene propiedad intelectual aqu√≠?
 * ¬øLa **organizaci√≥n** tiene propiedad intelectual aqu√≠?
 * Si existen estos derechos, ¬øc√≥mo los estamos protegiendo?

#### 2.4 Privacidad de los Datos

[La privacidad de los datos](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) o privacidad de la informaci√≥n se refiere a la preservaci√≥n de la privacidad del usuario y la protecci√≥n de su identidad con respecto a la informaci√≥n personalmente identificable. 

Preguntas para explorar aqu√≠ son:
 * ¬øLos datos personales de los usuarios est√°n protegidos contra hackeos y filtraciones?
 * ¬øLos datos de los usuarios son accesibles solo para usuarios y contextos autorizados?
 * ¬øSe preserva el anonimato de los usuarios cuando los datos se comparten o difunden?
 * ¬øSe puede desidentificar a un usuario de conjuntos de datos anonimizados?

#### 2.5 Derecho al Olvido

El [Derecho al Olvido](https://en.wikipedia.org/wiki/Right_to_be_forgotten) o [Derecho de Eliminaci√≥n](https://www.gdpreu.org/right-to-be-forgotten/) proporciona protecci√≥n adicional de datos personales a los usuarios. Espec√≠ficamente, da a los usuarios el derecho de solicitar la eliminaci√≥n o eliminaci√≥n de datos personales de b√∫squedas en Internet y otros lugares, _bajo circunstancias espec√≠ficas_, permiti√©ndoles un nuevo comienzo en l√≠nea sin que se les juzgue por acciones pasadas.

Preguntas para explorar aqu√≠ son:
 * ¬øEl sistema permite a los sujetos de los datos solicitar la eliminaci√≥n?
 * ¬øDeber√≠a el retiro del consentimiento del usuario activar la eliminaci√≥n autom√°tica?
 * ¬øSe recolectaron datos sin consentimiento o por medios ilegales?
 * ¬øCumplimos con las regulaciones gubernamentales sobre privacidad de datos?

#### 2.6 Sesgo en los Conjuntos de Datos

El sesgo en los conjuntos de datos o [sesgo de recolecci√≥n](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) trata de seleccionar un subconjunto _no representativo_ de datos para el desarrollo de algoritmos, creando potencialmente injusticia en los resultados para diversos grupos. Tipos de sesgo incluyen sesgo de selecci√≥n o muestreo, sesgo de voluntarios y sesgo de instrumentos. 

Preguntas para explorar aqu√≠ son:
 * ¬øReclutamos un conjunto representativo de sujetos de datos?
 * ¬øProbamos nuestro conjunto de datos recolectado o curado para diversos sesgos?
 * ¬øPodemos mitigar o eliminar los sesgos descubiertos?

#### 2.7 Calidad de los Datos

[La calidad de los datos](https://lakefs.io/data-quality-testing/) analiza la validez del conjunto de datos curado utilizado para desarrollar nuestros algoritmos, verificando si las caracter√≠sticas y registros cumplen con los requisitos para el nivel de precisi√≥n y consistencia necesario para nuestro prop√≥sito de IA.

Preguntas para explorar aqu√≠ son:
 * ¬øCapturamos caracter√≠sticas v√°lidas para nuestro caso de uso?
 * ¬øSe capturaron datos de manera consistente en diversas fuentes de datos?
 * ¬øEl conjunto de datos est√° completo para diversas condiciones o escenarios?
 * ¬øLa informaci√≥n capturada refleja la realidad con precisi√≥n?

#### 2.8 Equidad en los Algoritmos
[Equidad Algor√≠tmica](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) verifica si el dise√±o del algoritmo discrimina sistem√°ticamente contra subgrupos espec√≠ficos de sujetos de datos, lo que puede llevar a [posibles da√±os](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) en la _asignaci√≥n_ (donde se niegan o retienen recursos a ese grupo) y en la _calidad del servicio_ (donde la IA no es tan precisa para algunos subgrupos como lo es para otros).

Preguntas para explorar aqu√≠ son:
 * ¬øEvaluamos la precisi√≥n del modelo para diversos subgrupos y condiciones?
 * ¬øExaminamos el sistema en busca de posibles da√±os (por ejemplo, estereotipos)?
 * ¬øPodemos revisar los datos o reentrenar los modelos para mitigar los da√±os identificados?

Explora recursos como [listas de verificaci√≥n de equidad en IA](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) para aprender m√°s.

#### 2.9 Representaci√≥n Err√≥nea

[Representaci√≥n Err√≥nea de Datos](https://www.sciencedirect.com/topics/computer-science/misrepresentation) se refiere a preguntarnos si estamos comunicando conocimientos de datos reportados de manera honesta, pero de forma enga√±osa, para respaldar una narrativa deseada.

Preguntas para explorar aqu√≠ son:
 * ¬øEstamos reportando datos incompletos o inexactos?
 * ¬øEstamos visualizando datos de una manera que lleva a conclusiones enga√±osas?
 * ¬øEstamos utilizando t√©cnicas estad√≠sticas selectivas para manipular resultados?
 * ¬øExisten explicaciones alternativas que puedan ofrecer una conclusi√≥n diferente?

#### 2.10 Libre Elecci√≥n
La [Ilusi√≥n de Libre Elecci√≥n](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) ocurre cuando las "arquitecturas de elecci√≥n" del sistema utilizan algoritmos de toma de decisiones para influir a las personas hacia un resultado preferido, mientras aparentan darles opciones y control. Estos [patrones oscuros](https://www.darkpatterns.org/) pueden causar da√±os sociales y econ√≥micos a los usuarios. Dado que las decisiones de los usuarios impactan los perfiles de comportamiento, estas acciones pueden potencialmente impulsar elecciones futuras que amplifiquen o extiendan el impacto de estos da√±os.

Preguntas para explorar aqu√≠ son:
 * ¬øEl usuario entendi√≥ las implicaciones de tomar esa decisi√≥n?
 * ¬øEl usuario estaba al tanto de las opciones (alternativas) y los pros y contras de cada una?
 * ¬øPuede el usuario revertir una decisi√≥n automatizada o influenciada m√°s tarde?

### 3. Estudios de Caso

Para contextualizar estos desaf√≠os √©ticos en el mundo real, es √∫til analizar estudios de caso que destacan los posibles da√±os y consecuencias para individuos y la sociedad cuando se pasan por alto estas violaciones √©ticas.

Aqu√≠ hay algunos ejemplos:

| Desaf√≠o √âtico | Estudio de Caso  | 
|--- |--- |
| **Consentimiento Informado** | 1972 - [Estudio de S√≠filis de Tuskegee](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - A los hombres afroamericanos que participaron en el estudio se les prometi√≥ atenci√≥n m√©dica gratuita _pero fueron enga√±ados_ por investigadores que no les informaron sobre su diagn√≥stico ni sobre la disponibilidad de tratamiento. Muchos murieron y sus parejas o hijos se vieron afectados; el estudio dur√≥ 40 a√±os. | 
| **Privacidad de Datos** |  2007 - El [premio de datos de Netflix](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) proporcion√≥ a los investigadores _10M de clasificaciones de pel√≠culas anonimizadas de 50K clientes_ para mejorar los algoritmos de recomendaci√≥n. Sin embargo, los investigadores pudieron correlacionar datos anonimizados con datos personales en _conjuntos de datos externos_ (por ejemplo, comentarios en IMDb), "desanonimizando" efectivamente a algunos suscriptores de Netflix.|
| **Sesgo en la Recolecci√≥n de Datos**  | 2013 - La ciudad de Boston [desarroll√≥ Street Bump](https://www.boston.gov/transportation/street-bump), una app que permit√≠a a los ciudadanos reportar baches, proporcionando mejores datos sobre carreteras para encontrar y solucionar problemas. Sin embargo, [las personas de grupos de ingresos m√°s bajos ten√≠an menos acceso a autos y tel√©fonos](https://hbr.org/2013/04/the-hidden-biases-in-big-data), haciendo que sus problemas de carreteras fueran invisibles en esta app. Los desarrolladores trabajaron con acad√©micos para abordar _problemas de acceso equitativo y brechas digitales_ por equidad. |
| **Equidad Algor√≠tmica**  | 2018 - El MIT [Estudio Gender Shades](http://gendershades.org/overview.html) evalu√≥ la precisi√≥n de productos de IA para clasificaci√≥n de g√©nero, exponiendo brechas en precisi√≥n para mujeres y personas de color. Una [tarjeta de cr√©dito de Apple en 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) parec√≠a ofrecer menos cr√©dito a mujeres que a hombres. Ambos casos ilustraron problemas de sesgo algor√≠tmico que llevaron a da√±os socioecon√≥micos.|
| **Representaci√≥n Err√≥nea de Datos** | 2020 - El [Departamento de Salud P√∫blica de Georgia public√≥ gr√°ficos de COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) que parec√≠an enga√±ar a los ciudadanos sobre las tendencias de casos confirmados con un eje x no cronol√≥gico. Esto ilustra la representaci√≥n err√≥nea a trav√©s de trucos de visualizaci√≥n. |
| **Ilusi√≥n de Libre Elecci√≥n** | 2020 - La app educativa [ABCmouse pag√≥ $10M para resolver una queja de la FTC](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) donde los padres quedaron atrapados pagando suscripciones que no pod√≠an cancelar. Esto ilustra patrones oscuros en arquitecturas de elecci√≥n, donde los usuarios fueron influenciados hacia decisiones potencialmente da√±inas. |
| **Privacidad de Datos y Derechos de los Usuarios** | 2021 - La [brecha de datos de Facebook](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) expuso datos de 530M de usuarios, resultando en un acuerdo de $5B con la FTC. Sin embargo, se neg√≥ a notificar a los usuarios sobre la brecha, violando los derechos de los usuarios en torno a la transparencia y el acceso a los datos. |

¬øQuieres explorar m√°s estudios de caso? Consulta estos recursos:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - dilemas √©ticos en diversas industrias. 
* [Curso de √âtica en Ciencia de Datos](https://www.coursera.org/learn/data-science-ethics#syllabus) - estudios de caso destacados.
* [Donde las cosas han salido mal](https://deon.drivendata.org/examples/) - lista de verificaci√≥n de Deon con ejemplos.

> üö® Piensa en los estudios de caso que has visto: ¬øhas experimentado o te has visto afectado por un desaf√≠o √©tico similar en tu vida? ¬øPuedes pensar en al menos un estudio de caso adicional que ilustre uno de los desaf√≠os √©ticos discutidos en esta secci√≥n?

## √âtica Aplicada

Hemos hablado sobre conceptos √©ticos, desaf√≠os y estudios de caso en contextos del mundo real. Pero, ¬øc√≥mo comenzamos a _aplicar_ principios y pr√°cticas √©ticas en nuestros proyectos? ¬øY c√≥mo _operacionalizamos_ estas pr√°cticas para una mejor gobernanza? Exploremos algunas soluciones del mundo real:

### 1. C√≥digos Profesionales

Los C√≥digos Profesionales ofrecen una opci√≥n para que las organizaciones "incentiven" a sus miembros a apoyar sus principios √©ticos y declaraci√≥n de misi√≥n. Los c√≥digos son _gu√≠as morales_ para el comportamiento profesional, ayudando a los empleados o miembros a tomar decisiones que se alineen con los principios de su organizaci√≥n. Solo son efectivos si los miembros cumplen voluntariamente; sin embargo, muchas organizaciones ofrecen recompensas y sanciones adicionales para motivar el cumplimiento.

Ejemplos incluyen:

 * [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) C√≥digo de √âtica
 * [Data Science Association](http://datascienceassn.org/code-of-conduct.html) C√≥digo de Conducta (creado en 2013)
 * [ACM C√≥digo de √âtica y Conducta Profesional](https://www.acm.org/code-of-ethics) (desde 1993)

> üö® ¬øPerteneces a una organizaci√≥n profesional de ingenier√≠a o ciencia de datos? Explora su sitio para ver si definen un c√≥digo profesional de √©tica. ¬øQu√© dice esto sobre sus principios √©ticos? ¬øC√≥mo est√°n "incentivando" a los miembros a seguir el c√≥digo?

### 2. Listas de Verificaci√≥n √âticas

Mientras que los c√≥digos profesionales definen el _comportamiento √©tico_ requerido de los practicantes, [tienen limitaciones conocidas](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) en su aplicaci√≥n, particularmente en proyectos a gran escala. En cambio, muchos expertos en ciencia de datos [abogan por listas de verificaci√≥n](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), que pueden **conectar principios con pr√°cticas** de manera m√°s determinista y accionable.

Las listas de verificaci√≥n convierten preguntas en tareas de "s√≠/no" que pueden ser operacionalizadas, permitiendo que se rastreen como parte de los flujos de trabajo est√°ndar de lanzamiento de productos.

Ejemplos incluyen:
 * [Deon](https://deon.drivendata.org/) - una lista de verificaci√≥n √©tica de prop√≥sito general creada a partir de [recomendaciones de la industria](https://deon.drivendata.org/#checklist-citations) con una herramienta de l√≠nea de comandos para f√°cil integraci√≥n.
 * [Lista de Verificaci√≥n de Auditor√≠a de Privacidad](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - proporciona orientaci√≥n general para pr√°cticas de manejo de informaci√≥n desde perspectivas legales y sociales.
 * [Lista de Verificaci√≥n de Equidad en IA](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - creada por practicantes de IA para apoyar la adopci√≥n e integraci√≥n de verificaciones de equidad en los ciclos de desarrollo de IA.
 * [22 preguntas para la √©tica en datos e IA](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - un marco m√°s abierto, estructurado para la exploraci√≥n inicial de problemas √©ticos en dise√±o, implementaci√≥n y contextos organizacionales.

### 3. Regulaciones √âticas

La √©tica trata de definir valores compartidos y hacer lo correcto _voluntariamente_. **Cumplimiento** trata de _seguir la ley_ si y donde est√© definida. **Gobernanza** abarca todas las formas en que las organizaciones operan para hacer cumplir principios √©ticos y cumplir con las leyes establecidas.

Hoy en d√≠a, la gobernanza toma dos formas dentro de las organizaciones. Primero, se trata de definir principios de **IA √©tica** y establecer pr√°cticas para operacionalizar la adopci√≥n en todos los proyectos relacionados con IA en la organizaci√≥n. Segundo, se trata de cumplir con todas las **regulaciones de protecci√≥n de datos** impuestas por el gobierno en las regiones donde opera.

Ejemplos de regulaciones de protecci√≥n de datos y privacidad:

 * `1974`, [Ley de Privacidad de EE.UU.](https://www.justice.gov/opcl/privacy-act-1974) - regula la recolecci√≥n, uso y divulgaci√≥n de informaci√≥n personal por parte del _gobierno federal_.
 * `1996`, [Ley de Portabilidad y Responsabilidad de Seguros de Salud de EE.UU. (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - protege datos personales de salud.
 * `1998`, [Ley de Protecci√≥n de la Privacidad Infantil en L√≠nea de EE.UU. (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - protege la privacidad de datos de ni√±os menores de 13 a√±os.
 * `2018`, [Reglamento General de Protecci√≥n de Datos (GDPR)](https://gdpr-info.eu/) - proporciona derechos de usuario, protecci√≥n de datos y privacidad.
 * `2018`, [Ley de Privacidad del Consumidor de California (CCPA)](https://www.oag.ca.gov/privacy/ccpa) - otorga a los consumidores m√°s _derechos_ sobre sus datos personales.
 * `2021`, [Ley de Protecci√≥n de Informaci√≥n Personal de China](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) - una de las regulaciones de privacidad de datos en l√≠nea m√°s estrictas del mundo.

> üö® El Reglamento General de Protecci√≥n de Datos (GDPR) definido por la Uni√≥n Europea sigue siendo una de las regulaciones de privacidad de datos m√°s influyentes hoy en d√≠a. ¬øSab√≠as que tambi√©n define [8 derechos de usuario](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) para proteger la privacidad digital y los datos personales de los ciudadanos? Aprende cu√°les son y por qu√© son importantes.

### 4. Cultura √âtica

Nota que existe una brecha intangible entre el _cumplimiento_ (hacer lo suficiente para cumplir "con la letra de la ley") y abordar [problemas sist√©micos](https://www.coursera.org/learn/data-science-ethics/home/week/4) (como la osificaci√≥n, la asimetr√≠a de informaci√≥n y la inequidad distributiva) que pueden acelerar la instrumentalizaci√≥n de la IA.

Esto √∫ltimo requiere [enfoques colaborativos para definir culturas √©ticas](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) que construyan conexiones emocionales y valores compartidos consistentes _a trav√©s de las organizaciones_ en la industria. Esto exige m√°s [culturas √©ticas formalizadas en datos](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) en las organizaciones, permitiendo que _cualquiera_ [active el cord√≥n Andon](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (para plantear preocupaciones √©ticas temprano en el proceso) y haciendo que las _evaluaciones √©ticas_ (por ejemplo, en contrataciones) sean un criterio central en la formaci√≥n de equipos para proyectos de IA.

---
## [Cuestionario posterior a la clase](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## Revisi√≥n y Autoestudio 

Los cursos y libros ayudan a comprender los conceptos √©ticos fundamentales y los desaf√≠os, mientras que los estudios de caso y herramientas ayudan con las pr√°cticas √©ticas aplicadas en contextos del mundo real. Aqu√≠ hay algunos recursos para comenzar:

* [Machine Learning For Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - lecci√≥n sobre Equidad, de Microsoft.
* [Principios de IA Responsable](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - ruta de aprendizaje gratuita de Microsoft Learn.
* [√âtica y Ciencia de Datos](https://resources.oreilly.com/examples/0636920203964) - EBook de O'Reilly (M. Loukides, H. Mason et. al)
* [√âtica en Ciencia de Datos](https://www.coursera.org/learn/data-science-ethics#syllabus) - curso en l√≠nea de la Universidad de Michigan.
* [√âtica Desenvuelta](https://ethicsunwrapped.utexas.edu/case-studies) - estudios de caso de la Universidad de Texas.

# Tarea

[Escribe un Estudio de Caso sobre √âtica de Datos](assignment.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.