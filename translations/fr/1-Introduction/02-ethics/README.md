<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1341f6da63d434f5ba31b08ea951b02c",
  "translation_date": "2025-09-05T12:32:27+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "fr"
}
-->
# Introduction √† l'√©thique des donn√©es

|![ Sketchnote par [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| √âthique des donn√©es - _Sketchnote par [@nitya](https://twitter.com/nitya)_ |

---

Nous sommes tous des citoyens des donn√©es vivant dans un monde ax√© sur les donn√©es.

Les tendances du march√© indiquent qu'en 2022, une grande organisation sur trois ach√®tera et vendra ses donn√©es via des [places de march√© et des √©changes](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) en ligne. En tant que **d√©veloppeurs d'applications**, il sera plus facile et moins co√ªteux d'int√©grer des analyses bas√©es sur les donn√©es et des automatisations pilot√©es par des algorithmes dans les exp√©riences quotidiennes des utilisateurs. Mais √† mesure que l'IA devient omnipr√©sente, nous devrons √©galement comprendre les dommages potentiels caus√©s par la [militarisation](https://www.youtube.com/watch?v=TQHs8SA1qpk) de ces algorithmes √† grande √©chelle.

Les tendances montrent √©galement que nous cr√©erons et consommerons plus de [180 zettaoctets](https://www.statista.com/statistics/871513/worldwide-data-created/) de donn√©es d'ici 2025. En tant que **scientifiques des donn√©es**, cela nous donne un acc√®s sans pr√©c√©dent aux donn√©es personnelles. Cela signifie que nous pouvons construire des profils comportementaux des utilisateurs et influencer la prise de d√©cision de mani√®re √† cr√©er une [illusion de libre choix](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice), tout en orientant potentiellement les utilisateurs vers des r√©sultats que nous pr√©f√©rons. Cela soul√®ve √©galement des questions plus larges sur la confidentialit√© des donn√©es et la protection des utilisateurs.

L'√©thique des donn√©es est d√©sormais une _barri√®re n√©cessaire_ pour la science et l'ing√©nierie des donn√©es, nous aidant √† minimiser les dommages potentiels et les cons√©quences involontaires de nos actions bas√©es sur les donn√©es. Le [cycle de vie des tendances de Gartner pour l'IA](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifie des tendances pertinentes en mati√®re d'√©thique num√©rique, d'IA responsable et de gouvernance de l'IA comme des moteurs cl√©s des m√©gatendances plus larges autour de la _d√©mocratisation_ et de l'_industrialisation_ de l'IA.

![Cycle de vie des tendances de Gartner pour l'IA - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

Dans cette le√ßon, nous explorerons le domaine fascinant de l'√©thique des donn√©es - des concepts et d√©fis fondamentaux aux √©tudes de cas et aux concepts appliqu√©s de l'IA comme la gouvernance - qui aident √† √©tablir une culture √©thique dans les √©quipes et les organisations travaillant avec les donn√©es et l'IA.

## [Quiz avant la le√ßon](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## D√©finitions de base

Commen√ßons par comprendre la terminologie de base.

Le mot "√©thique" vient du [mot grec "ethikos"](https://en.wikipedia.org/wiki/Ethics) (et de sa racine "ethos") signifiant _caract√®re ou nature morale_. 

**L'√©thique** concerne les valeurs partag√©es et les principes moraux qui r√©gissent notre comportement en soci√©t√©. L'√©thique ne repose pas sur des lois mais sur des normes largement accept√©es de ce qui est "bien contre mal". Cependant, les consid√©rations √©thiques peuvent influencer les initiatives de gouvernance d'entreprise et les r√©glementations gouvernementales qui cr√©ent davantage d'incitations √† la conformit√©.

**L'√©thique des donn√©es** est une [nouvelle branche de l'√©thique](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) qui "√©tudie et √©value les probl√®mes moraux li√©s aux _donn√©es, algorithmes et pratiques correspondantes_". Ici, **"donn√©es"** se concentre sur les actions li√©es √† la g√©n√©ration, l'enregistrement, la conservation, le traitement, la diffusion, le partage et l'utilisation, **"algorithmes"** se concentre sur l'IA, les agents, l'apprentissage automatique et les robots, et **"pratiques"** se concentre sur des sujets comme l'innovation responsable, la programmation, le piratage et les codes √©thiques.

**L'√©thique appliqu√©e** est [l'application pratique des consid√©rations morales](https://en.wikipedia.org/wiki/Applied_ethics). C'est le processus d'enqu√™te active sur les questions √©thiques dans le contexte des _actions, produits et processus du monde r√©el_, et de prise de mesures correctives pour s'assurer qu'ils restent align√©s avec nos valeurs √©thiques d√©finies.

**La culture √©thique** consiste √† [_op√©rationnaliser_ l'√©thique appliqu√©e](https://hbr.org/2019/05/how-to-design-an-ethical-organization) pour s'assurer que nos principes et pratiques √©thiques sont adopt√©s de mani√®re coh√©rente et √©volutive dans toute l'organisation. Les cultures √©thiques r√©ussies d√©finissent des principes √©thiques √† l'√©chelle de l'organisation, offrent des incitations significatives √† la conformit√© et renforcent les normes √©thiques en encourageant et en amplifiant les comportements souhait√©s √† tous les niveaux de l'organisation.

## Concepts d'√©thique

Dans cette section, nous discuterons des concepts tels que les **valeurs partag√©es** (principes) et les **d√©fis √©thiques** (probl√®mes) pour l'√©thique des donn√©es - et explorerons des **√©tudes de cas** qui vous aident √† comprendre ces concepts dans des contextes r√©els.

### 1. Principes √©thiques

Toute strat√©gie d'√©thique des donn√©es commence par la d√©finition des _principes √©thiques_ - les "valeurs partag√©es" qui d√©crivent les comportements acceptables et guident les actions conformes dans nos projets de donn√©es et d'IA. Vous pouvez les d√©finir au niveau individuel ou de l'√©quipe. Cependant, la plupart des grandes organisations les d√©crivent dans une d√©claration de mission ou un cadre d'_IA √©thique_ d√©fini au niveau de l'entreprise et appliqu√© de mani√®re coh√©rente √† toutes les √©quipes.

**Exemple :** La d√©claration de mission de [Microsoft sur l'IA responsable](https://www.microsoft.com/en-us/ai/responsible-ai) indique : _"Nous nous engageons √† faire progresser l'IA guid√©e par des principes √©thiques qui placent les personnes au premier plan"_ - en identifiant 6 principes √©thiques dans le cadre ci-dessous :

![IA responsable chez Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Explorons bri√®vement ces principes. _La transparence_ et _la responsabilit√©_ sont des valeurs fondamentales sur lesquelles les autres principes se construisent - commen√ßons donc par l√† :

* [**Responsabilit√©**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) rend les praticiens _responsables_ de leurs op√©rations de donn√©es et d'IA, et de leur conformit√© √† ces principes √©thiques.
* [**Transparence**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) garantit que les actions li√©es aux donn√©es et √† l'IA sont _compr√©hensibles_ (interpr√©tables) pour les utilisateurs, en expliquant le quoi et le pourquoi derri√®re les d√©cisions.
* [**√âquit√©**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - se concentre sur le fait de garantir que l'IA traite _toutes les personnes_ √©quitablement, en abordant les biais sociotechniques syst√©miques ou implicites dans les donn√©es et les syst√®mes.
* [**Fiabilit√© et s√©curit√©**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - garantit que l'IA se comporte de mani√®re _coh√©rente_ avec les valeurs d√©finies, en minimisant les dommages potentiels ou les cons√©quences involontaires.
* [**Confidentialit√© et s√©curit√©**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - concerne la compr√©hension de la provenance des donn√©es et la fourniture de _protection de la confidentialit√© des donn√©es_ aux utilisateurs.
* [**Inclusivit√©**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - consiste √† concevoir des solutions d'IA avec intention, en les adaptant pour r√©pondre √† un _large √©ventail de besoins et de capacit√©s humaines_.

> üö® R√©fl√©chissez √† ce que pourrait √™tre votre d√©claration de mission sur l'√©thique des donn√©es. Explorez les cadres d'IA √©thique d'autres organisations - voici des exemples de [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles), et [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Quelles valeurs partag√©es ont-ils en commun ? Comment ces principes se rapportent-ils au produit ou √† l'industrie de l'IA dans laquelle ils op√®rent ?

### 2. D√©fis √©thiques

Une fois que nous avons d√©fini des principes √©thiques, l'√©tape suivante consiste √† √©valuer nos actions li√©es aux donn√©es et √† l'IA pour voir si elles s'alignent sur ces valeurs partag√©es. R√©fl√©chissez √† vos actions dans deux cat√©gories : _collecte de donn√©es_ et _conception d'algorithmes_.

Avec la collecte de donn√©es, les actions impliqueront probablement des **donn√©es personnelles** ou des informations personnellement identifiables (PII) pour des individus identifiables. Cela inclut [divers √©l√©ments de donn√©es non personnelles](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) qui _identifient collectivement_ un individu. Les d√©fis √©thiques peuvent concerner la _confidentialit√© des donn√©es_, la _propri√©t√© des donn√©es_, et des sujets connexes comme le _consentement √©clair√©_ et les _droits de propri√©t√© intellectuelle_ des utilisateurs.

Avec la conception d'algorithmes, les actions impliqueront la collecte et la conservation de **ensembles de donn√©es**, puis leur utilisation pour entra√Æner et d√©ployer des **mod√®les de donn√©es** qui pr√©disent des r√©sultats ou automatisent des d√©cisions dans des contextes r√©els. Les d√©fis √©thiques peuvent d√©couler de _biais dans les ensembles de donn√©es_, de probl√®mes de _qualit√© des donn√©es_, de _manque d'√©quit√©_, et de _mauvaise repr√©sentation_ dans les algorithmes - y compris certains probl√®mes qui sont syst√©miques.

Dans les deux cas, les d√©fis √©thiques mettent en √©vidence les domaines o√π nos actions peuvent entrer en conflit avec nos valeurs partag√©es. Pour d√©tecter, att√©nuer, minimiser ou √©liminer ces pr√©occupations, nous devons poser des questions morales "oui/non" li√©es √† nos actions, puis prendre des mesures correctives si n√©cessaire. Examinons quelques d√©fis √©thiques et les questions morales qu'ils soul√®vent :

#### 2.1 Propri√©t√© des donn√©es

La collecte de donn√©es implique souvent des donn√©es personnelles qui peuvent identifier les sujets des donn√©es. La [propri√©t√© des donn√©es](https://permission.io/blog/data-ownership) concerne le _contr√¥le_ et les [_droits des utilisateurs_](https://permission.io/blog/data-ownership) li√©s √† la cr√©ation, au traitement et √† la diffusion des donn√©es.

Les questions morales √† poser sont :  
* Qui poss√®de les donn√©es ? (utilisateur ou organisation)  
* Quels droits les sujets des donn√©es ont-ils ? (ex : acc√®s, effacement, portabilit√©)  
* Quels droits les organisations ont-elles ? (ex : rectifier des avis malveillants des utilisateurs)  

#### 2.2 Consentement √©clair√©

Le [consentement √©clair√©](https://legaldictionary.net/informed-consent/) d√©finit l'acte des utilisateurs d'accepter une action (comme la collecte de donn√©es) avec une _compr√©hension compl√®te_ des faits pertinents, y compris le but, les risques potentiels et les alternatives.

Questions √† explorer ici :  
* L'utilisateur (sujet des donn√©es) a-t-il donn√© son autorisation pour la capture et l'utilisation des donn√©es ?  
* L'utilisateur a-t-il compris le but pour lequel ces donn√©es ont √©t√© captur√©es ?  
* L'utilisateur a-t-il compris les risques potentiels li√©s √† sa participation ?  

#### 2.3 Propri√©t√© intellectuelle

La [propri√©t√© intellectuelle](https://en.wikipedia.org/wiki/Intellectual_property) fait r√©f√©rence aux cr√©ations intangibles r√©sultant de l'initiative humaine, qui peuvent _avoir une valeur √©conomique_ pour les individus ou les entreprises.

Questions √† explorer ici :  
* Les donn√©es collect√©es avaient-elles une valeur √©conomique pour un utilisateur ou une entreprise ?  
* L'**utilisateur** a-t-il une propri√©t√© intellectuelle ici ?  
* L'**organisation** a-t-elle une propri√©t√© intellectuelle ici ?  
* Si ces droits existent, comment les prot√©geons-nous ?  

#### 2.4 Confidentialit√© des donn√©es

La [confidentialit√© des donn√©es](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) ou confidentialit√© des informations fait r√©f√©rence √† la pr√©servation de la vie priv√©e des utilisateurs et √† la protection de leur identit√© en ce qui concerne les informations personnellement identifiables.

Questions √† explorer ici :  
* Les donn√©es (personnelles) des utilisateurs sont-elles s√©curis√©es contre les piratages et les fuites ?  
* Les donn√©es des utilisateurs sont-elles accessibles uniquement aux utilisateurs et contextes autoris√©s ?  
* L'anonymat des utilisateurs est-il pr√©serv√© lorsque les donn√©es sont partag√©es ou diffus√©es ?  
* Un utilisateur peut-il √™tre d√©sidentifi√© √† partir d'ensembles de donn√©es anonymis√©s ?  

#### 2.5 Droit √† l'oubli

Le [droit √† l'oubli](https://en.wikipedia.org/wiki/Right_to_be_forgotten) ou [droit √† l'effacement](https://www.gdpreu.org/right-to-be-forgotten/) offre une protection suppl√©mentaire des donn√©es personnelles aux utilisateurs. En particulier, il donne aux utilisateurs le droit de demander la suppression ou le retrait de donn√©es personnelles des recherches Internet et d'autres emplacements, _dans des circonstances sp√©cifiques_ - leur permettant de repartir √† z√©ro en ligne sans que leurs actions pass√©es ne soient retenues contre eux.

Questions √† explorer ici :  
* Le syst√®me permet-il aux sujets des donn√©es de demander l'effacement ?  
* Le retrait du consentement de l'utilisateur devrait-il d√©clencher un effacement automatis√© ?  
* Les donn√©es ont-elles √©t√© collect√©es sans consentement ou par des moyens ill√©gaux ?  
* Sommes-nous conformes aux r√©glementations gouvernementales sur la confidentialit√© des donn√©es ?  

#### 2.6 Biais dans les ensembles de donn√©es

Le biais dans les ensembles de donn√©es ou [biais de collecte](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) concerne la s√©lection d'un sous-ensemble _non repr√©sentatif_ de donn√©es pour le d√©veloppement d'algorithmes, cr√©ant une potentielle injustice dans les r√©sultats pour divers groupes. Les types de biais incluent le biais de s√©lection ou d'√©chantillonnage, le biais de volontariat et le biais d'instrument.

Questions √† explorer ici :  
* Avons-nous recrut√© un ensemble repr√©sentatif de sujets des donn√©es ?  
* Avons-nous test√© notre ensemble de donn√©es collect√© ou conserv√© pour divers biais ?  
* Pouvons-nous att√©nuer ou supprimer les biais d√©couverts ?  

#### 2.7 Qualit√© des donn√©es

La [qualit√© des donn√©es](https://lakefs.io/data-quality-testing/) examine la validit√© de l'ensemble de donn√©es conserv√© utilis√© pour d√©velopper nos algorithmes, en v√©rifiant si les caract√©ristiques et les enregistrements r√©pondent aux exigences du niveau de pr√©cision et de coh√©rence n√©cessaire √† notre objectif d'IA.

Questions √† explorer ici :  
* Avons-nous captur√© des _caract√©ristiques_ valides pour notre cas d'utilisation ?  
* Les donn√©es ont-elles √©t√© captur√©es de mani√®re _coh√©rente_ √† travers diverses sources de donn√©es ?  
* L'ensemble de donn√©es est-il _complet_ pour diverses conditions ou sc√©narios ?  
* Les informations captur√©es sont-elles _pr√©cises_ et refl√®tent-elles la r√©alit√© ?  

#### 2.8 √âquit√© des algorithmes
[Algorithm Fairness](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) v√©rifie si la conception de l'algorithme discrimine syst√©matiquement certains sous-groupes de sujets de donn√©es, entra√Ænant des [pr√©judices potentiels](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) dans l'_allocation_ (o√π des ressources sont refus√©es ou retenues pour ce groupe) et la _qualit√© du service_ (o√π l'IA est moins pr√©cise pour certains sous-groupes par rapport √† d'autres).

Questions √† explorer ici :
 * Avons-nous √©valu√© la pr√©cision du mod√®le pour des sous-groupes et des conditions diversifi√©s ?
 * Avons-nous examin√© le syst√®me pour identifier des pr√©judices potentiels (par exemple, des st√©r√©otypes) ?
 * Pouvons-nous r√©viser les donn√©es ou r√©entra√Æner les mod√®les pour att√©nuer les pr√©judices identifi√©s ?

Explorez des ressources comme les [checklists sur l'√©quit√© de l'IA](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) pour en savoir plus.

#### 2.9 Fausse repr√©sentation

La [fausse repr√©sentation des donn√©es](https://www.sciencedirect.com/topics/computer-science/misrepresentation) consiste √† se demander si nous communiquons des informations issues de donn√©es honn√™tement rapport√©es de mani√®re trompeuse pour soutenir un r√©cit souhait√©.

Questions √† explorer ici :
 * Rapportons-nous des donn√©es incompl√®tes ou inexactes ?
 * Visualisons-nous les donn√©es d'une mani√®re qui conduit √† des conclusions trompeuses ?
 * Utilisons-nous des techniques statistiques s√©lectives pour manipuler les r√©sultats ?
 * Existe-t-il des explications alternatives qui pourraient offrir une conclusion diff√©rente ?

#### 2.10 Libre choix
L'[illusion du libre choix](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) se produit lorsque les "architectures de choix" des syst√®mes utilisent des algorithmes de prise de d√©cision pour inciter les gens √† adopter un r√©sultat pr√©f√©r√© tout en leur donnant l'impression d'avoir des options et du contr√¥le. Ces [dark patterns](https://www.darkpatterns.org/) peuvent causer des pr√©judices sociaux et √©conomiques aux utilisateurs. √âtant donn√© que les d√©cisions des utilisateurs influencent les profils de comportement, ces actions peuvent potentiellement orienter les choix futurs, amplifiant ou prolongeant l'impact de ces pr√©judices.

Questions √† explorer ici :
 * L'utilisateur a-t-il compris les implications de son choix ?
 * L'utilisateur √©tait-il conscient des choix (alternatifs) et des avantages et inconv√©nients de chacun ?
 * L'utilisateur peut-il revenir sur un choix automatis√© ou influenc√© ult√©rieurement ?

### 3. √âtudes de cas

Pour mettre ces d√©fis √©thiques dans des contextes r√©els, il est utile d'examiner des √©tudes de cas qui mettent en lumi√®re les pr√©judices et cons√©quences potentiels pour les individus et la soci√©t√© lorsque ces violations √©thiques sont ignor√©es.

Voici quelques exemples :

| D√©fi √©thique | √âtude de cas  | 
|--- |--- |
| **Consentement √©clair√©** | 1972 - [√âtude sur la syphilis de Tuskegee](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Les hommes afro-am√©ricains ayant particip√© √† l'√©tude ont √©t√© promis des soins m√©dicaux gratuits _mais tromp√©s_ par des chercheurs qui n'ont pas inform√© les sujets de leur diagnostic ou de la disponibilit√© d'un traitement. De nombreux sujets sont morts et leurs partenaires ou enfants ont √©t√© affect√©s ; l'√©tude a dur√© 40 ans. | 
| **Confidentialit√© des donn√©es** |  2007 - Le [concours de donn√©es Netflix](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) a fourni aux chercheurs _10M de classements de films anonymis√©s de 50K clients_ pour am√©liorer les algorithmes de recommandation. Cependant, les chercheurs ont pu corr√©ler les donn√©es anonymis√©es avec des donn√©es personnellement identifiables dans des _ensembles de donn√©es externes_ (par exemple, des commentaires IMDb), "d√©sanonymisant" ainsi certains abonn√©s Netflix.|
| **Biais de collecte**  | 2013 - La ville de Boston a [d√©velopp√© Street Bump](https://www.boston.gov/transportation/street-bump), une application permettant aux citoyens de signaler les nids-de-poule, offrant √† la ville de meilleures donn√©es sur les routes pour identifier et r√©soudre les probl√®mes. Cependant, [les personnes des groupes √† faible revenu avaient moins acc√®s aux voitures et aux t√©l√©phones](https://hbr.org/2013/04/the-hidden-biases-in-big-data), rendant leurs probl√®mes de routes invisibles dans cette application. Les d√©veloppeurs ont travaill√© avec des universitaires pour r√©soudre les probl√®mes d'_acc√®s √©quitable et de fractures num√©riques_ pour plus d'√©quit√©. |
| **√âquit√© algorithmique**  | 2018 - L'√©tude MIT [Gender Shades](http://gendershades.org/overview.html) a √©valu√© la pr√©cision des produits d'IA de classification de genre, r√©v√©lant des √©carts de pr√©cision pour les femmes et les personnes de couleur. Une [carte Apple de 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) semblait offrir moins de cr√©dit aux femmes qu'aux hommes. Les deux cas ont illustr√© des probl√®mes de biais algorithmique entra√Ænant des pr√©judices socio-√©conomiques.|
| **Fausse repr√©sentation des donn√©es** | 2020 - Le [D√©partement de la sant√© publique de G√©orgie a publi√© des graphiques COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) qui semblaient induire les citoyens en erreur sur les tendances des cas confirm√©s avec un ordre non chronologique sur l'axe des x. Cela illustre la fausse repr√©sentation par des astuces de visualisation. |
| **Illusion du libre choix** | 2020 - L'application √©ducative [ABCmouse a pay√© 10M$ pour r√©gler une plainte de la FTC](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) o√π les parents √©taient pi√©g√©s dans des abonnements qu'ils ne pouvaient pas annuler. Cela illustre les dark patterns dans les architectures de choix, o√π les utilisateurs √©taient incit√©s √† faire des choix potentiellement nuisibles. |
| **Confidentialit√© des donn√©es et droits des utilisateurs** | 2021 - La [violation de donn√©es Facebook](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) a expos√© les donn√©es de 530M d'utilisateurs, entra√Ænant un r√®glement de 5B$ avec la FTC. Cependant, Facebook a refus√© de notifier les utilisateurs de la violation, violant les droits des utilisateurs en mati√®re de transparence et d'acc√®s aux donn√©es. |

Vous voulez explorer plus d'√©tudes de cas ? Consultez ces ressources :
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - dilemmes √©thiques dans divers secteurs. 
* [Cours sur l'√©thique en science des donn√©es](https://www.coursera.org/learn/data-science-ethics#syllabus) - √©tudes de cas embl√©matiques explor√©es.
* [O√π les choses ont mal tourn√©](https://deon.drivendata.org/examples/) - checklist Deon avec des exemples.

> üö® Pensez aux √©tudes de cas que vous avez vues - avez-vous v√©cu ou √©t√© affect√© par un d√©fi √©thique similaire dans votre vie ? Pouvez-vous penser √† au moins une autre √©tude de cas qui illustre l'un des d√©fis √©thiques discut√©s dans cette section ?

## √âthique appliqu√©e

Nous avons parl√© des concepts √©thiques, des d√©fis et des √©tudes de cas dans des contextes r√©els. Mais comment commencer √† _appliquer_ des principes et pratiques √©thiques dans nos projets ? Et comment _op√©rationnaliser_ ces pratiques pour une meilleure gouvernance ? Explorons quelques solutions concr√®tes : 

### 1. Codes professionnels

Les codes professionnels offrent une option pour que les organisations "incitent" leurs membres √† soutenir leurs principes √©thiques et leur mission. Les codes sont des _lignes directrices morales_ pour le comportement professionnel, aidant les employ√©s ou membres √† prendre des d√©cisions align√©es sur les principes de leur organisation. Ils ne sont efficaces que si les membres les respectent volontairement ; cependant, de nombreuses organisations offrent des r√©compenses et des sanctions suppl√©mentaires pour motiver la conformit√©.

Exemples :
 * [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) Code d'√©thique
 * [Data Science Association](http://datascienceassn.org/code-of-conduct.html) Code de conduite (cr√©√© en 2013)
 * [ACM Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics) (depuis 1993)

> üö® Faites-vous partie d'une organisation professionnelle en ing√©nierie ou en science des donn√©es ? Explorez leur site pour voir s'ils d√©finissent un code d'√©thique professionnel. Que dit-il sur leurs principes √©thiques ? Comment incitent-ils leurs membres √† suivre le code ?

### 2. Checklists √©thiques

Alors que les codes professionnels d√©finissent le comportement _√©thique requis_ des praticiens, ils [ont des limites connues](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) en mati√®re d'application, en particulier dans les projets √† grande √©chelle. Au lieu de cela, de nombreux experts en science des donn√©es [pr√©conisent des checklists](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), qui peuvent **connecter les principes aux pratiques** de mani√®re plus d√©terministe et actionnable.

Les checklists transforment les questions en t√¢ches "oui/non" qui peuvent √™tre op√©rationnalis√©es, permettant leur suivi dans le cadre des workflows standard de lancement de produit.

Exemples :
 * [Deon](https://deon.drivendata.org/) - une checklist √©thique g√©n√©rale cr√©√©e √† partir de [recommandations de l'industrie](https://deon.drivendata.org/#checklist-citations) avec un outil en ligne de commande pour une int√©gration facile.
 * [Checklist d'audit de confidentialit√©](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - fournit des conseils g√©n√©raux sur les pratiques de gestion de l'information du point de vue juridique et social.
 * [Checklist sur l'√©quit√© de l'IA](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - cr√©√©e par des praticiens de l'IA pour soutenir l'adoption et l'int√©gration des v√©rifications d'√©quit√© dans les cycles de d√©veloppement de l'IA.
 * [22 questions pour l'√©thique dans les donn√©es et l'IA](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - cadre plus ouvert, structur√© pour une exploration initiale des probl√®mes √©thiques dans la conception, la mise en ≈ìuvre et les contextes organisationnels.

### 3. R√©glementations √©thiques

L'√©thique consiste √† d√©finir des valeurs partag√©es et √† faire ce qui est juste _volontairement_. **La conformit√©** consiste √† _respecter la loi_ l√† o√π elle est d√©finie. **La gouvernance** couvre globalement toutes les fa√ßons dont les organisations op√®rent pour appliquer des principes √©thiques et se conformer aux lois √©tablies.

Aujourd'hui, la gouvernance prend deux formes au sein des organisations. Premi√®rement, il s'agit de d√©finir des principes d'**IA √©thique** et d'√©tablir des pratiques pour op√©rationnaliser leur adoption dans tous les projets li√©s √† l'IA de l'organisation. Deuxi√®mement, il s'agit de se conformer √† toutes les **r√©glementations sur la protection des donn√©es** impos√©es par les gouvernements des r√©gions o√π elle op√®re.

Exemples de r√©glementations sur la protection des donn√©es et la confidentialit√© :
 * `1974`, [US Privacy Act](https://www.justice.gov/opcl/privacy-act-1974) - r√©gule la collecte, l'utilisation et la divulgation des informations personnelles par le _gouvernement f√©d√©ral_.
 * `1996`, [US Health Insurance Portability & Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - prot√®ge les donn√©es personnelles de sant√©.
 * `1998`, [US Children's Online Privacy Protection Act (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - prot√®ge la confidentialit√© des donn√©es des enfants de moins de 13 ans.
 * `2018`, [R√®glement g√©n√©ral sur la protection des donn√©es (RGPD)](https://gdpr-info.eu/) - fournit des droits aux utilisateurs, prot√®ge les donn√©es et la vie priv√©e.
 * `2018`, [California Consumer Privacy Act (CCPA)](https://www.oag.ca.gov/privacy/ccpa) donne aux consommateurs plus de _droits_ sur leurs donn√©es personnelles.
 * `2021`, La [Loi sur la protection des informations personnelles de la Chine](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) vient d'√™tre adopt√©e, cr√©ant l'une des r√©glementations les plus strictes sur la confidentialit√© des donn√©es en ligne dans le monde.

> üö® L'Union europ√©enne a d√©fini le RGPD (R√®glement g√©n√©ral sur la protection des donn√©es), qui reste l'une des r√©glementations les plus influentes sur la confidentialit√© des donn√©es aujourd'hui. Saviez-vous qu'il d√©finit √©galement [8 droits des utilisateurs](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) pour prot√©ger la vie priv√©e num√©rique et les donn√©es personnelles des citoyens ? D√©couvrez quels sont ces droits et pourquoi ils sont importants.

### 4. Culture √©thique

Notez qu'il existe un √©cart intangible entre _la conformit√©_ (faire juste assez pour respecter "la lettre de la loi") et le traitement des [probl√®mes syst√©miques](https://www.coursera.org/learn/data-science-ethics/home/week/4) (comme l'ossification, l'asym√©trie de l'information et l'injustice distributive) qui peuvent acc√©l√©rer la militarisation de l'IA.

Ce dernier n√©cessite [des approches collaboratives pour d√©finir des cultures √©thiques](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) qui construisent des connexions √©motionnelles et des valeurs partag√©es coh√©rentes _entre les organisations_ de l'industrie. Cela appelle √† des [cultures √©thiques formalis√©es](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) dans les organisations - permettant √† _n'importe qui_ de [tirer le cordon Andon](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (pour soulever des pr√©occupations √©thiques t√¥t dans le processus) et faisant des _√©valuations √©thiques_ (par exemple, lors du recrutement) un crit√®re central de la formation des √©quipes dans les projets d'IA.

---
## [Quiz post-conf√©rence](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## R√©vision et auto-apprentissage 

Les cours et les livres aident √† comprendre les concepts et d√©fis √©thiques fondamentaux, tandis que les √©tudes de cas et les outils aident √† appliquer les pratiques √©thiques dans des contextes r√©els. Voici quelques ressources pour commencer.

* [Machine Learning For Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - le√ßon sur l'√©quit√©, de Microsoft.
* [Principes de l'IA Responsable](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - parcours d'apprentissage gratuit de Microsoft Learn.  
* [√âthique et Science des Donn√©es](https://resources.oreilly.com/examples/0636920203964) - EBook d'O'Reilly (M. Loukides, H. Mason et al.)  
* [√âthique en Science des Donn√©es](https://www.coursera.org/learn/data-science-ethics#syllabus) - cours en ligne de l'Universit√© du Michigan.  
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - √©tudes de cas de l'Universit√© du Texas.  

# Devoir  

[√âcrire une √âtude de Cas sur l'√âthique des Donn√©es](assignment.md)  

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de faire appel √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.