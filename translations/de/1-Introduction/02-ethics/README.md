<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1341f6da63d434f5ba31b08ea951b02c",
  "translation_date": "2025-09-05T14:03:41+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "de"
}
-->
# Einf√ºhrung in Datenethik

|![ Sketchnote von [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| Datenethik - _Sketchnote von [@nitya](https://twitter.com/nitya)_ |

---

Wir sind alle Datenb√ºrger, die in einer datengetriebenen Welt leben.

Markttrends zeigen, dass bis 2022 jede dritte gro√üe Organisation ihre Daten √ºber Online-[Marktpl√§tze und B√∂rsen](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) kaufen und verkaufen wird. Als **App-Entwickler** wird es f√ºr uns einfacher und g√ºnstiger, datengesteuerte Erkenntnisse und algorithmusgesteuerte Automatisierung in den Alltag der Nutzer zu integrieren. Doch je allgegenw√§rtiger KI wird, desto mehr m√ºssen wir auch die potenziellen Sch√§den verstehen, die durch die [Instrumentalisierung](https://www.youtube.com/watch?v=TQHs8SA1qpk) solcher Algorithmen in gro√üem Ma√üstab entstehen k√∂nnen.

Trends zeigen auch, dass wir bis 2025 √ºber [180 Zettabyte](https://www.statista.com/statistics/871513/worldwide-data-created/) an Daten erstellen und konsumieren werden. Als **Datenwissenschaftler** gibt uns dies beispiellosen Zugang zu pers√∂nlichen Daten. Das bedeutet, dass wir Verhaltensprofile von Nutzern erstellen und Entscheidungsprozesse beeinflussen k√∂nnen, wodurch eine [Illusion der freien Wahl](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) entsteht, w√§hrend Nutzer m√∂glicherweise in von uns bevorzugte Richtungen gelenkt werden. Dies wirft auch gr√∂√üere Fragen zu Datenschutz und Nutzerschutz auf.

Datenethik ist nun ein _notwendiges Leitplanken-System_ f√ºr Datenwissenschaft und Ingenieurwesen, das uns hilft, potenzielle Sch√§den und unbeabsichtigte Konsequenzen unserer datengetriebenen Handlungen zu minimieren. Der [Gartner Hype Cycle f√ºr KI](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifiziert relevante Trends in digitaler Ethik, verantwortungsvoller KI und KI-Governance als Schl√ºsselfaktoren f√ºr gr√∂√üere Megatrends wie die _Demokratisierung_ und _Industrialisierung_ von KI.

![Gartner's Hype Cycle f√ºr KI - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

In dieser Lektion werden wir den faszinierenden Bereich der Datenethik erkunden ‚Äì von grundlegenden Konzepten und Herausforderungen bis hin zu Fallstudien und angewandten KI-Konzepten wie Governance, die dazu beitragen, eine Ethikkultur in Teams und Organisationen zu etablieren, die mit Daten und KI arbeiten.

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Grundlegende Definitionen

Beginnen wir mit dem Verst√§ndnis der grundlegenden Terminologie.

Das Wort "Ethik" stammt vom [griechischen Wort "ethikos"](https://en.wikipedia.org/wiki/Ethics) (und seiner Wurzel "ethos") ab, was _Charakter oder moralische Natur_ bedeutet.

**Ethik** bezieht sich auf die gemeinsamen Werte und moralischen Prinzipien, die unser Verhalten in der Gesellschaft bestimmen. Ethik basiert nicht auf Gesetzen, sondern auf allgemein akzeptierten Normen dessen, was "richtig vs. falsch" ist. Dennoch k√∂nnen ethische √úberlegungen Unternehmensf√ºhrungsinitiativen und staatliche Vorschriften beeinflussen, die mehr Anreize f√ºr die Einhaltung schaffen.

**Datenethik** ist ein [neuer Zweig der Ethik](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1), der "moralische Probleme im Zusammenhang mit _Daten, Algorithmen und entsprechenden Praktiken_ untersucht und bewertet". Hierbei konzentrieren sich **"Daten"** auf Aktionen wie Erzeugung, Aufzeichnung, Kuratierung, Verarbeitung, Verbreitung, Teilen und Nutzung, **"Algorithmen"** auf KI, Agenten, maschinelles Lernen und Roboter, und **"Praktiken"** auf Themen wie verantwortungsvolle Innovation, Programmierung, Hacking und Ethikkodizes.

**Angewandte Ethik** ist die [praktische Anwendung moralischer √úberlegungen](https://en.wikipedia.org/wiki/Applied_ethics). Es handelt sich um den Prozess der aktiven Untersuchung ethischer Fragen im Kontext von _realen Handlungen, Produkten und Prozessen_ und der Ergreifung von Korrekturma√ünahmen, um sicherzustellen, dass diese mit unseren definierten ethischen Werten √ºbereinstimmen.

**Ethikkultur** bezieht sich auf das [_Operationalisieren_ angewandter Ethik](https://hbr.org/2019/05/how-to-design-an-ethical-organization), um sicherzustellen, dass unsere ethischen Prinzipien und Praktiken konsistent und skalierbar in der gesamten Organisation √ºbernommen werden. Erfolgreiche Ethikkulturen definieren organisationsweite ethische Prinzipien, bieten sinnvolle Anreize f√ºr die Einhaltung und verst√§rken ethische Normen, indem sie gew√ºnschte Verhaltensweisen auf allen Ebenen der Organisation f√∂rdern und verst√§rken.

## Ethikkonzepte

In diesem Abschnitt werden wir Konzepte wie **gemeinsame Werte** (Prinzipien) und **ethische Herausforderungen** (Probleme) f√ºr die Datenethik diskutieren ‚Äì und **Fallstudien** untersuchen, die Ihnen helfen, diese Konzepte in realen Kontexten zu verstehen.

### 1. Ethische Prinzipien

Jede Datenethikstrategie beginnt mit der Definition von _ethischen Prinzipien_ ‚Äì den "gemeinsamen Werten", die akzeptables Verhalten beschreiben und regelkonforme Handlungen in unseren Daten- und KI-Projekten leiten. Sie k√∂nnen diese auf individueller oder Teamebene definieren. Die meisten gro√üen Organisationen legen diese jedoch in einer _Ethik-KI-Missionserkl√§rung_ oder einem Rahmenwerk fest, das auf Unternehmensebene definiert und konsistent in allen Teams durchgesetzt wird.

**Beispiel:** Microsofts [Verantwortungsvolle KI](https://www.microsoft.com/en-us/ai/responsible-ai)-Missionserkl√§rung lautet: _"Wir verpflichten uns zur Weiterentwicklung von KI, die von ethischen Prinzipien geleitet wird, die den Menschen in den Mittelpunkt stellen"_ ‚Äì und identifiziert 6 ethische Prinzipien im folgenden Rahmenwerk:

![Verantwortungsvolle KI bei Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Lassen Sie uns diese Prinzipien kurz erkunden. _Transparenz_ und _Verantwortlichkeit_ sind grundlegende Werte, auf denen andere Prinzipien aufbauen ‚Äì beginnen wir also damit:

* [**Verantwortlichkeit**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) macht Praktiker _verantwortlich_ f√ºr ihre Daten- und KI-Operationen sowie die Einhaltung dieser ethischen Prinzipien.
* [**Transparenz**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) stellt sicher, dass Daten- und KI-Aktionen f√ºr Nutzer _verst√§ndlich_ (interpretierbar) sind und erkl√§rt, was und warum Entscheidungen getroffen werden.
* [**Fairness**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) ‚Äì konzentriert sich darauf, sicherzustellen, dass KI _alle Menschen_ fair behandelt und systemische oder implizite sozio-technische Vorurteile in Daten und Systemen angeht.
* [**Zuverl√§ssigkeit & Sicherheit**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) ‚Äì stellt sicher, dass KI _konsistent_ mit definierten Werten handelt und potenzielle Sch√§den oder unbeabsichtigte Konsequenzen minimiert.
* [**Datenschutz & Sicherheit**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) ‚Äì bezieht sich auf das Verst√§ndnis der Datenherkunft und den Schutz der _Datenprivatsph√§re und verwandter Rechte_ der Nutzer.
* [**Inklusivit√§t**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) ‚Äì bedeutet, KI-L√∂sungen mit Absicht zu entwerfen und sie so anzupassen, dass sie _eine breite Palette menschlicher Bed√ºrfnisse_ und F√§higkeiten erf√ºllen.

> üö® √úberlegen Sie, was Ihre Datenethik-Missionserkl√§rung sein k√∂nnte. Erkunden Sie ethische KI-Rahmenwerke anderer Organisationen ‚Äì hier sind Beispiele von [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) und [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Welche gemeinsamen Werte haben sie? Wie beziehen sich diese Prinzipien auf das KI-Produkt oder die Branche, in der sie t√§tig sind?

### 2. Ethische Herausforderungen

Sobald wir ethische Prinzipien definiert haben, besteht der n√§chste Schritt darin, unsere Daten- und KI-Aktionen zu bewerten, um zu sehen, ob sie mit diesen gemeinsamen Werten √ºbereinstimmen. Denken Sie √ºber Ihre Aktionen in zwei Kategorien nach: _Datenerhebung_ und _Algorithmendesign_.

Bei der Datenerhebung werden wahrscheinlich **pers√∂nliche Daten** oder personenbezogene Informationen (PII) f√ºr identifizierbare lebende Personen verwendet. Dazu geh√∂ren [verschiedene Arten nicht-personenbezogener Daten](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en), die _zusammen_ eine Person identifizieren k√∂nnen. Ethische Herausforderungen k√∂nnen sich auf _Datenschutz_, _Datenbesitz_ und verwandte Themen wie _informierte Zustimmung_ und _geistige Eigentumsrechte_ der Nutzer beziehen.

Beim Algorithmendesign umfassen die Aktionen das Sammeln und Kuratieren von **Datens√§tzen** und deren Verwendung zum Trainieren und Bereitstellen von **Datenmodellen**, die Ergebnisse vorhersagen oder Entscheidungen in realen Kontexten automatisieren. Ethische Herausforderungen k√∂nnen sich aus _Datensatzverzerrungen_, _Datenqualit√§tsproblemen_, _Unfairness_ und _Fehldarstellungen_ in Algorithmen ergeben ‚Äì einschlie√ülich einiger systemischer Probleme.

In beiden F√§llen heben ethische Herausforderungen Bereiche hervor, in denen unsere Handlungen m√∂glicherweise mit unseren gemeinsamen Werten in Konflikt geraten. Um diese Bedenken zu erkennen, zu mindern, zu minimieren oder zu beseitigen, m√ºssen wir moralische "Ja/Nein"-Fragen zu unseren Handlungen stellen und gegebenenfalls Korrekturma√ünahmen ergreifen. Schauen wir uns einige ethische Herausforderungen und die moralischen Fragen an, die sie aufwerfen:

#### 2.1 Datenbesitz

Die Datenerhebung umfasst oft pers√∂nliche Daten, die die Datensubjekte identifizieren k√∂nnen. [Datenbesitz](https://permission.io/blog/data-ownership) bezieht sich auf _Kontrolle_ und [_Nutzerrechte_](https://permission.io/blog/data-ownership) in Bezug auf die Erstellung, Verarbeitung und Verbreitung von Daten.

Die moralischen Fragen, die wir stellen m√ºssen, sind:
 * Wem geh√∂ren die Daten? (Nutzer oder Organisation)
 * Welche Rechte haben Datensubjekte? (z. B. Zugriff, L√∂schung, √úbertragbarkeit)
 * Welche Rechte haben Organisationen? (z. B. Korrektur b√∂swilliger Nutzerbewertungen)

#### 2.2 Informierte Zustimmung

[Informierte Zustimmung](https://legaldictionary.net/informed-consent/) definiert die Handlung, bei der Nutzer einer Aktion (wie der Datenerhebung) mit einem _vollst√§ndigen Verst√§ndnis_ relevanter Fakten einschlie√ülich Zweck, potenzieller Risiken und Alternativen zustimmen.

Fragen, die hier zu untersuchen sind:
 * Hat der Nutzer (Datensubjekt) die Erfassung und Nutzung von Daten genehmigt?
 * Hat der Nutzer den Zweck verstanden, f√ºr den die Daten erfasst wurden?
 * Hat der Nutzer die potenziellen Risiken seiner Teilnahme verstanden?

#### 2.3 Geistiges Eigentum

[Geistiges Eigentum](https://en.wikipedia.org/wiki/Intellectual_property) bezieht sich auf immaterielle Sch√∂pfungen, die aus menschlicher Initiative resultieren und _wirtschaftlichen Wert_ f√ºr Einzelpersonen oder Unternehmen haben k√∂nnen.

Fragen, die hier zu untersuchen sind:
 * Hatten die gesammelten Daten wirtschaftlichen Wert f√ºr einen Nutzer oder ein Unternehmen?
 * Hat der **Nutzer** hier geistiges Eigentum?
 * Hat die **Organisation** hier geistiges Eigentum?
 * Wenn diese Rechte existieren, wie sch√ºtzen wir sie?

#### 2.4 Datenschutz

[Datenschutz](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) oder Informationsschutz bezieht sich auf die Wahrung der Privatsph√§re der Nutzer und den Schutz ihrer Identit√§t in Bezug auf personenbezogene Informationen.

Fragen, die hier zu untersuchen sind:
 * Sind die (pers√∂nlichen) Daten der Nutzer vor Hacks und Lecks gesch√ºtzt?
 * Sind die Daten der Nutzer nur autorisierten Nutzern und Kontexten zug√§nglich?
 * Wird die Anonymit√§t der Nutzer gewahrt, wenn Daten geteilt oder verbreitet werden?
 * Kann ein Nutzer aus anonymisierten Datens√§tzen de-identifiziert werden?

#### 2.5 Recht auf Vergessenwerden

Das [Recht auf Vergessenwerden](https://en.wikipedia.org/wiki/Right_to_be_forgotten) oder [Recht auf L√∂schung](https://www.gdpreu.org/right-to-be-forgotten/) bietet Nutzern zus√§tzlichen Schutz ihrer pers√∂nlichen Daten. Insbesondere gibt es Nutzern das Recht, die L√∂schung oder Entfernung pers√∂nlicher Daten aus Internetsuchen und anderen Orten _unter bestimmten Umst√§nden_ zu verlangen ‚Äì und ihnen so einen Neuanfang online zu erm√∂glichen, ohne dass vergangene Handlungen gegen sie verwendet werden.

Fragen, die hier zu untersuchen sind:
 * Erm√∂glicht das System Datensubjekten, die L√∂schung zu beantragen?
 * Sollte der Widerruf der Zustimmung des Nutzers eine automatisierte L√∂schung ausl√∂sen?
 * Wurden Daten ohne Zustimmung oder auf rechtswidrige Weise erhoben?
 * Sind wir konform mit staatlichen Vorschriften zum Datenschutz?

#### 2.6 Verzerrungen in Datens√§tzen

Verzerrungen in Datens√§tzen oder [Erhebungsverzerrungen](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) beziehen sich auf die Auswahl eines _nicht-repr√§sentativen_ Teils von Daten f√ºr die Algorithmusentwicklung, was potenzielle Unfairness in den Ergebnissen f√ºr verschiedene Gruppen schaffen kann. Arten von Verzerrungen umfassen Auswahl- oder Stichprobenverzerrungen, Freiwilligenverzerrungen und Instrumentenverzerrungen.

Fragen, die hier zu untersuchen sind:
 * Haben wir eine repr√§sentative Gruppe von Datensubjekten rekrutiert?
 * Haben wir unseren gesammelten oder kuratierten Datensatz auf verschiedene Verzerrungen getestet?
 * K√∂nnen wir entdeckte Verzerrungen mindern oder entfernen?

#### 2.7 Datenqualit√§t

[Datenqualit√§t](https://lakefs.io/data-quality-testing/) bezieht sich auf die G√ºltigkeit des kuratierten Datensatzes, der zur Entwicklung unserer Algorithmen verwendet wird, und √ºberpr√ºft, ob Merkmale und Datens√§tze die Anforderungen an Genauigkeit und Konsistenz f√ºr unseren KI-Zweck erf√ºllen.

Fragen, die hier zu untersuchen sind:
 * Haben wir g√ºltige _Merkmale_ f√ºr unseren Anwendungsfall erfasst?
 * Wurden Daten _konsistent_ √ºber verschiedene Datenquellen hinweg erfasst?
 * Ist der Datensatz _vollst√§ndig_ f√ºr verschiedene Bedingungen oder Szenarien?
 * Wurden Informationen _genau_ erfasst und spiegeln die Realit√§t wider?

#### 2.8 Algorithmische Fairness
[Algorithmische Fairness](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) √ºberpr√ºft, ob das Design des Algorithmus systematisch bestimmte Untergruppen von Datensubjekten diskriminiert, was zu [potenziellen Sch√§den](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) in der _Ressourcenverteilung_ (bei der Ressourcen verweigert oder zur√ºckgehalten werden) und der _Dienstleistungsqualit√§t_ (bei der KI f√ºr einige Untergruppen weniger genau ist als f√ºr andere) f√ºhren kann.

Fragen, die hier untersucht werden sollten:
 * Haben wir die Modellgenauigkeit f√ºr verschiedene Untergruppen und Bedingungen bewertet?
 * Haben wir das System auf potenzielle Sch√§den (z. B. Stereotypisierung) √ºberpr√ºft?
 * K√∂nnen wir Daten √ºberarbeiten oder Modelle neu trainieren, um identifizierte Sch√§den zu mindern?

Erkunden Sie Ressourcen wie [Checklisten zur Fairness von KI](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA), um mehr zu erfahren.

#### 2.9 Fehlrepr√§sentation

[Fehlrepr√§sentation von Daten](https://www.sciencedirect.com/topics/computer-science/misrepresentation) bezieht sich darauf, ob wir Erkenntnisse aus ehrlich berichteten Daten auf eine t√§uschende Weise kommunizieren, um eine gew√ºnschte Erz√§hlung zu unterst√ºtzen.

Fragen, die hier untersucht werden sollten:
 * Berichten wir unvollst√§ndige oder ungenaue Daten?
 * Visualisieren wir Daten so, dass irref√ºhrende Schlussfolgerungen gezogen werden k√∂nnen?
 * Verwenden wir selektive statistische Techniken, um Ergebnisse zu manipulieren?
 * Gibt es alternative Erkl√§rungen, die zu einer anderen Schlussfolgerung f√ºhren k√∂nnten?

#### 2.10 Freie Wahl
Die [Illusion der freien Wahl](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) tritt auf, wenn "Entscheidungsarchitekturen" eines Systems Algorithmen verwenden, um Menschen dazu zu bewegen, ein bevorzugtes Ergebnis zu w√§hlen, w√§hrend sie scheinbar Optionen und Kontrolle haben. Diese [dunklen Muster](https://www.darkpatterns.org/) k√∂nnen sozialen und wirtschaftlichen Schaden f√ºr Nutzer verursachen. Da Nutzerentscheidungen Verhaltensprofile beeinflussen, k√∂nnen diese Aktionen zuk√ºnftige Entscheidungen antreiben, die die Auswirkungen dieser Sch√§den verst√§rken oder verl√§ngern.

Fragen, die hier untersucht werden sollten:
 * Hat der Nutzer die Auswirkungen seiner Entscheidung verstanden?
 * War sich der Nutzer der (alternativen) Optionen und der Vor- und Nachteile jeder Option bewusst?
 * Kann der Nutzer eine automatisierte oder beeinflusste Entscheidung sp√§ter r√ºckg√§ngig machen?

### 3. Fallstudien

Um diese ethischen Herausforderungen in realen Kontexten zu verstehen, hilft es, Fallstudien zu betrachten, die die potenziellen Sch√§den und Konsequenzen f√ºr Einzelpersonen und die Gesellschaft aufzeigen, wenn solche ethischen Verst√∂√üe √ºbersehen werden.

Hier einige Beispiele:

| Ethische Herausforderung | Fallstudie  | 
|--- |--- |
| **Informierte Zustimmung** | 1972 - [Tuskegee-Syphilis-Studie](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Afroamerikanische M√§nner, die an der Studie teilnahmen, wurden kostenlose medizinische Versorgung versprochen, _aber von Forschern get√§uscht_, die die Teilnehmer nicht √ºber ihre Diagnose oder die Verf√ºgbarkeit von Behandlung informierten. Viele Teilnehmer starben, und Partner oder Kinder waren betroffen; die Studie dauerte 40 Jahre. | 
| **Datenschutz** |  2007 - Der [Netflix-Datenpreis](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) stellte Forschern _10 Millionen anonymisierte Filmbewertungen von 50.000 Kunden_ zur Verf√ºgung, um Empfehlungsalgorithmen zu verbessern. Forscher konnten jedoch anonymisierte Daten mit pers√∂nlich identifizierbaren Daten in _externen Datens√§tzen_ (z. B. IMDb-Kommentaren) korrelieren und so einige Netflix-Abonnenten effektiv "de-anonymisieren".|
| **Erfassungsbias**  | 2013 - Die Stadt Boston [entwickelte Street Bump](https://www.boston.gov/transportation/street-bump), eine App, mit der B√ºrger Schlagl√∂cher melden konnten, um der Stadt bessere Stra√üendaten zur Verf√ºgung zu stellen. Allerdings hatten [Menschen in einkommensschw√§cheren Gruppen weniger Zugang zu Autos und Telefonen](https://hbr.org/2013/04/the-hidden-biases-in-big-data), wodurch ihre Stra√üenprobleme in dieser App unsichtbar blieben. Entwickler arbeiteten mit Akademikern zusammen, um _Probleme des gerechten Zugangs und der digitalen Kluft_ f√ºr mehr Fairness zu l√∂sen. |
| **Algorithmische Fairness**  | 2018 - Die MIT-Studie [Gender Shades](http://gendershades.org/overview.html) bewertete die Genauigkeit von KI-Produkten zur Geschlechtsklassifikation und deckte L√ºcken in der Genauigkeit f√ºr Frauen und farbige Personen auf. Eine [2019 Apple Card](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) schien Frauen weniger Kredit zu gew√§hren als M√§nnern. Beide F√§lle illustrieren Probleme algorithmischer Verzerrungen, die zu sozio√∂konomischen Sch√§den f√ºhren.|
| **Fehlrepr√§sentation von Daten** | 2020 - Das [Georgia Department of Public Health ver√∂ffentlichte COVID-19-Diagramme](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening), die B√ºrger √ºber Trends bei best√§tigten F√§llen mit nicht-chronologischer Anordnung auf der x-Achse irref√ºhren k√∂nnten. Dies zeigt Fehlrepr√§sentation durch Visualisierungstricks. |
| **Illusion der freien Wahl** | 2020 - Lern-App [ABCmouse zahlte $10M, um eine FTC-Beschwerde beizulegen](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/), bei der Eltern in Abonnements gefangen waren, die sie nicht k√ºndigen konnten. Dies zeigt dunkle Muster in Entscheidungsarchitekturen, bei denen Nutzer zu potenziell sch√§dlichen Entscheidungen gedr√§ngt wurden. |
| **Datenschutz & Nutzerrechte** | 2021 - Facebook [Datenleck](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) enth√ºllte Daten von 530 Millionen Nutzern, was zu einer $5B-Abfindung an die FTC f√ºhrte. Facebook weigerte sich jedoch, die Nutzer √ºber das Leck zu informieren, was die Nutzerrechte in Bezug auf Datentransparenz und Zugang verletzte. |

M√∂chten Sie weitere Fallstudien erkunden? Schauen Sie sich diese Ressourcen an:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - ethische Dilemmata in verschiedenen Branchen. 
* [Data Science Ethics course](https://www.coursera.org/learn/data-science-ethics#syllabus) - wegweisende Fallstudien.
* [Wo Dinge schiefgelaufen sind](https://deon.drivendata.org/examples/) - Deon-Checkliste mit Beispielen.

> üö® Denken Sie √ºber die Fallstudien nach, die Sie gesehen haben ‚Äì haben Sie √§hnliche ethische Herausforderungen in Ihrem Leben erlebt oder wurden davon betroffen? K√∂nnen Sie mindestens eine weitere Fallstudie nennen, die eine der ethischen Herausforderungen in diesem Abschnitt illustriert?

## Angewandte Ethik

Wir haben √ºber ethische Konzepte, Herausforderungen und Fallstudien in realen Kontexten gesprochen. Aber wie beginnen wir, _ethische Prinzipien und Praktiken_ in unseren Projekten anzuwenden? Und wie _operationalisieren_ wir diese Praktiken f√ºr eine bessere Governance? Lassen Sie uns einige L√∂sungen aus der Praxis erkunden:

### 1. Berufliche Kodizes

Berufliche Kodizes bieten eine M√∂glichkeit f√ºr Organisationen, Mitglieder zu "motivieren", ihre ethischen Prinzipien und ihre Mission zu unterst√ºtzen. Kodizes sind _moralische Leitlinien_ f√ºr berufliches Verhalten, die Mitarbeitern oder Mitgliedern helfen, Entscheidungen zu treffen, die mit den Prinzipien ihrer Organisation √ºbereinstimmen. Sie sind nur so gut wie die freiwillige Einhaltung durch die Mitglieder; viele Organisationen bieten jedoch zus√§tzliche Belohnungen und Strafen, um die Einhaltung zu f√∂rdern.

Beispiele:
 * [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) Code of Ethics
 * [Data Science Association](http://datascienceassn.org/code-of-conduct.html) Verhaltenskodex (erstellt 2013)
 * [ACM Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics) (seit 1993)

> üö® Sind Sie Mitglied einer professionellen Ingenieur- oder Datenwissenschaftsorganisation? Erkunden Sie deren Website, um zu sehen, ob sie einen beruflichen Ethikkodex definieren. Was sagt dieser √ºber ihre ethischen Prinzipien aus? Wie motivieren sie Mitglieder, den Kodex zu befolgen?

### 2. Ethik-Checklisten

W√§hrend berufliche Kodizes erforderliches _ethisches Verhalten_ von Praktikern definieren, haben sie [bekannte Einschr√§nkungen](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) bei der Durchsetzung, insbesondere in gro√ü angelegten Projekten. Stattdessen bef√ºrworten viele Datenwissenschaftsexperten [Checklisten](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), die **Prinzipien mit Praktiken** auf deterministische und umsetzbare Weise verbinden k√∂nnen.

Checklisten wandeln Fragen in "Ja/Nein"-Aufgaben um, die operationalisiert werden k√∂nnen, sodass sie als Teil standardm√§√üiger Produktfreigabeworkflows verfolgt werden k√∂nnen.

Beispiele:
 * [Deon](https://deon.drivendata.org/) - eine allgemeine Datenethik-Checkliste, erstellt aus [Empfehlungen der Branche](https://deon.drivendata.org/#checklist-citations) mit einem Befehlszeilentool f√ºr einfache Integration.
 * [Privacy Audit Checklist](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - bietet allgemeine Leitlinien f√ºr Informationshandhabungspraktiken aus rechtlicher und sozialer Perspektive.
 * [AI Fairness Checklist](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - erstellt von KI-Praktikern zur Unterst√ºtzung der Integration von Fairness-Checks in KI-Entwicklungszyklen.
 * [22 Fragen zur Ethik in Daten und KI](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - ein offeneres Framework, strukturiert f√ºr die erste Untersuchung ethischer Fragen in Design-, Implementierungs- und organisatorischen Kontexten.

### 3. Ethik-Regulierungen

Ethik bedeutet, gemeinsame Werte zu definieren und das Richtige _freiwillig_ zu tun. **Compliance** bedeutet, _das Gesetz zu befolgen_, wenn und wo es definiert ist. **Governance** umfasst allgemein alle M√∂glichkeiten, wie Organisationen ethische Prinzipien durchsetzen und gesetzliche Vorschriften einhalten.

Heute nimmt Governance zwei Formen innerhalb von Organisationen an. Erstens geht es darum, **ethische KI**-Prinzipien zu definieren und Praktiken zu etablieren, um die Einf√ºhrung in allen KI-bezogenen Projekten der Organisation zu operationalisieren. Zweitens geht es darum, alle staatlich vorgeschriebenen **Datenschutzvorschriften** f√ºr die Regionen, in denen sie t√§tig sind, einzuhalten.

Beispiele f√ºr Datenschutz- und Privatsph√§re-Regulierungen:

 * `1974`, [US Privacy Act](https://www.justice.gov/opcl/privacy-act-1974) - regelt die _Sammlung, Nutzung und Offenlegung_ pers√∂nlicher Informationen durch die Bundesregierung.
 * `1996`, [US Health Insurance Portability & Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - sch√ºtzt pers√∂nliche Gesundheitsdaten.
 * `1998`, [US Children's Online Privacy Protection Act (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - sch√ºtzt die Datenprivatsph√§re von Kindern unter 13 Jahren.
 * `2018`, [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) - bietet Nutzerrechte, Datenschutz und Privatsph√§re.
 * `2018`, [California Consumer Privacy Act (CCPA)](https://www.oag.ca.gov/privacy/ccpa) gibt Verbrauchern mehr _Rechte_ √ºber ihre (pers√∂nlichen) Daten.
 * `2021`, Chinas [Gesetz zum Schutz pers√∂nlicher Informationen](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) wurde gerade verabschiedet und schafft eine der weltweit st√§rksten Online-Datenschutzvorschriften.

> üö® Die von der Europ√§ischen Union definierte Datenschutz-Grundverordnung (GDPR) bleibt eine der einflussreichsten Datenschutzvorschriften heute. Wussten Sie, dass sie auch [8 Nutzerrechte](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) definiert, um die digitale Privatsph√§re und pers√∂nlichen Daten der B√ºrger zu sch√ºtzen? Informieren Sie sich dar√ºber, was diese sind und warum sie wichtig sind.

### 4. Ethikkultur

Beachten Sie, dass es eine immaterielle L√ºcke zwischen _Compliance_ (genug tun, um "den Buchstaben des Gesetzes" zu erf√ºllen) und der Bew√§ltigung [systemischer Probleme](https://www.coursera.org/learn/data-science-ethics/home/week/4) (wie Versteinerung, Informationsasymmetrie und Verteilungsungerechtigkeit) gibt, die die Waffenf√§higkeit von KI beschleunigen k√∂nnen.

Letzteres erfordert [kollaborative Ans√§tze zur Definition von Ethikkulturen](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f), die emotionale Verbindungen und konsistente gemeinsame Werte _√ºber Organisationen hinweg_ in der Branche schaffen. Dies erfordert mehr [formalisierte Datenethikkulturen](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) in Organisationen ‚Äì sodass _jeder_ [die Andon-Schnur ziehen](https://en.wikipedia.org/wiki/Andon_(manufacturing)) kann (um ethische Bedenken fr√ºhzeitig zu √§u√üern) und _ethische Bewertungen_ (z. B. bei der Einstellung) ein Kernkriterium f√ºr die Teamzusammenstellung in KI-Projekten werden.

---
## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## √úberpr√ºfung & Selbststudium 

Kurse und B√ºcher helfen, grundlegende Ethikkonzepte und Herausforderungen zu verstehen, w√§hrend Fallstudien und Tools bei der Anwendung ethischer Praktiken in realen Kontexten helfen. Hier sind einige Ressourcen f√ºr den Einstieg:

* [Machine Learning For Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - Lektion √ºber Fairness von Microsoft.
* [Grunds√§tze f√ºr verantwortungsvolle KI](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) ‚Äì kostenloser Lernpfad von Microsoft Learn.  
* [Ethik und Data Science](https://resources.oreilly.com/examples/0636920203964) ‚Äì O'Reilly E-Book (M. Loukides, H. Mason et. al).  
* [Ethik in der Datenwissenschaft](https://www.coursera.org/learn/data-science-ethics#syllabus) ‚Äì Online-Kurs der Universit√§t Michigan.  
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) ‚Äì Fallstudien der Universit√§t Texas.  

# Aufgabe  

[Schreiben Sie eine Fallstudie zur Datenethik](assignment.md)  

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, weisen wir darauf hin, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.