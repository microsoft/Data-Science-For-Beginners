{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Olasılık ve İstatistiğe Giriş\n",
    "Bu not defterinde, daha önce tartıştığımız bazı kavramlarla oynayacağız. Olasılık ve istatistiğin birçok kavramı, Python'da veri işleme için kullanılan `numpy` ve `pandas` gibi büyük kütüphanelerde iyi bir şekilde temsil edilmektedir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rastgele Değişkenler ve Dağılımlar\n",
    "0 ile 9 arasında uniform bir dağılımdan 30 değerlik bir örneklem çizmeyle başlayalım. Ayrıca ortalama ve varyansı da hesaplayacağız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [ random.randint(0,10) for _ in range(30) ]\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Mean = {np.mean(sample)}\")\n",
    "print(f\"Variance = {np.var(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Örnekte kaç farklı değer olduğunu görsel olarak tahmin etmek için **histogram** çizebiliriz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerçek Verilerin Analizi\n",
    "\n",
    "Ortalama ve varyans, gerçek dünya verilerini incelerken çok önemlidir. [SOCR MLB Height/Weight Data](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) sitesinden beyzbol oyuncularına ait verileri yükleyelim.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/SOCR_MLB.tsv\",sep='\\t', header=None, names=['Name','Team','Role','Weight','Height','Age'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Burada veri analizi için [**Pandas**](https://pandas.pydata.org/) adlı bir paket kullanıyoruz. Pandas ve Python'da veri ile çalışma konusu kursun ilerleyen bölümlerinde daha ayrıntılı ele alınacaktır.\n",
    "\n",
    "Yaş, boy ve kilo için ortalama değerleri hesaplayalım:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Age','Height','Weight']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi yükseklik üzerine odaklanalım ve standart sapma ile varyansı hesaplayalım:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df['Height'])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df['Height'].mean()\n",
    "var = df['Height'].var()\n",
    "std = df['Height'].std()\n",
    "print(f\"Mean = {mean}\\nVariance = {var}\\nStandard Deviation = {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ortalamanın yanı sıra, medyan değeri ve çeyrek değerlerine bakmak da mantıklıdır. Bunlar bir **kutu grafiği** kullanılarak görselleştirilebilir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "plt.boxplot(df['Height'].ffill(), vert=False, showmeans=True)\n",
    "plt.grid(color='gray', linestyle='dotted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veri setimizin alt kümelerinin kutu grafikleri de oluşturabiliriz, örneğin, oyuncu rolüne göre gruplanmış olarak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='Height', by='Role', figsize=(10,8))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Not**: Bu diyagram, ortalama olarak birinci kale oyuncularının boylarının ikinci kale oyuncularının boylarından daha uzun olduğunu göstermektedir. Daha sonra, bu hipotezi daha resmi olarak nasıl test edebileceğimizi ve verilerimizin bunu göstermek için istatistiksel olarak anlamlı olduğunu nasıl kanıtlayabileceğimizi öğreneceğiz.  \n",
    "\n",
    "Yaş, boy ve kilo sürekli rastgele değişkenlerdir. Sizce bunların dağılımı nedir? Bunu anlamanın iyi bir yolu değerlerin histogramını çizmektir: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weight'].hist(bins=15, figsize=(10,6))\n",
    "plt.suptitle('Weight distribution of MLB Players')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Dağılım\n",
    "\n",
    "Gerçek verilerimizle aynı ortalama ve varyansa sahip normal dağılımı takip eden yapay bir ağırlık örneği oluşturalım:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = np.random.normal(mean, std, 1000)\n",
    "generated[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(generated, bins=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(np.random.normal(0,1,50000), bins=300)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerçek hayattaki değerlerin çoğu normal dağıldığı için, örnek veri oluşturmak için uniform rastgele sayı üreteci kullanmamalıyız. İşte uniform dağılımla ağırlık oluşturmaya çalışırsak ( `np.random.rand` tarafından oluşturulan) ne olur:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_sample = np.random.rand(1000)*2*std+mean-std\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(wrong_sample)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Güven Aralıkları\n",
    "\n",
    "Şimdi beyzbol oyuncularının kilo ve boyları için güven aralıklarını hesaplayalım. [Bu stackoverflow tartışmasından](https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data) kodu kullanacağız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, h\n",
    "\n",
    "for p in [0.85, 0.9, 0.95]:\n",
    "    m, h = mean_confidence_interval(df['Weight'].fillna(method='pad'),p)\n",
    "    print(f\"p={p:.2f}, mean = {m:.2f} ± {h:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hipotez Testi\n",
    "\n",
    "Hadi beyzbol oyuncuları veri setimizde farklı rolleri inceleyelim:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Role').agg({ 'Weight' : 'mean', 'Height' : 'mean', 'Age' : 'count'}).rename(columns={ 'Age' : 'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birinci Kale Oyuncularının İkinci Kale Oyuncularından daha uzun olduğu hipotezini test edelim. Bunu yapmanın en basit yolu güven aralıklarını test etmektir:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [0.85,0.9,0.95]:\n",
    "    m1, h1 = mean_confidence_interval(df.loc[df['Role']=='First_Baseman',['Height']],p)\n",
    "    m2, h2 = mean_confidence_interval(df.loc[df['Role']=='Second_Baseman',['Height']],p)\n",
    "    print(f'Conf={p:.2f}, 1st basemen height: {m1-h1[0]:.2f}..{m1+h1[0]:.2f}, 2nd basemen height: {m2-h2[0]:.2f}..{m2+h2[0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aralıkların çakışmadığını görebiliyoruz.\n",
    "\n",
    "Hipotezi kanıtlamak için istatistiksel olarak daha doğru bir yol **Student t-testi** kullanmaktır:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "tval, pval = ttest_ind(df.loc[df['Role']=='First_Baseman',['Height']], df.loc[df['Role']=='Second_Baseman',['Height']],equal_var=False)\n",
    "print(f\"T-value = {tval[0]:.2f}\\nP-value: {pval[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ttest_ind` fonksiyonunun döndürdüğü iki değer şunlardır:\n",
    "* p-değeri, iki dağılımın aynı ortalamaya sahip olma olasılığı olarak düşünülebilir. Bizim durumumuzda, bu çok düşüktür, bu da ilk baz oyuncularının daha uzun olduğunu destekleyen güçlü kanıt olduğu anlamına gelir.\n",
    "* t-değeri, t-testinde kullanılan normalize edilmiş ortalama farkının ara değeridir ve belirli bir güven değeri için eşik değerle karşılaştırılır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merkezi Limit Teoremi ile Normal Dağılım Simülasyonu\n",
    "\n",
    "Python'daki sahte rasgele sayı üreteci, bize uniform dağılım sağlaması için tasarlanmıştır. Normal dağılım için bir üreteç oluşturmak istiyorsak, merkezi limit teoremini kullanabiliriz. Normal dağılımlı bir değer elde etmek için, uniform dağılım kullanılarak oluşturulan bir örneklemin ortalamasını hesaplarız.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_random(sample_size=100):\n",
    "    sample = [random.uniform(0,1) for _ in range(sample_size) ]\n",
    "    return sum(sample)/sample_size\n",
    "\n",
    "sample = [normal_random() for _ in range(100)]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(sample)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korelasyon ve Kötü Niyetli Beyzbol Şirketi\n",
    "\n",
    "Korelasyon, veri dizileri arasındaki ilişkileri bulmamıza olanak tanır. Oyuncak örneğimizde, kötücül bir beyzbol şirketinin oyuncularına boylarına göre ödeme yaptığını varsayalım - oyuncu ne kadar uzunsa, o kadar çok para kazanır. Diyelim ki bir taban maaş 1000 $ ve boy uzunluğuna bağlı olarak 0 ile 100 $ arasında ek bir bonus vardır. MLB'den gerçek oyuncuları alacağız ve hayali maaşlarını hesaplayacağız:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = df['Height'].fillna(method='pad')\n",
    "salaries = 1000+(heights-heights.min())/(heights.max()-heights.mean())*100\n",
    "print(list(zip(heights, salaries))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Şimdi bu dizilerin kovaryansını ve korelasyonunu hesaplayalım. `np.cov` bize çoklu değişkenlere genişletilmiş olan **kovaryans matrisini** verecektir. Kovaryans matrisi $M$'nin elemanı $M_{ij}$, giriş değişkenleri $X_i$ ve $X_j$ arasındaki bir korelasyondur ve çapraz değerler $M_{ii}$, $X_{i}$'nin varyansıdır. Benzer şekilde, `np.corrcoef` bize **korelasyon matrisini** verecektir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Covariance matrix:\\n{np.cov(heights, salaries)}\")\n",
    "print(f\"Covariance = {np.cov(heights, salaries)[0,1]}\")\n",
    "print(f\"Correlation = {np.corrcoef(heights, salaries)[0,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir korelasyonun 1'e eşit olması, iki değişken arasında güçlü bir **doğrusal ilişki** olduğu anlamına gelir. Doğrusal ilişkiyi, bir değeri diğerine karşı çizerek görsel olarak görebiliriz:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(heights,salaries)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İlişki doğrusal değilse ne olur, buna bakalım. Diyelim ki şirketimiz, boylar ile maaşlar arasındaki bariz doğrusal bağımlılığı gizlemeye karar verdi ve formüle `sin` gibi bazı doğrusal olmayanlıklar ekledi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = 1000+np.sin((heights-heights.min())/(heights.max()-heights.mean()))*100\n",
    "print(f\"Correlation = {np.corrcoef(heights, salaries)[0,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu durumda, korelasyon biraz daha küçük, ancak yine de oldukça yüksek. Şimdi, ilişkiyi daha az belirgin hale getirmek için maaşa bazı rastgele değişkenler ekleyerek ekstra rastgelelik katmak isteyebiliriz. Bakalım ne olacak:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = 1000+np.sin((heights-heights.min())/(heights.max()-heights.mean()))*100+np.random.random(size=len(heights))*20-10\n",
    "print(f\"Correlation = {np.corrcoef(heights, salaries)[0,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(heights, salaries)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Noktaların neden bu şekilde dikey çizgiler halinde sıralandığını tahmin edebilir misiniz?\n",
    "\n",
    "Maaş gibi yapay olarak tasarlanmış bir kavram ile gözlemlenen değişken *boy* arasındaki korelasyonu gözlemledik. Şimdi de boy ve kilo gibi iki gözlemlenen değişkenin birbirleriyle korelasyon gösterip göstermediğine bakalım:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(df['Height'].ffill(),df['Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maalesef, herhangi bir sonuç alamadık - sadece bazı garip `nan` değerler. Bu, serimizdeki bazı değerlerin tanımsız olması, `nan` olarak temsil edilmesi ve bunun işlemin sonucunun da tanımsız olmasına neden olmasıdır. Matrikse baktığımızda, `Weight` sütununun problemin kaynağı olduğunu görebiliriz, çünkü `Height` değerleri arasındaki kendi kendine korelasyon hesaplanmıştır.\n",
    "\n",
    "> Bu örnek, **veri hazırlama** ve **temizleme**nin önemini göstermektedir. Doğru veriler olmadan hiçbir şey hesaplayamayız.\n",
    "\n",
    "Eksik değerleri doldurmak için `fillna` metodunu kullanalım ve korelasyonu hesaplayalım: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(df['Height'].fillna(method='pad'), df['Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerçekten bir korelasyon vardır, ancak bizim yapay örneğimizdeki kadar güçlü değildir. Gerçekten de, bir değeri diğerine karşı gösteren dağılım grafiğine bakarsak, ilişki çok daha az belirgin olacaktır:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df['Weight'],df['Height'])\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Height')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonuç\n",
    "\n",
    "Bu not defterinde, istatistiksel fonksiyonları hesaplamak için veri üzerinde temel işlemlerin nasıl yapılacağını öğrendik. Artık bazı hipotezleri kanıtlamak için sağlam bir matematik ve istatistik düzenini nasıl kullanacağımızı ve veri örneği verilmiş rastgele değişkenler için güven aralıklarının nasıl hesaplanacağını biliyoruz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**Feragatname**:  \nBu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hatalar veya yanlışlıklar içerebileceğini lütfen unutmayın. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımı sonucu oluşabilecek yanlış anlamalar veya yanlış yorumlardan sorumlu tutulmayız.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86193a1ab0ba47eac1c69c1756090baa3b420b3eea7d4aafab8b85f8b312f0c5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "coopTranslator": {
   "original_hash": "0f899e3c5019f948e7c787b22f3b2304",
   "translation_date": "2026-01-16T13:51:51+00:00",
   "source_file": "1-Introduction/04-stats-and-probability/notebook.ipynb",
   "language_code": "tr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}