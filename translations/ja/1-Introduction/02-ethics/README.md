<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8796f41f566a0a8ebb72863a83d558ed",
  "translation_date": "2025-08-24T12:28:22+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "ja"
}
-->
# データ倫理の入門

|![ スケッチノート by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| データサイエンス倫理 - _スケッチノート by [@nitya](https://twitter.com/nitya)_ |

---

私たちは皆、データ化された世界に生きるデータ市民です。

市場のトレンドによると、2022年までに3分の1の大企業がオンラインの[マーケットプレイスや取引所](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/)を通じてデータを売買するようになると言われています。**アプリ開発者**として、データ駆動型の洞察やアルゴリズム駆動の自動化を日常のユーザー体験に統合することが、より簡単かつ安価になります。しかし、AIが普及するにつれ、そのようなアルゴリズムが大規模に[武器化](https://www.youtube.com/watch?v=TQHs8SA1qpk)されることで引き起こされる潜在的な害についても理解する必要があります。

また、2025年までに[180ゼタバイト](https://www.statista.com/statistics/871513/worldwide-data-created/)以上のデータを生成・消費するという予測もあります。**データサイエンティスト**として、これにより個人データへの前例のないアクセスが可能になります。これにより、ユーザーの行動プロファイルを構築し、[自由選択の幻想](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice)を作り出しながら、ユーザーを私たちが望む結果に誘導することが可能になります。しかし、これにはデータプライバシーやユーザー保護に関する広範な問題も含まれます。

データ倫理は、データサイエンスやエンジニアリングにおける潜在的な害や意図しない結果を最小限に抑えるための_必要なガードレール_です。[GartnerのAIハイプサイクル](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/)は、デジタル倫理、責任あるAI、AIガバナンスに関連するトレンドを、AIの_民主化_と_産業化_に関する大きなメガトレンドの主要な推進要因として特定しています。

![GartnerのAIハイプサイクル - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

このレッスンでは、データ倫理の魅力的な分野を探求します。基本的な概念や課題から、ケーススタディやガバナンスのような応用AIの概念まで、データやAIを扱うチームや組織に倫理文化を確立するための方法を学びます。

## [講義前のクイズ](https://purple-hill-04aebfb03.1.azurestaticapps.net/quiz/2) 🎯

## 基本的な定義

まずは基本的な用語を理解することから始めましょう。

「倫理」という言葉は、[ギリシャ語の「ethikos」](https://en.wikipedia.org/wiki/Ethics)（その語源である「ethos」）に由来し、_性格や道徳的性質_を意味します。

**倫理**とは、社会における私たちの行動を支配する共有価値観や道徳的原則のことです。倫理は法律ではなく、「正しい対間違っている」という広く受け入れられた規範に基づいています。しかし、倫理的な考慮事項は、企業ガバナンスの取り組みや、コンプライアンスを促進する政府規制に影響を与えることがあります。

**データ倫理**は、_データ、アルゴリズム、およびそれに関連する実践_に関連する道徳的問題を「研究し評価する」[新しい倫理の分野](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1)です。ここで、**「データ」**は生成、記録、キュレーション、処理、普及、共有、使用に関連する行動に焦点を当て、**「アルゴリズム」**はAI、エージェント、機械学習、ロボットに焦点を当て、**「実践」**は責任あるイノベーション、プログラミング、ハッキング、倫理規範などのトピックに焦点を当てます。

**応用倫理**は、_現実世界の行動、製品、プロセス_の文脈で倫理的問題を積極的に調査し、それらが定義された倫理的価値観と一致するように是正措置を講じる[道徳的考慮事項の実践的応用](https://en.wikipedia.org/wiki/Applied_ethics)です。

**倫理文化**は、[応用倫理を_運用化_](https://hbr.org/2019/05/how-to-design-an-ethical-organization)することで、倫理的原則と実践が組織全体で一貫してスケーラブルに採用されることを保証します。成功する倫理文化は、組織全体の倫理原則を定義し、コンプライアンスに対する意味のあるインセンティブを提供し、組織のあらゆるレベルで望ましい行動を奨励し強化することで倫理規範を強化します。

## 倫理の概念

このセクションでは、データ倫理における**共有価値観**（原則）や**倫理的課題**（問題）といった概念を議論し、これらの概念を現実世界の文脈で理解するための**ケーススタディ**を探求します。

### 1. 倫理原則

すべてのデータ倫理戦略は、_倫理原則_を定義することから始まります。これは、データやAIプロジェクトにおける許容可能な行動を記述し、コンプライアンス行動を導く「共有価値観」です。これらは個人やチームレベルで定義することができますが、多くの大企業では、企業レベルで定義され、すべてのチームで一貫して実施される_倫理的AI_のミッションステートメントやフレームワークにまとめられています。

**例:** Microsoftの[責任あるAI](https://www.microsoft.com/en-us/ai/responsible-ai)ミッションステートメントは次のように述べています：_「私たちは、人々を第一に考える倫理原則によって推進されるAIの進歩に取り組んでいます」_ - 以下の6つの倫理原則を特定しています：

![Microsoftの責任あるAI](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

これらの原則を簡単に見てみましょう。_透明性_と_説明責任_は他の原則の基盤となる価値観であるため、ここから始めます：

* [**説明責任**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - 実務者がデータとAIの運用に対して_責任_を持ち、これらの倫理原則を遵守することを保証します。
* [**透明性**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - データとAIの行動がユーザーにとって_理解可能_（解釈可能）であり、意思決定の背景を説明します。
* [**公平性**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - AIが_すべての人々_を公平に扱うことを保証し、データやシステムにおける体系的または暗黙的な社会技術的バイアスに対処します。
* [**信頼性と安全性**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - AIが定義された価値観に_一貫して_従い、潜在的な害や意図しない結果を最小限に抑えることを保証します。
* [**プライバシーとセキュリティ**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - データの系譜を理解し、ユーザーに_データプライバシーと関連する保護_を提供します。
* [**包括性**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - 意図を持ってAIソリューションを設計し、_幅広い人間のニーズ_と能力に適応させることを目指します。

> 🚨 あなたのデータ倫理ミッションステートメントはどのようなものになるでしょうか。他の組織の倫理的AIフレームワークを探求してみてください。例えば、[IBM](https://www.ibm.com/cloud/learn/ai-ethics)、[Google](https://ai.google/principles)、[Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/)の例があります。これらの共通の価値観は何でしょうか？これらの原則は、それぞれのAI製品や業界にどのように関連していますか？

### 2. 倫理的課題

倫理原則が定義されたら、次のステップは、データとAIの行動がこれらの共有価値観と一致しているかどうかを評価することです。行動を_データ収集_と_アルゴリズム設計_の2つのカテゴリに分けて考えてみましょう。

データ収集では、行動は**個人データ**や個人を特定できる情報（PII）に関わる可能性が高いです。これには、[多様な非個人データ](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en)が_集合的に_個人を特定するものも含まれます。倫理的課題は、_データプライバシー_、_データ所有権_、および_インフォームドコンセント_や_知的財産権_といった関連トピックに関連します。

アルゴリズム設計では、行動は**データセット**の収集とキュレーション、そしてそれを使用して現実世界の文脈で結果を予測したり意思決定を自動化する**データモデル**のトレーニングと展開に関わります。倫理的課題は、_データセットのバイアス_、_データ品質_の問題、_不公平性_、およびアルゴリズムの_誤表現_に関連する可能性があります。これには、体系的な問題も含まれることがあります。

どちらの場合も、倫理的課題は、行動が共有価値観と衝突する可能性のある領域を強調します。これらの懸念を検出、軽減、最小化、または排除するためには、行動に関連する道徳的な「はい/いいえ」の質問を行い、必要に応じて是正措置を講じる必要があります。以下に、いくつかの倫理的課題とそれが提起する道徳的な質問を見てみましょう：

#### 2.1 データ所有権

データ収集は、データ主体を特定できる個人データを含むことがよくあります。[データ所有権](https://permission.io/blog/data-ownership)は、データの作成、処理、普及に関連する_コントロール_と[_ユーザー権利_](https://permission.io/blog/data-ownership)に関するものです。

考えるべき道徳的な質問は次の通りです：
 * データの所有者は誰か？（ユーザーまたは組織）
 * データ主体にはどのような権利があるか？（例：アクセス、削除、移植性）
 * 組織にはどのような権利があるか？（例：悪意のあるユーザーレビューの修正）

#### 2.2 インフォームドコンセント

[インフォームドコンセント](https://legaldictionary.net/informed-consent/)は、ユーザーが（データ収集のような）行動に同意する際に、目的、潜在的リスク、代替案などの関連事実を_完全に理解_していることを定義します。

ここで探求すべき質問は次の通りです：
 * ユーザー（データ主体）はデータの収集と使用に許可を与えたか？
 * ユーザーはそのデータが収集された目的を理解していたか？
 * ユーザーは参加による潜在的リスクを理解していたか？

#### 2.3 知的財産

[知的財産](https://en.wikipedia.org/wiki/Intellectual_property)は、人間の創意工夫から生まれた無形の創造物で、個人や企業にとって_経済的価値_を持つ可能性があります。

ここで探求すべき質問は次の通りです：
 * 収集されたデータはユーザーや企業にとって経済的価値を持っていたか？
 * **ユーザー**にはここで知的財産があるか？
 * **組織**にはここで知的財産があるか？
 * これらの権利が存在する場合、それをどのように保護しているか？

#### 2.4 データプライバシー

[データプライバシー](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/)または情報プライバシーは、個人を特定できる情報に関して、ユーザーのプライバシーを保護し、ユーザーの身元を守ることを指します。

ここで探求すべき質問は次の通りです：
 * ユーザーの（個人）データはハッキングや漏洩から保護されているか？
 * ユーザーデータは許可されたユーザーや文脈にのみアクセス可能か？
 * データが共有または普及される際にユーザーの匿名性は保たれているか？
 * 匿名化されたデータセットからユーザーを再識別できるか？

#### 2.5 忘れられる権利

[忘れられる権利](https://en.wikipedia.org/wiki/Right_to_be_forgotten)または[削除権](https://www.gdpreu.org/right-to-be-forgotten/)は、ユーザーに追加の個人データ保護を提供します。具体的には、特定の状況下で、インターネット検索やその他の場所から個人データの削除または除去を要求する権利をユーザーに与えます。これにより、過去の行動が将来にわたって不利に働かないように、オンラインで新たなスタートを切ることができます。

ここで探求すべき質問は次の通りです：
 * システムはデータ主体が削除を要求することを許可しているか？
 * ユーザーの同意の撤回が自動削除を引き起こすべきか？
 * データは同意なしまたは違法な手段で収集されたか？
 * データプライバシーに関する政府規制に準拠しているか？

#### 2.6 データセットのバイアス

データセットまたは[収集バイアス](http://researcharticles.com/index.php/bias-in-data-collection-in-research/)は、アルゴリズム開発のために_非代表的な_データのサブセットを選択することで、多様なグループに対して結果が不公平になる可能性を指します。バイアスの種類には、選択バイアス、サンプリングバイアス、ボランティアバイアス、機器バイアスなどがあります。

ここで探求すべき質問は次の通りです：
 * 代表的なデータ主体のセットを募集したか？
 * 収集またはキュレーションされたデータセットをさま
[アルゴリズムの公平性](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f)は、アルゴリズムの設計が特定のデータ主体のサブグループに対して体系的に差別を行い、_資源の配分_（そのグループに資源が拒否または提供されない場合）や_サービスの質_（AIが特定のサブグループに対して他のグループほど正確でない場合）における[潜在的な被害](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml)を引き起こしていないかを確認するものです。

ここで検討すべき質問は以下の通りです：
 * 多様なサブグループや条件に対してモデルの精度を評価しましたか？
 * 潜在的な被害（例：ステレオタイプ化）についてシステムを精査しましたか？
 * 特定された被害を軽減するためにデータを修正したり、モデルを再学習させることができますか？

[AIの公平性チェックリスト](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA)などのリソースを活用して、さらに学びましょう。

#### 2.9 誤った表現

[データの誤表現](https://www.sciencedirect.com/topics/computer-science/misrepresentation)とは、正直に報告されたデータから得られた洞察を、望ましいストーリーを支持するために欺瞞的に伝えていないかを問うことです。

ここで検討すべき質問は以下の通りです：
 * 不完全または不正確なデータを報告していませんか？
 * 誤解を招く結論を導くような方法でデータを可視化していませんか？
 * 結果を操作するために選択的な統計手法を使用していませんか？
 * 別の結論を導く可能性のある代替説明はありませんか？

#### 2.10 自由選択

[自由選択の錯覚](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice)は、システムの「選択アーキテクチャ」が意思決定アルゴリズムを使用して、人々に選択肢やコントロールを与えているように見せかけながら、好ましい結果を取るよう促す場合に発生します。これらの[ダークパターン](https://www.darkpatterns.org/)は、ユーザーに社会的および経済的な被害をもたらす可能性があります。ユーザーの意思決定が行動プロファイルに影響を与えるため、これらの行動は将来の選択に影響を与え、被害の影響を拡大または延長する可能性があります。

ここで検討すべき質問は以下の通りです：
 * ユーザーはその選択を行うことの影響を理解していましたか？
 * ユーザーは（代替の）選択肢とそれぞれの利点と欠点を認識していましたか？
 * ユーザーは自動化された選択や影響を受けた選択を後で取り消すことができますか？

### 3. ケーススタディ

これらの倫理的課題を現実世界の文脈で考えるには、倫理違反が見過ごされた場合に個人や社会にどのような潜在的な被害や結果が生じるかを示すケーススタディを検討することが役立ちます。

以下はいくつかの例です：

| 倫理的課題 | ケーススタディ  | 
|--- |--- |
| **インフォームド・コンセント** | 1972年 - [タスキギー梅毒研究](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - 研究に参加したアフリカ系アメリカ人男性は無料の医療を約束されましたが、研究者によって診断や治療の利用可能性について知らされないまま騙されました。多くの被験者が死亡し、パートナーや子供にも影響が及びました。この研究は40年間続きました。 | 
| **データプライバシー** | 2007年 - [Netflixデータ賞](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/)では、研究者に_50,000人の顧客からの1,000万件の匿名化された映画評価_が提供され、推薦アルゴリズムの改善が試みられました。しかし、研究者は匿名化されたデータを外部データセット（例：IMDbのコメント）と照合することで、Netflixの一部の利用者を「再識別」することができました。|
| **収集バイアス** | 2013年 - ボストン市は[Street Bump](https://www.boston.gov/transportation/street-bump)というアプリを開発し、市民が道路の穴を報告できるようにしました。これにより、市は道路データを改善し、問題を特定して修正することができました。しかし、[低所得層の人々は車や携帯電話へのアクセスが少ない](https://hbr.org/2013/04/the-hidden-biases-in-big-data)ため、これらのアプリでは彼らの道路問題が見えなくなっていました。開発者は公平性のために_アクセスの平等性とデジタル格差_の問題に取り組むため、学術関係者と協力しました。 |
| **アルゴリズムの公平性** | 2018年 - MITの[Gender Shades Study](http://gendershades.org/overview.html)は、性別分類AI製品の精度を評価し、女性や有色人種に対する精度のギャップを明らかにしました。[2019年のApple Card](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/)では、男性よりも女性に対してクレジットが少なく提供されるように見えました。これらは、アルゴリズムのバイアスが社会経済的な被害を引き起こす問題を示しています。|
| **データの誤表現** | 2020年 - [ジョージア州公衆衛生局が発表したCOVID-19のグラフ](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening)は、x軸の非時系列順序によって、確認された症例の傾向について市民を誤解させるように見えました。これは、視覚化のトリックによる誤表現を示しています。 |
| **自由選択の錯覚** | 2020年 - 学習アプリ[ABCmouseがFTCの苦情を解決するために1,000万ドルを支払った](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/)ケースでは、親がキャンセルできないサブスクリプションを支払うように追い込まれました。これは、ユーザーが潜在的に有害な選択をするよう促される選択アーキテクチャにおけるダークパターンを示しています。 |
| **データプライバシーとユーザーの権利** | 2021年 - Facebookの[データ漏洩](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users)では、5億3,000万人のユーザーのデータが漏洩し、FTCに50億ドルの和解金を支払う結果となりました。しかし、Facebookは漏洩についてユーザーに通知することを拒否し、データの透明性とアクセスに関するユーザーの権利を侵害しました。|

さらにケーススタディを探したいですか？以下のリソースをチェックしてください：
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - 多様な業界における倫理的ジレンマ。
* [データサイエンス倫理コース](https://www.coursera.org/learn/data-science-ethics#syllabus) - 重要なケーススタディを探求。
* [問題が発生した事例](https://deon.drivendata.org/examples/) - Deonチェックリストとその例。

> 🚨 あなたが見たケーススタディについて考えてみてください。これまでに似たような倫理的課題を経験したり、影響を受けたりしたことがありますか？このセクションで議論した倫理的課題の1つを示す別のケーススタディを少なくとも1つ考えることができますか？

## 応用倫理

これまでに倫理の概念、課題、そして現実世界の文脈でのケーススタディについて話しました。しかし、プロジェクトで倫理的な原則や実践を_適用_するにはどうすればよいでしょうか？また、これらの実践を_運用化_してより良いガバナンスを実現するにはどうすればよいでしょうか？いくつかの現実的な解決策を探ってみましょう：

### 1. プロフェッショナルコード

プロフェッショナルコードは、組織が倫理的原則やミッションステートメントを支持するようメンバーを「奨励」するための1つの方法です。コードはプロフェッショナルな行動のための_道徳的ガイドライン_であり、従業員やメンバーが組織の原則に沿った意思決定を行うのを助けます。これらはメンバーの自主的な遵守に依存しますが、多くの組織は遵守を促すために追加の報酬や罰則を提供しています。

例：
 * [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) 倫理規範
 * [データサイエンス協会](http://datascienceassn.org/code-of-conduct.html) 行動規範（2013年作成）
 * [ACM倫理規範とプロフェッショナル行動規範](https://www.acm.org/code-of-ethics)（1993年以降）

> 🚨 あなたはプロフェッショナルなエンジニアリングまたはデータサイエンスの組織に所属していますか？そのウェブサイトを調べて、プロフェッショナルな倫理規範を定義しているかどうかを確認してください。それはどのような倫理的原則を示していますか？メンバーがその規範を遵守するようにどのように「奨励」していますか？

### 2. 倫理チェックリスト

プロフェッショナルコードは実践者に求められる_倫理的行動_を定義しますが、大規模プロジェクトにおける[実施の限界](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md)が知られています。その代わりに、多くのデータサイエンスの専門家は[チェックリスト](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md)を推奨しています。これにより、原則をより決定論的で実行可能な方法で実践に結びつけることができます。

チェックリストは質問を「はい/いいえ」のタスクに変換し、運用化することで、標準的な製品リリースワークフローの一部として追跡可能にします。

例：
 * [Deon](https://deon.drivendata.org/) - [業界の推奨事項](https://deon.drivendata.org/#checklist-citations)に基づいて作成された汎用データ倫理チェックリストで、コマンドラインツールを使用して簡単に統合可能。
 * [プライバシー監査チェックリスト](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - 法的および社会的なリスクの観点から情報取り扱いの一般的なガイダンスを提供。
 * [AI公平性チェックリスト](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - AI開発サイクルに公平性チェックを統合するためにAI実践者によって作成。
 * [データとAIにおける倫理のための22の質問](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - 設計、実装、組織的文脈における倫理的問題の初期探索のためのよりオープンなフレームワーク。

### 3. 倫理規制

倫理は共有価値を定義し、自発的に正しいことを行うことに関するものです。一方、**コンプライアンス**は、定義されている場合に_法律を遵守する_ことに関するものです。**ガバナンス**は、組織が倫理的原則を実施し、確立された法律を遵守するために運営するすべての方法を広くカバーします。

今日、ガバナンスは組織内で2つの形を取ります。1つ目は、**倫理的AI**の原則を定義し、組織内のすべてのAI関連プロジェクトで採用を運用化するための実践を確立することです。2つ目は、組織が運営する地域で政府が義務付けたすべての**データ保護規制**を遵守することです。

データ保護およびプライバシー規制の例：

 * `1974年`、[米国プライバシー法](https://www.justice.gov/opcl/privacy-act-1974) - _連邦政府_による個人情報の収集、使用、開示を規制。
 * `1996年`、[米国医療保険の携行性と責任に関する法律（HIPAA）](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - 個人の健康データを保護。
 * `1998年`、[米国児童オンラインプライバシー保護法（COPPA）](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - 13歳未満の子供のデータプライバシーを保護。
 * `2018年`、[一般データ保護規則（GDPR）](https://gdpr-info.eu/) - ユーザーの権利、データ保護、プライバシーを提供。
 * `2018年`、[カリフォルニア州消費者プライバシー法（CCPA）](https://www.oag.ca.gov/privacy/ccpa) - 消費者に（個人）データに関するより多くの_権利_を付与。
 * `2021年`、中国の[個人情報保護法](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) - 世界で最も強力なオンラインデータプライバシー規制の1つを制定。

> 🚨 欧州連合が定義したGDPR（一般データ保護規則）は、今日最も影響力のあるデータプライバシー規制の1つです。GDPRは市民のデジタルプライバシーと個人データを保護するための[8つのユーザー権利](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr)も定義していることをご存知ですか？これらが何であるか、そしてなぜ重要なのかを学んでください。

### 4. 倫理文化

_コンプライアンス_（「法律の文言」を満たすために十分なことを行う）と、AIの武器化を加速させる可能性のある[システム的な問題](https://www.coursera.org/learn/data-science-ethics/home/week/4)（硬直化、情報の非対称性、分配の不公平性など）に対処することの間には、依然として無形のギャップがあります。

後者には、業界内で一貫した共有価値と感情的なつながりを構築する[倫理文化を定義するための協力的アプローチ](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f)が必要です。これには、組織内
* [責任あるAIの原則](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - Microsoft Learnの無料学習パス。
* [倫理とデータサイエンス](https://resources.oreilly.com/examples/0636920203964) - O'Reillyの電子書籍 (M. Loukides, H. Mason 他)
* [データサイエンス倫理](https://www.coursera.org/learn/data-science-ethics#syllabus) - ミシガン大学のオンラインコース。
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - テキサス大学のケーススタディ。

# 課題

[データ倫理のケーススタディを書く](assignment.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解釈について、当社は責任を負いません。