<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1341f6da63d434f5ba31b08ea951b02c",
  "translation_date": "2025-09-05T13:28:47+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "pt"
}
-->
# Introdu√ß√£o √† √âtica de Dados

|![ Sketchnote por [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| √âtica em Ci√™ncia de Dados - _Sketchnote por [@nitya](https://twitter.com/nitya)_ |

---

Somos todos cidad√£os de dados vivendo num mundo dataficado.

As tend√™ncias de mercado indicam que, at√© 2022, 1 em cada 3 grandes organiza√ß√µes comprar√° e vender√° os seus dados atrav√©s de [Mercados e Bolsas](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) online. Como **Desenvolvedores de Aplica√ß√µes**, ser√° mais f√°cil e barato integrar insights baseados em dados e automa√ß√£o orientada por algoritmos nas experi√™ncias di√°rias dos utilizadores. Mas, √† medida que a IA se torna mais presente, tamb√©m ser√° necess√°rio compreender os potenciais danos causados pela [arma√ß√£o](https://www.youtube.com/watch?v=TQHs8SA1qpk) desses algoritmos em larga escala.

As tend√™ncias tamb√©m indicam que criaremos e consumiremos mais de [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) de dados at√© 2025. Como **Cientistas de Dados**, isso nos d√° n√≠veis sem precedentes de acesso a dados pessoais. Isso significa que podemos construir perfis comportamentais de utilizadores e influenciar a tomada de decis√µes de formas que criam uma [ilus√£o de escolha livre](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice), enquanto potencialmente direcionamos os utilizadores para resultados que preferimos. Isso tamb√©m levanta quest√µes mais amplas sobre privacidade de dados e prote√ß√£o dos utilizadores.

A √©tica de dados √© agora _uma barreira necess√°ria_ para a ci√™ncia e engenharia de dados, ajudando-nos a minimizar potenciais danos e consequ√™ncias n√£o intencionais das nossas a√ß√µes orientadas por dados. O [Ciclo de Hype da Gartner para IA](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifica tend√™ncias relevantes em √©tica digital, IA respons√°vel e governan√ßa de IA como motores-chave para megatend√™ncias maiores em torno da _democratiza√ß√£o_ e _industrializa√ß√£o_ da IA.

![Ciclo de Hype da Gartner para IA - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

Nesta li√ß√£o, exploraremos a √°rea fascinante da √©tica de dados - desde conceitos e desafios fundamentais at√© estudos de caso e conceitos aplicados de IA, como governan√ßa - que ajudam a estabelecer uma cultura √©tica em equipas e organiza√ß√µes que trabalham com dados e IA.

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Defini√ß√µes B√°sicas

Vamos come√ßar por entender a terminologia b√°sica.

A palavra "√©tica" vem da [palavra grega "ethikos"](https://en.wikipedia.org/wiki/Ethics) (e sua raiz "ethos"), que significa _car√°ter ou natureza moral_.

**√âtica** trata dos valores compartilhados e princ√≠pios morais que governam o nosso comportamento na sociedade. A √©tica n√£o se baseia em leis, mas em normas amplamente aceitas sobre o que √© "certo versus errado". No entanto, considera√ß√µes √©ticas podem influenciar iniciativas de governan√ßa corporativa e regulamenta√ß√µes governamentais que criam mais incentivos para conformidade.

**√âtica de Dados** √© um [novo ramo da √©tica](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) que "estuda e avalia problemas morais relacionados a _dados, algoritmos e pr√°ticas correspondentes_". Aqui, **"dados"** foca em a√ß√µes relacionadas √† gera√ß√£o, grava√ß√£o, curadoria, processamento, dissemina√ß√£o, partilha e uso; **"algoritmos"** foca em IA, agentes, aprendizagem autom√°tica e rob√¥s; e **"pr√°ticas"** foca em t√≥picos como inova√ß√£o respons√°vel, programa√ß√£o, hacking e c√≥digos de √©tica.

**√âtica Aplicada** √© a [aplica√ß√£o pr√°tica de considera√ß√µes morais](https://en.wikipedia.org/wiki/Applied_ethics). √â o processo de investigar ativamente quest√µes √©ticas no contexto de _a√ß√µes, produtos e processos do mundo real_, e tomar medidas corretivas para garantir que permane√ßam alinhados com os nossos valores √©ticos definidos.

**Cultura √âtica** trata de [_operacionalizar_ a √©tica aplicada](https://hbr.org/2019/05/how-to-design-an-ethical-organization) para garantir que os nossos princ√≠pios e pr√°ticas √©ticas sejam adotados de forma consistente e escal√°vel em toda a organiza√ß√£o. Culturas √©ticas bem-sucedidas definem princ√≠pios √©ticos em toda a organiza√ß√£o, fornecem incentivos significativos para conformidade e refor√ßam normas √©ticas ao encorajar e amplificar comportamentos desejados em todos os n√≠veis da organiza√ß√£o.

## Conceitos de √âtica

Nesta sec√ß√£o, discutiremos conceitos como **valores compartilhados** (princ√≠pios) e **desafios √©ticos** (problemas) para √©tica de dados - e exploraremos **estudos de caso** que ajudam a entender esses conceitos em contextos do mundo real.

### 1. Princ√≠pios √âticos

Toda estrat√©gia de √©tica de dados come√ßa por definir _princ√≠pios √©ticos_ - os "valores compartilhados" que descrevem comportamentos aceit√°veis e orientam a√ß√µes conformes nos nossos projetos de dados e IA. Pode-se defini-los a n√≠vel individual ou de equipa. No entanto, a maioria das grandes organiza√ß√µes delineia isso numa declara√ß√£o de miss√£o ou estrutura de _IA √©tica_ definida a n√≠vel corporativo e aplicada consistentemente em todas as equipas.

**Exemplo:** A declara√ß√£o de miss√£o de [IA Respons√°vel](https://www.microsoft.com/en-us/ai/responsible-ai) da Microsoft diz: _"Estamos comprometidos com o avan√ßo da IA orientada por princ√≠pios √©ticos que colocam as pessoas em primeiro lugar"_ - identificando 6 princ√≠pios √©ticos na estrutura abaixo:

![IA Respons√°vel na Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Vamos explorar brevemente esses princ√≠pios. _Transpar√™ncia_ e _responsabilidade_ s√£o valores fundamentais sobre os quais outros princ√≠pios s√£o constru√≠dos - ent√£o vamos come√ßar por a√≠:

* [**Responsabilidade**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) torna os profissionais _respons√°veis_ pelas suas opera√ß√µes de dados e IA, e pela conformidade com esses princ√≠pios √©ticos.
* [**Transpar√™ncia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) garante que as a√ß√µes de dados e IA sejam _compreens√≠veis_ (interpret√°veis) para os utilizadores, explicando o qu√™ e o porqu√™ por tr√°s das decis√µes.
* [**Justi√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - foca em garantir que a IA trate _todas as pessoas_ de forma justa, abordando quaisquer preconceitos sociot√©cnicos sist√©micos ou impl√≠citos nos dados e sistemas.
* [**Fiabilidade e Seguran√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - garante que a IA se comporte de forma _consistente_ com os valores definidos, minimizando potenciais danos ou consequ√™ncias n√£o intencionais.
* [**Privacidade e Seguran√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de entender a origem dos dados e fornecer _privacidade de dados e prote√ß√µes relacionadas_ aos utilizadores.
* [**Inclus√£o**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de projetar solu√ß√µes de IA com inten√ß√£o, adaptando-as para atender a uma _ampla gama de necessidades e capacidades humanas_.

> üö® Pense no que poderia ser a sua declara√ß√£o de miss√£o de √©tica de dados. Explore estruturas de IA √©tica de outras organiza√ß√µes - aqui est√£o exemplos da [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) e [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Quais valores compartilhados t√™m em comum? Como esses princ√≠pios se relacionam com o produto ou ind√∫stria de IA em que operam?

### 2. Desafios √âticos

Depois de definir os princ√≠pios √©ticos, o pr√≥ximo passo √© avaliar as nossas a√ß√µes de dados e IA para ver se est√£o alinhadas com esses valores compartilhados. Pense nas suas a√ß√µes em duas categorias: _coleta de dados_ e _design de algoritmos_.

Na coleta de dados, as a√ß√µes provavelmente envolver√£o **dados pessoais** ou informa√ß√µes pessoalmente identific√°veis (PII) de indiv√≠duos identific√°veis. Isso inclui [diversos itens de dados n√£o pessoais](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) que, _coletivamente_, identificam um indiv√≠duo. Os desafios √©ticos podem estar relacionados √† _privacidade de dados_, _propriedade de dados_ e t√≥picos relacionados, como _consentimento informado_ e _direitos de propriedade intelectual_ dos utilizadores.

No design de algoritmos, as a√ß√µes envolver√£o a coleta e curadoria de **conjuntos de dados**, e o uso deles para treinar e implementar **modelos de dados** que preveem resultados ou automatizam decis√µes em contextos do mundo real. Os desafios √©ticos podem surgir de _vi√©s nos conjuntos de dados_, problemas de _qualidade dos dados_, _injusti√ßa_ e _m√° representa√ß√£o_ nos algoritmos - incluindo alguns problemas que s√£o sist√©micos por natureza.

Em ambos os casos, os desafios √©ticos destacam √°reas onde as nossas a√ß√µes podem entrar em conflito com os nossos valores compartilhados. Para detetar, mitigar, minimizar ou eliminar essas preocupa√ß√µes, precisamos fazer perguntas morais "sim/n√£o" relacionadas √†s nossas a√ß√µes e tomar medidas corretivas conforme necess√°rio. Vamos dar uma olhada em alguns desafios √©ticos e nas perguntas morais que eles levantam:

#### 2.1 Propriedade de Dados

A coleta de dados muitas vezes envolve dados pessoais que podem identificar os sujeitos dos dados. [Propriedade de dados](https://permission.io/blog/data-ownership) trata do _controlo_ e [_direitos dos utilizadores_](https://permission.io/blog/data-ownership) relacionados √† cria√ß√£o, processamento e dissemina√ß√£o de dados.

As perguntas morais que precisamos fazer s√£o:
 * Quem √© o propriet√°rio dos dados? (utilizador ou organiza√ß√£o)
 * Quais direitos t√™m os sujeitos dos dados? (ex: acesso, elimina√ß√£o, portabilidade)
 * Quais direitos t√™m as organiza√ß√µes? (ex: retificar avalia√ß√µes maliciosas de utilizadores)

#### 2.2 Consentimento Informado

[Consentimento informado](https://legaldictionary.net/informed-consent/) define o ato de os utilizadores concordarem com uma a√ß√£o (como coleta de dados) com um _entendimento completo_ dos factos relevantes, incluindo o prop√≥sito, os riscos potenciais e as alternativas.

Perguntas a explorar aqui s√£o:
 * O utilizador (sujeito dos dados) deu permiss√£o para a captura e uso dos dados?
 * O utilizador compreendeu o prop√≥sito para o qual os dados foram capturados?
 * O utilizador compreendeu os riscos potenciais da sua participa√ß√£o?

#### 2.3 Propriedade Intelectual

[Propriedade intelectual](https://en.wikipedia.org/wiki/Intellectual_property) refere-se a cria√ß√µes intang√≠veis resultantes da iniciativa humana, que podem _ter valor econ√≥mico_ para indiv√≠duos ou empresas.

Perguntas a explorar aqui s√£o:
 * Os dados coletados t√™m valor econ√≥mico para um utilizador ou empresa?
 * O **utilizador** tem propriedade intelectual aqui?
 * A **organiza√ß√£o** tem propriedade intelectual aqui?
 * Se esses direitos existirem, como estamos a proteg√™-los?

#### 2.4 Privacidade de Dados

[Privacidade de dados](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) ou privacidade da informa√ß√£o refere-se √† preserva√ß√£o da privacidade do utilizador e prote√ß√£o da identidade do utilizador em rela√ß√£o a informa√ß√µes pessoalmente identific√°veis.

Perguntas a explorar aqui s√£o:
 * Os dados (pessoais) dos utilizadores est√£o protegidos contra ataques e vazamentos?
 * Os dados dos utilizadores est√£o acess√≠veis apenas a utilizadores e contextos autorizados?
 * A anonimidade dos utilizadores √© preservada quando os dados s√£o partilhados ou disseminados?
 * Um utilizador pode ser desidentificado de conjuntos de dados anonimizados?

#### 2.5 Direito ao Esquecimento

O [Direito ao Esquecimento](https://en.wikipedia.org/wiki/Right_to_be_forgotten) ou [Direito √† Elimina√ß√£o](https://www.gdpreu.org/right-to-be-forgotten/) fornece prote√ß√£o adicional de dados pessoais aos utilizadores. Especificamente, d√° aos utilizadores o direito de solicitar a elimina√ß√£o ou remo√ß√£o de dados pessoais de pesquisas na Internet e outros locais, _sob circunst√¢ncias espec√≠ficas_ - permitindo-lhes um novo come√ßo online sem que a√ß√µes passadas sejam usadas contra eles.

Perguntas a explorar aqui s√£o:
 * O sistema permite que os sujeitos dos dados solicitem a elimina√ß√£o?
 * A retirada do consentimento do utilizador deve acionar a elimina√ß√£o autom√°tica?
 * Os dados foram coletados sem consentimento ou por meios ilegais?
 * Estamos em conformidade com as regulamenta√ß√µes governamentais de privacidade de dados?

#### 2.6 Vi√©s nos Conjuntos de Dados

Vi√©s nos conjuntos de dados ou [Vi√©s de Coleta](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) trata da sele√ß√£o de um subconjunto _n√£o representativo_ de dados para o desenvolvimento de algoritmos, criando potencial injusti√ßa nos resultados para diversos grupos. Tipos de vi√©s incluem vi√©s de sele√ß√£o ou amostragem, vi√©s de voluntariado e vi√©s de instrumento.

Perguntas a explorar aqui s√£o:
 * Recrut√°mos um conjunto representativo de sujeitos dos dados?
 * Test√°mos o nosso conjunto de dados coletado ou curado para v√°rios tipos de vi√©s?
 * Podemos mitigar ou remover quaisquer vieses descobertos?

#### 2.7 Qualidade dos Dados

[Qualidade dos Dados](https://lakefs.io/data-quality-testing/) analisa a validade do conjunto de dados curado usado para desenvolver os nossos algoritmos, verificando se as caracter√≠sticas e os registos atendem aos requisitos para o n√≠vel de precis√£o e consist√™ncia necess√°rio para o nosso prop√≥sito de IA.

Perguntas a explorar aqui s√£o:
 * Captur√°mos caracter√≠sticas v√°lidas para o nosso caso de uso?
 * Os dados foram capturados de forma consistente em diversas fontes de dados?
 * O conjunto de dados est√° completo para diversas condi√ß√µes ou cen√°rios?
 * As informa√ß√µes capturadas refletem a realidade com precis√£o?
[Justi√ßa Algor√≠tmica](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) verifica se o design do algoritmo discrimina sistematicamente subgrupos espec√≠ficos de sujeitos de dados, levando a [potenciais danos](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) em _aloca√ß√£o_ (quando recursos s√£o negados ou retidos desse grupo) e _qualidade do servi√ßo_ (quando a IA n√£o √© t√£o precisa para alguns subgrupos quanto para outros).

Quest√µes a explorar aqui s√£o:
 * Avali√°mos a precis√£o do modelo para subgrupos e condi√ß√µes diversas?
 * Examin√°mos o sistema para identificar potenciais danos (por exemplo, estere√≥tipos)?
 * Podemos rever os dados ou re-treinar os modelos para mitigar os danos identificados?

Explore recursos como [checklists de Justi√ßa em IA](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) para saber mais.

#### 2.9 Distor√ß√£o de Dados

[Distor√ß√£o de Dados](https://www.sciencedirect.com/topics/computer-science/misrepresentation) refere-se a questionar se estamos a comunicar insights de dados relatados de forma honesta, mas de maneira enganosa, para apoiar uma narrativa desejada.

Quest√µes a explorar aqui s√£o:
 * Estamos a relatar dados incompletos ou imprecisos?
 * Estamos a visualizar dados de forma a induzir conclus√µes enganosas?
 * Estamos a usar t√©cnicas estat√≠sticas seletivas para manipular resultados?
 * Existem explica√ß√µes alternativas que possam oferecer uma conclus√£o diferente?

#### 2.10 Livre Arb√≠trio
A [Ilus√£o do Livre Arb√≠trio](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) ocorre quando "arquiteturas de escolha" do sistema usam algoritmos de decis√£o para influenciar as pessoas a tomarem um resultado preferido, enquanto aparentam dar-lhes op√ß√µes e controlo. Estes [padr√µes obscuros](https://www.darkpatterns.org/) podem causar danos sociais e econ√≥micos aos utilizadores. Como as decis√µes dos utilizadores impactam os perfis de comportamento, essas a√ß√µes podem potencialmente impulsionar escolhas futuras que ampliam ou prolongam o impacto desses danos.

Quest√µes a explorar aqui s√£o:
 * O utilizador compreendeu as implica√ß√µes de fazer essa escolha?
 * O utilizador estava ciente das (alternativas) escolhas e dos pr√≥s e contras de cada uma?
 * O utilizador pode reverter uma escolha automatizada ou influenciada mais tarde?

### 3. Estudos de Caso

Para colocar esses desafios √©ticos em contextos do mundo real, √© √∫til analisar estudos de caso que destacam os potenciais danos e consequ√™ncias para indiv√≠duos e para a sociedade, quando essas viola√ß√µes √©ticas s√£o ignoradas.

Aqui est√£o alguns exemplos:

| Desafio √âtico | Estudo de Caso  | 
|--- |--- |
| **Consentimento Informado** | 1972 - [Estudo de S√≠filis de Tuskegee](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Homens afro-americanos que participaram no estudo foram prometidos cuidados m√©dicos gratuitos, _mas foram enganados_ por investigadores que n√£o informaram os sujeitos sobre o diagn√≥stico ou a disponibilidade de tratamento. Muitos morreram e parceiros ou filhos foram afetados; o estudo durou 40 anos. | 
| **Privacidade de Dados** | 2007 - O [pr√©mio de dados da Netflix](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) forneceu a investigadores _10 milh√µes de classifica√ß√µes de filmes anonimizadas de 50 mil clientes_ para ajudar a melhorar algoritmos de recomenda√ß√£o. No entanto, os investigadores conseguiram correlacionar dados anonimizados com dados pessoalmente identific√°veis em _conjuntos de dados externos_ (por exemplo, coment√°rios no IMDb), efetivamente "desanonimizando" alguns assinantes da Netflix.|
| **Vi√©s na Coleta de Dados**  | 2013 - A cidade de Boston [desenvolveu o Street Bump](https://www.boston.gov/transportation/street-bump), uma aplica√ß√£o que permitia aos cidad√£os reportar buracos na estrada, fornecendo √† cidade melhores dados para identificar e corrigir problemas. No entanto, [pessoas de grupos de baixa renda tinham menos acesso a carros e telem√≥veis](https://hbr.org/2013/04/the-hidden-biases-in-big-data), tornando os seus problemas de estrada invis√≠veis nesta aplica√ß√£o. Os desenvolvedores trabalharam com acad√©micos para abordar quest√µes de _acesso equitativo e divis√µes digitais_ para garantir justi√ßa. |
| **Justi√ßa Algor√≠tmica**  | 2018 - O [Estudo Gender Shades do MIT](http://gendershades.org/overview.html) avaliou a precis√£o de produtos de IA para classifica√ß√£o de g√©nero, expondo lacunas na precis√£o para mulheres e pessoas de cor. Um [cart√£o de cr√©dito da Apple de 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) parecia oferecer menos cr√©dito a mulheres do que a homens. Ambos ilustraram problemas de vi√©s algor√≠tmico que levaram a danos socioecon√≥micos.|
| **Distor√ß√£o de Dados** | 2020 - O [Departamento de Sa√∫de P√∫blica da Ge√≥rgia divulgou gr√°ficos de COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) que pareciam enganar os cidad√£os sobre as tend√™ncias de casos confirmados com ordena√ß√£o n√£o cronol√≥gica no eixo x. Isto ilustra distor√ß√£o atrav√©s de truques de visualiza√ß√£o. |
| **Ilus√£o de Livre Arb√≠trio** | 2020 - A aplica√ß√£o de aprendizagem [ABCmouse pagou $10M para resolver uma queixa da FTC](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) onde os pais foram presos em subscri√ß√µes que n√£o conseguiam cancelar. Isto ilustra padr√µes obscuros em arquiteturas de escolha, onde os utilizadores foram influenciados a tomar decis√µes potencialmente prejudiciais. |
| **Privacidade de Dados & Direitos dos Utilizadores** | 2021 - A [viola√ß√£o de dados do Facebook](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) exp√¥s dados de 530 milh√µes de utilizadores, resultando num acordo de $5B com a FTC. No entanto, a empresa recusou-se a notificar os utilizadores da viola√ß√£o, violando os direitos dos utilizadores em rela√ß√£o √† transpar√™ncia e acesso aos dados. |

Quer explorar mais estudos de caso? Confira estes recursos:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - dilemas √©ticos em diversas ind√∫strias. 
* [Curso de √âtica em Ci√™ncia de Dados](https://www.coursera.org/learn/data-science-ethics#syllabus) - estudos de caso emblem√°ticos explorados.
* [Onde as coisas deram errado](https://deon.drivendata.org/examples/) - checklist de √©tica Deon com exemplos.

> üö® Pense nos estudos de caso que viu - j√° experienciou ou foi afetado por um desafio √©tico semelhante na sua vida? Consegue pensar em pelo menos um outro estudo de caso que ilustre um dos desafios √©ticos discutidos nesta sec√ß√£o?

## √âtica Aplicada

Fal√°mos sobre conceitos √©ticos, desafios e estudos de caso em contextos do mundo real. Mas como come√ßar a _aplicar_ princ√≠pios e pr√°ticas √©ticas nos nossos projetos? E como _operacionalizar_ essas pr√°ticas para uma melhor governan√ßa? Vamos explorar algumas solu√ß√µes pr√°ticas:

### 1. C√≥digos Profissionais

C√≥digos Profissionais oferecem uma op√ß√£o para as organiza√ß√µes "incentivarem" os membros a apoiar os seus princ√≠pios √©ticos e declara√ß√µes de miss√£o. Os c√≥digos s√£o _diretrizes morais_ para o comportamento profissional, ajudando os funcion√°rios ou membros a tomarem decis√µes alinhadas com os princ√≠pios da organiza√ß√£o. Eles s√≥ s√£o eficazes com a ades√£o volunt√°ria dos membros; no entanto, muitas organiza√ß√µes oferecem recompensas e penalidades adicionais para motivar a conformidade.

Exemplos incluem:

 * [C√≥digo de √âtica Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/)
 * [C√≥digo de Conduta da Data Science Association](http://datascienceassn.org/code-of-conduct.html) (criado em 2013)
 * [C√≥digo de √âtica e Conduta Profissional da ACM](https://www.acm.org/code-of-ethics) (desde 1993)

> üö® Pertence a alguma organiza√ß√£o profissional de engenharia ou ci√™ncia de dados? Explore o site para ver se definem um c√≥digo de √©tica profissional. O que isso diz sobre os seus princ√≠pios √©ticos? Como est√£o a "incentivar" os membros a seguir o c√≥digo?

### 2. Checklists de √âtica

Enquanto os c√≥digos profissionais definem o _comportamento √©tico_ exigido dos profissionais, eles [t√™m limita√ß√µes conhecidas](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) na aplica√ß√£o, particularmente em projetos de grande escala. Em vez disso, muitos especialistas em ci√™ncia de dados [defendem checklists](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), que podem **conectar princ√≠pios a pr√°ticas** de forma mais determin√≠stica e acion√°vel.

Os checklists convertem quest√µes em tarefas de "sim/n√£o" que podem ser operacionalizadas, permitindo que sejam rastreadas como parte dos fluxos de trabalho padr√£o de lan√ßamento de produtos.

Exemplos incluem:
 * [Deon](https://deon.drivendata.org/) - um checklist de √©tica em dados de uso geral criado a partir de [recomenda√ß√µes da ind√∫stria](https://deon.drivendata.org/#checklist-citations) com uma ferramenta de linha de comando para f√°cil integra√ß√£o.
 * [Checklist de Auditoria de Privacidade](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - fornece orienta√ß√µes gerais para pr√°ticas de manuseio de informa√ß√µes sob perspetivas legais e sociais.
 * [Checklist de Justi√ßa em IA](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - criado por profissionais de IA para apoiar a ado√ß√£o e integra√ß√£o de verifica√ß√µes de justi√ßa nos ciclos de desenvolvimento de IA.
 * [22 quest√µes para √©tica em dados e IA](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - uma estrutura mais aberta, estruturada para explora√ß√£o inicial de quest√µes √©ticas no design, implementa√ß√£o e contextos organizacionais.

### 3. Regulamenta√ß√µes de √âtica

A √©tica trata de definir valores compartilhados e fazer o que √© certo _voluntariamente_. **Conformidade** trata de _seguir a lei_ onde e quando definida. **Governan√ßa** abrange amplamente todas as formas como as organiza√ß√µes operam para aplicar princ√≠pios √©ticos e cumprir as leis estabelecidas.

Hoje, a governan√ßa assume duas formas dentro das organiza√ß√µes. Primeiro, trata-se de definir princ√≠pios de **IA √©tica** e estabelecer pr√°ticas para operacionalizar a ado√ß√£o em todos os projetos relacionados √† IA na organiza√ß√£o. Segundo, trata-se de cumprir todas as regulamenta√ß√µes governamentais de **prote√ß√£o de dados** para as regi√µes onde opera.

Exemplos de regulamenta√ß√µes de prote√ß√£o e privacidade de dados:

 * `1974`, [Lei de Privacidade dos EUA](https://www.justice.gov/opcl/privacy-act-1974) - regula a _coleta, uso e divulga√ß√£o_ de informa√ß√µes pessoais pelo governo federal.
 * `1996`, [Lei de Portabilidade e Responsabilidade de Seguro de Sa√∫de dos EUA (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - protege dados de sa√∫de pessoais.
 * `1998`, [Lei de Prote√ß√£o √† Privacidade Online das Crian√ßas dos EUA (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - protege a privacidade de dados de crian√ßas menores de 13 anos.
 * `2018`, [Regulamento Geral de Prote√ß√£o de Dados (GDPR)](https://gdpr-info.eu/) - fornece direitos aos utilizadores, prote√ß√£o de dados e privacidade.
 * `2018`, [Lei de Privacidade do Consumidor da Calif√≥rnia (CCPA)](https://www.oag.ca.gov/privacy/ccpa) - d√° aos consumidores mais _direitos_ sobre os seus dados pessoais.
 * `2021`, [Lei de Prote√ß√£o de Informa√ß√µes Pessoais da China](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) - uma das regulamenta√ß√µes de privacidade de dados online mais rigorosas do mundo.

> üö® O Regulamento Geral de Prote√ß√£o de Dados (GDPR) definido pela Uni√£o Europeia continua a ser uma das regulamenta√ß√µes de privacidade de dados mais influentes atualmente. Sabia que tamb√©m define [8 direitos dos utilizadores](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) para proteger a privacidade digital e os dados pessoais dos cidad√£os? Descubra quais s√£o esses direitos e por que s√£o importantes.

### 4. Cultura de √âtica

Note que ainda existe uma lacuna intang√≠vel entre _conformidade_ (fazer o suficiente para cumprir "a letra da lei") e abordar [quest√µes sist√©micas](https://www.coursera.org/learn/data-science-ethics/home/week/4) (como ossifica√ß√£o, assimetria de informa√ß√£o e injusti√ßa distributiva) que podem acelerar a instrumentaliza√ß√£o da IA.

O √∫ltimo requer [abordagens colaborativas para definir culturas de √©tica](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) que construam conex√µes emocionais e valores compartilhados consistentes _entre organiza√ß√µes_ na ind√∫stria. Isso exige mais [culturas de √©tica de dados formalizadas](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) nas organiza√ß√µes - permitindo que _qualquer pessoa_ [puxe o cord√£o Andon](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (para levantar preocupa√ß√µes √©ticas cedo no processo) e tornando _avalia√ß√µes √©ticas_ (por exemplo, em contrata√ß√µes) um crit√©rio central na forma√ß√£o de equipas em projetos de IA.

---
## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## Revis√£o & Autoestudo 

Cursos e livros ajudam a compreender conceitos e desafios √©ticos fundamentais, enquanto estudos de caso e ferramentas ajudam a aplicar pr√°ticas √©ticas em contextos do mundo real. Aqui est√£o alguns recursos para come√ßar:

* [Machine Learning For Beginners](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - li√ß√£o sobre Justi√ßa, da Microsoft.
* [Princ√≠pios de IA Respons√°vel](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - percurso de aprendizagem gratuito da Microsoft Learn.
* [√âtica e Ci√™ncia de Dados](https://resources.oreilly.com/examples/0636920203964) - EBook da O'Reilly (M. Loukides, H. Mason et. al)
* [√âtica na Ci√™ncia de Dados](https://www.coursera.org/learn/data-science-ethics#syllabus) - curso online da Universidade de Michigan.
* [√âtica Desvendada](https://ethicsunwrapped.utexas.edu/case-studies) - estudos de caso da Universidade do Texas.

# Tarefa 

[Escreva Um Estudo de Caso Sobre √âtica de Dados](assignment.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original no seu idioma nativo deve ser considerado a fonte oficial. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas resultantes do uso desta tradu√ß√£o.