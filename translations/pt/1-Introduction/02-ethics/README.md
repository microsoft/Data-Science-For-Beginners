<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58860ce9a4b8a564003d2752f7c72851",
  "translation_date": "2025-10-03T16:24:25+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "pt"
}
-->
# Introdu√ß√£o √† √âtica de Dados

|![ Sketchnote por [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| √âtica em Ci√™ncia de Dados - _Sketchnote por [@nitya](https://twitter.com/nitya)_ |

---

Somos todos cidad√£os de dados a viver num mundo dataficado.

As tend√™ncias de mercado indicam que, at√© 2022, 1 em cada 3 grandes organiza√ß√µes comprar√° e vender√° os seus dados atrav√©s de [Mercados e Bolsas](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) online. Como **Desenvolvedores de Aplica√ß√µes**, ser√° mais f√°cil e barato integrar insights baseados em dados e automa√ß√£o orientada por algoritmos nas experi√™ncias di√°rias dos utilizadores. Mas, √† medida que a IA se torna omnipresente, tamb√©m ser√° necess√°rio compreender os potenciais danos causados pela [armamentiza√ß√£o](https://www.youtube.com/watch?v=TQHs8SA1qpk) desses algoritmos em larga escala.

As tend√™ncias sugerem que, at√© 2025, iremos gerar e consumir mais de [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) de dados. Para **Cientistas de Dados**, esta explos√£o de informa√ß√£o oferece acesso sem precedentes a dados pessoais e comportamentais. Com isso, surge o poder de construir perfis detalhados de utilizadores e influenciar subtilmente a tomada de decis√µes‚Äîfrequentemente de formas que promovem uma [ilus√£o de escolha livre](https://www.datasciencecentral.com/the-pareto-set-and-the-paradox-of-choice/). Embora isso possa ser usado para orientar os utilizadores em dire√ß√£o a resultados preferidos, tamb√©m levanta quest√µes cr√≠ticas sobre privacidade de dados, autonomia e os limites √©ticos da influ√™ncia algor√≠tmica.

A √©tica de dados √© agora _uma barreira necess√°ria_ para a ci√™ncia e engenharia de dados, ajudando-nos a minimizar potenciais danos e consequ√™ncias n√£o intencionais das nossas a√ß√µes orientadas por dados. O [Ciclo de Hype da Gartner para IA](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifica tend√™ncias relevantes em √©tica digital, IA respons√°vel e governan√ßa de IA como motores-chave para megatend√™ncias maiores em torno da _democratiza√ß√£o_ e _industrializa√ß√£o_ da IA.

![Ciclo de Hype da Gartner para IA - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

Nesta li√ß√£o, exploraremos a √°rea fascinante da √©tica de dados - desde conceitos e desafios fundamentais at√© estudos de caso e conceitos aplicados de IA, como governan√ßa - que ajudam a estabelecer uma cultura de √©tica em equipas e organiza√ß√µes que trabalham com dados e IA.

## [Question√°rio pr√©-aula](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Defini√ß√µes B√°sicas

Vamos come√ßar por compreender a terminologia b√°sica.

A palavra "√©tica" vem da [palavra grega "ethikos"](https://en.wikipedia.org/wiki/Ethics) (e sua raiz "ethos"), que significa _car√°ter ou natureza moral_.

**√âtica** refere-se aos valores partilhados e princ√≠pios morais que governam o nosso comportamento na sociedade. A √©tica n√£o se baseia em leis, mas em normas amplamente aceites sobre o que √© "certo vs. errado". No entanto, considera√ß√µes √©ticas podem influenciar iniciativas de governan√ßa corporativa e regulamenta√ß√µes governamentais que criam mais incentivos para conformidade.

**√âtica de Dados** √© um [novo ramo da √©tica](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) que "estuda e avalia problemas morais relacionados a _dados, algoritmos e pr√°ticas correspondentes_". Aqui, **"dados"** foca-se em a√ß√µes relacionadas √† gera√ß√£o, grava√ß√£o, curadoria, processamento, dissemina√ß√£o, partilha e uso; **"algoritmos"** foca-se em IA, agentes, aprendizagem autom√°tica e rob√¥s; e **"pr√°ticas"** foca-se em t√≥picos como inova√ß√£o respons√°vel, programa√ß√£o, hacking e c√≥digos de √©tica.

**√âtica Aplicada** √© a [aplica√ß√£o pr√°tica de considera√ß√µes morais](https://en.wikipedia.org/wiki/Applied_ethics). Trata-se do processo de investigar ativamente quest√µes √©ticas no contexto de _a√ß√µes, produtos e processos do mundo real_, e tomar medidas corretivas para garantir que permane√ßam alinhados com os nossos valores √©ticos definidos.

**Cultura de √âtica** refere-se a [_operacionalizar_ a √©tica aplicada](https://hbr.org/2019/05/how-to-design-an-ethical-organization) para garantir que os nossos princ√≠pios e pr√°ticas √©ticos sejam adotados de forma consistente e escal√°vel em toda a organiza√ß√£o. Culturas de √©tica bem-sucedidas definem princ√≠pios √©ticos em toda a organiza√ß√£o, oferecem incentivos significativos para conformidade e refor√ßam normas √©ticas ao encorajar e amplificar comportamentos desejados em todos os n√≠veis da organiza√ß√£o.

## Conceitos de √âtica

Nesta sec√ß√£o, discutiremos conceitos como **valores partilhados** (princ√≠pios) e **desafios √©ticos** (problemas) para √©tica de dados - e exploraremos **estudos de caso** que ajudam a compreender esses conceitos em contextos do mundo real.

### 1. Princ√≠pios √âticos

Toda estrat√©gia de √©tica de dados come√ßa por definir _princ√≠pios √©ticos_ - os "valores partilhados" que descrevem comportamentos aceit√°veis e orientam a√ß√µes conformes nos nossos projetos de dados e IA. Pode-se defini-los a n√≠vel individual ou de equipa. No entanto, a maioria das grandes organiza√ß√µes delineia-os numa declara√ß√£o de miss√£o ou estrutura de _IA √©tica_ definida a n√≠vel corporativo e aplicada de forma consistente em todas as equipas.

**Exemplo:** A declara√ß√£o de miss√£o de [IA Respons√°vel](https://www.microsoft.com/en-us/ai/responsible-ai) da Microsoft afirma: _"Estamos comprometidos com o avan√ßo da IA orientada por princ√≠pios √©ticos que colocam as pessoas em primeiro lugar"_ - identificando 6 princ√≠pios √©ticos na estrutura abaixo:

![IA Respons√°vel na Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Vamos explorar brevemente esses princ√≠pios. _Transpar√™ncia_ e _responsabilidade_ s√£o valores fundamentais sobre os quais outros princ√≠pios se constroem - ent√£o vamos come√ßar por a√≠:

* [**Responsabilidade**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) torna os profissionais _respons√°veis_ pelas suas opera√ß√µes de dados e IA, e pela conformidade com esses princ√≠pios √©ticos.
* [**Transpar√™ncia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) garante que as a√ß√µes de dados e IA sejam _compreens√≠veis_ (interpret√°veis) para os utilizadores, explicando o qu√™ e o porqu√™ por tr√°s das decis√µes.
* [**Justi√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - foca-se em garantir que a IA trate _todas as pessoas_ de forma justa, abordando quaisquer preconceitos socio-t√©cnicos sist√©micos ou impl√≠citos nos dados e sistemas.
* [**Fiabilidade e Seguran√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - garante que a IA se comporte de forma _consistente_ com os valores definidos, minimizando potenciais danos ou consequ√™ncias n√£o intencionais.
* [**Privacidade e Seguran√ßa**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de compreender a linhagem dos dados e fornecer _privacidade de dados e prote√ß√µes relacionadas_ aos utilizadores.
* [**Inclusividade**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata de projetar solu√ß√µes de IA com inten√ß√£o, adaptando-as para atender a uma _ampla gama de necessidades e capacidades humanas_.

> üö® Pense no que poderia ser a sua declara√ß√£o de miss√£o de √©tica de dados. Explore estruturas de IA √©tica de outras organiza√ß√µes - aqui est√£o exemplos da [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) e [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Que valores partilhados t√™m em comum? Como esses princ√≠pios se relacionam com o produto ou ind√∫stria de IA em que operam?

### 2. Desafios √âticos

Depois de definir os princ√≠pios √©ticos, o pr√≥ximo passo √© avaliar as nossas a√ß√µes de dados e IA para ver se est√£o alinhadas com esses valores partilhados. Pense nas suas a√ß√µes em duas categorias: _coleta de dados_ e _design de algoritmos_.

Na coleta de dados, as a√ß√µes provavelmente envolver√£o **dados pessoais** ou informa√ß√µes pessoalmente identific√°veis (PII) de indiv√≠duos identific√°veis. Isso inclui [diversos itens de dados n√£o pessoais](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) que, _coletivamente_, identificam um indiv√≠duo. Os desafios √©ticos podem estar relacionados √† _privacidade de dados_, _propriedade de dados_ e t√≥picos relacionados, como _consentimento informado_ e _direitos de propriedade intelectual_ dos utilizadores.

No design de algoritmos, as a√ß√µes envolver√£o a coleta e curadoria de **conjuntos de dados**, e o uso desses dados para treinar e implementar **modelos de dados** que preveem resultados ou automatizam decis√µes em contextos do mundo real. Os desafios √©ticos podem surgir de _vi√©s nos conjuntos de dados_, problemas de _qualidade dos dados_, _injusti√ßa_ e _m√° representa√ß√£o_ nos algoritmos - incluindo alguns problemas que s√£o sist√©micos por natureza.

Em ambos os casos, os desafios √©ticos destacam √°reas onde as nossas a√ß√µes podem entrar em conflito com os nossos valores partilhados. Para detetar, mitigar, minimizar ou eliminar essas preocupa√ß√µes, precisamos fazer perguntas morais de "sim/n√£o" relacionadas √†s nossas a√ß√µes e tomar medidas corretivas conforme necess√°rio. Vamos analisar alguns desafios √©ticos e as quest√µes morais que eles levantam:

#### 2.1 Propriedade de Dados

A coleta de dados frequentemente envolve dados pessoais que podem identificar os sujeitos dos dados. [Propriedade de dados](https://permission.io/blog/data-ownership) trata do _controlo_ e [_direitos dos utilizadores_](https://permission.io/blog/data-ownership) relacionados √† cria√ß√£o, processamento e dissemina√ß√£o de dados.

As quest√µes morais que precisamos perguntar s√£o:
 * Quem √© o propriet√°rio dos dados? (utilizador ou organiza√ß√£o)
 * Que direitos t√™m os sujeitos dos dados? (ex: acesso, elimina√ß√£o, portabilidade)
 * Que direitos t√™m as organiza√ß√µes? (ex: retificar avalia√ß√µes maliciosas de utilizadores)

#### 2.2 Consentimento Informado

[Consentimento informado](https://legaldictionary.net/informed-consent/) define o ato de os utilizadores concordarem com uma a√ß√£o (como coleta de dados) com um _entendimento completo_ dos factos relevantes, incluindo o prop√≥sito, os riscos potenciais e as alternativas.

Quest√µes a explorar aqui s√£o:
 * O utilizador (sujeito dos dados) deu permiss√£o para a captura e uso dos dados?
 * O utilizador compreendeu o prop√≥sito para o qual os dados foram capturados?
 * O utilizador compreendeu os riscos potenciais da sua participa√ß√£o?

#### 2.3 Propriedade Intelectual

[Propriedade intelectual](https://en.wikipedia.org/wiki/Intellectual_property) refere-se a cria√ß√µes intang√≠veis resultantes da iniciativa humana, que podem _ter valor econ√≥mico_ para indiv√≠duos ou empresas.

Quest√µes a explorar aqui s√£o:
 * Os dados coletados t√™m valor econ√≥mico para um utilizador ou empresa?
 * O **utilizador** tem propriedade intelectual aqui?
 * A **organiza√ß√£o** tem propriedade intelectual aqui?
 * Se esses direitos existem, como estamos a proteg√™-los?

#### 2.4 Privacidade de Dados

[Privacidade de dados](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) ou privacidade de informa√ß√µes refere-se √† preserva√ß√£o da privacidade do utilizador e prote√ß√£o da identidade do utilizador em rela√ß√£o a informa√ß√µes pessoalmente identific√°veis.

Quest√µes a explorar aqui s√£o:
 * Os dados (pessoais) dos utilizadores est√£o protegidos contra ataques e vazamentos?
 * Os dados dos utilizadores est√£o acess√≠veis apenas a utilizadores e contextos autorizados?
 * A anonimidade dos utilizadores √© preservada quando os dados s√£o partilhados ou disseminados?
 * Um utilizador pode ser desidentificado de conjuntos de dados anonimizados?

#### 2.5 Direito ao Esquecimento

O [Direito ao Esquecimento](https://en.wikipedia.org/wiki/Right_to_be_forgotten) ou [Direito √† Elimina√ß√£o](https://www.gdpreu.org/right-to-be-forgotten/) oferece prote√ß√£o adicional de dados pessoais aos utilizadores. Especificamente, d√° aos utilizadores o direito de solicitar a elimina√ß√£o ou remo√ß√£o de dados pessoais de pesquisas na Internet e outros locais, _sob circunst√¢ncias espec√≠ficas_ - permitindo-lhes um novo come√ßo online sem que a√ß√µes passadas sejam usadas contra eles.

Quest√µes a explorar aqui s√£o:
 * O sistema permite que os sujeitos dos dados solicitem a elimina√ß√£o?
 * A retirada do consentimento do utilizador deve acionar a elimina√ß√£o autom√°tica?
 * Os dados foram coletados sem consentimento ou por meios ilegais?
 * Estamos em conformidade com as regulamenta√ß√µes governamentais de privacidade de dados?

#### 2.6 Vi√©s nos Conjuntos de Dados

Vi√©s nos conjuntos de dados ou [Vi√©s de Coleta](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) refere-se √† sele√ß√£o de um subconjunto _n√£o representativo_ de dados para o desenvolvimento de algoritmos, criando potencial injusti√ßa nos resultados para grupos diversos. Tipos de vi√©s incluem vi√©s de sele√ß√£o ou amostragem, vi√©s de voluntariado e vi√©s de instrumento.

Quest√µes a explorar aqui s√£o:
 * Recrut√°mos um conjunto representativo de sujeitos dos dados?
 * Test√°mos o nosso conjunto de dados coletado ou curado para v√°rios tipos de vi√©s?
 * Podemos mitigar ou remover quaisquer vieses descobertos?

#### 2.7 Qualidade dos Dados

[Qualidade dos Dados](https://lakefs.io/data-quality-testing/) analisa a validade do conjunto de dados curado usado para desenvolver os nossos algoritmos, verificando se as caracter√≠sticas e os registos atendem aos requisitos para o n√≠vel de precis√£o e consist√™ncia necess√°rio para o nosso prop√≥sito de IA.

Quest√µes a explorar aqui s√£o:
 * Captur√°mos caracter√≠sticas v√°lidas para o nosso caso de uso?
 * Os dados foram capturados de forma _consistente_ em diversas fontes de dados?
 * O conjunto de dados est√° _completo_ para condi√ß√µes ou cen√°rios diversos?
* A informa√ß√£o foi capturada _com precis√£o_ ao refletir a realidade?

#### 2.8 Justi√ßa dos Algoritmos

[A Justi√ßa dos Algoritmos](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) verifica se o design do algoritmo discrimina sistematicamente subgrupos espec√≠ficos de sujeitos de dados, levando a [potenciais danos](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) na _distribui√ß√£o_ (onde recursos s√£o negados ou retidos desse grupo) e na _qualidade do servi√ßo_ (onde a IA n√£o √© t√£o precisa para alguns subgrupos como √© para outros).

Perguntas a explorar aqui incluem:
* Avali√°mos a precis√£o do modelo para diversos subgrupos e condi√ß√µes?
* Examin√°mos o sistema para identificar potenciais danos (por exemplo, estere√≥tipos)?
* Podemos rever os dados ou re-treinar os modelos para mitigar os danos identificados?

Explore recursos como [checklists de Justi√ßa na IA](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) para saber mais.

#### 2.9 Deturpa√ß√£o

[A Deturpa√ß√£o de Dados](https://www.sciencedirect.com/topics/computer-science/misrepresentation) trata de questionar se estamos a comunicar insights de dados relatados honestamente de forma enganosa para apoiar uma narrativa desejada.

Perguntas a explorar aqui incluem:
* Estamos a relatar dados incompletos ou imprecisos?
* Estamos a visualizar dados de forma a induzir conclus√µes enganosas?
* Estamos a usar t√©cnicas estat√≠sticas seletivas para manipular resultados?
* Existem explica√ß√µes alternativas que possam oferecer uma conclus√£o diferente?

#### 2.10 Livre Escolha

A [Ilus√£o de Livre Escolha](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) ocorre quando "arquiteturas de escolha" do sistema utilizam algoritmos de tomada de decis√£o para influenciar as pessoas a tomarem um resultado preferido, enquanto aparentam dar-lhes op√ß√µes e controlo. Estes [padr√µes obscuros](https://www.darkpatterns.org/) podem causar danos sociais e econ√≥micos aos utilizadores. Como as decis√µes dos utilizadores impactam os perfis de comportamento, estas a√ß√µes podem potencialmente impulsionar escolhas futuras que amplificam ou prolongam o impacto desses danos.

Perguntas a explorar aqui incluem:
* O utilizador compreendeu as implica√ß√µes de fazer essa escolha?
* O utilizador estava ciente de (alternativas) escolhas e dos pr√≥s e contras de cada uma?
* O utilizador pode reverter uma escolha automatizada ou influenciada posteriormente?

### 3. Estudos de Caso

Para colocar estes desafios √©ticos em contextos do mundo real, √© √∫til analisar estudos de caso que destacam os potenciais danos e consequ√™ncias para indiv√≠duos e para a sociedade, quando estas viola√ß√µes √©ticas s√£o ignoradas.

Aqui est√£o alguns exemplos:

| Desafio √âtico | Estudo de Caso | 
|--- |--- |
| **Consentimento Informado** | 1972 - [Estudo de S√≠filis de Tuskegee](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Homens afro-americanos que participaram no estudo foram prometidos cuidados m√©dicos gratuitos _mas enganados_ por investigadores que n√£o informaram os sujeitos sobre o seu diagn√≥stico ou sobre a disponibilidade de tratamento. Muitos sujeitos morreram e parceiros ou filhos foram afetados; o estudo durou 40 anos. | 
| **Privacidade de Dados** | 2007 - O [pr√©mio de dados da Netflix](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) forneceu aos investigadores _10M classifica√ß√µes de filmes anonimizadas de 50K clientes_ para ajudar a melhorar os algoritmos de recomenda√ß√£o. No entanto, os investigadores conseguiram correlacionar dados anonimizados com dados pessoalmente identific√°veis em _conjuntos de dados externos_ (por exemplo, coment√°rios no IMDb) - efetivamente "desanonimizando" alguns assinantes da Netflix. |
| **Vi√©s na Coleta de Dados** | 2013 - A cidade de Boston [desenvolveu Street Bump](https://www.boston.gov/transportation/street-bump), uma aplica√ß√£o que permitia aos cidad√£os reportar buracos na estrada, dando √† cidade melhores dados sobre vias para encontrar e resolver problemas. No entanto, [pessoas em grupos de rendimentos mais baixos tinham menos acesso a carros e telem√≥veis](https://hbr.org/2013/04/the-hidden-biases-in-big-data), tornando os seus problemas de vias invis√≠veis nesta aplica√ß√£o. Os desenvolvedores trabalharam com acad√©micos para abordar quest√µes de _acesso equitativo e divis√µes digitais_ para garantir justi√ßa. |
| **Justi√ßa Algor√≠tmica** | 2018 - O MIT [Estudo Gender Shades](http://gendershades.org/overview.html) avaliou a precis√£o de produtos de IA de classifica√ß√£o de g√©nero, expondo lacunas na precis√£o para mulheres e pessoas de cor. Um [Cart√£o Apple de 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) parecia oferecer menos cr√©dito √†s mulheres do que aos homens. Ambos ilustraram problemas de vi√©s algor√≠tmico que levam a danos socioecon√≥micos. |
| **Deturpa√ß√£o de Dados** | 2020 - O [Departamento de Sa√∫de P√∫blica da Ge√≥rgia divulgou gr√°ficos de COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) que pareciam enganar os cidad√£os sobre tend√™ncias de casos confirmados com ordena√ß√£o n√£o cronol√≥gica no eixo x. Isto ilustra deturpa√ß√£o atrav√©s de truques de visualiza√ß√£o. |
| **Ilus√£o de Livre Escolha** | 2020 - A aplica√ß√£o de aprendizagem [ABCmouse pagou $10M para resolver uma queixa da FTC](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) onde os pais ficaram presos a pagar por subscri√ß√µes que n√£o podiam cancelar. Isto ilustra padr√µes obscuros em arquiteturas de escolha, onde os utilizadores foram influenciados a tomar escolhas potencialmente prejudiciais. |
| **Privacidade de Dados e Direitos dos Utilizadores** | 2021 - A [Viola√ß√£o de Dados do Facebook](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) exp√¥s dados de 530M utilizadores, resultando num acordo de $5B com a FTC. No entanto, recusou-se a notificar os utilizadores sobre a viola√ß√£o, violando os direitos dos utilizadores em rela√ß√£o √† transpar√™ncia e ao acesso aos dados. |

Quer explorar mais estudos de caso? Veja estes recursos:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - dilemas √©ticos em diversas ind√∫strias. 
* [Curso de √âtica em Ci√™ncia de Dados](https://www.coursera.org/learn/data-science-ethics#syllabus) - estudos de caso marcantes explorados.
* [Onde as coisas correram mal](https://deon.drivendata.org/examples/) - checklist de Deon com exemplos.

> üö® Pense nos estudos de caso que viu - j√° experienciou ou foi afetado por um desafio √©tico semelhante na sua vida? Consegue pensar em pelo menos um outro estudo de caso que ilustre um dos desafios √©ticos discutidos nesta sec√ß√£o?

## √âtica Aplicada

Fal√°mos sobre conceitos √©ticos, desafios e estudos de caso em contextos do mundo real. Mas como come√ßamos a _aplicar_ princ√≠pios e pr√°ticas √©ticas nos nossos projetos? E como _operacionalizamos_ estas pr√°ticas para uma melhor governan√ßa? Vamos explorar algumas solu√ß√µes do mundo real:

### 1. C√≥digos Profissionais

Os C√≥digos Profissionais oferecem uma op√ß√£o para as organiza√ß√µes "incentivarem" os membros a apoiar os seus princ√≠pios √©ticos e declara√ß√£o de miss√£o. Os c√≥digos s√£o _diretrizes morais_ para o comportamento profissional, ajudando os funcion√°rios ou membros a tomar decis√µes que estejam alinhadas com os princ√≠pios da organiza√ß√£o. Eles s√£o t√£o eficazes quanto a conformidade volunt√°ria dos membros; no entanto, muitas organiza√ß√µes oferecem recompensas e penalidades adicionais para motivar a conformidade dos membros.

Exemplos incluem:

* [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) C√≥digo de √âtica
* [Data Science Association](http://datascienceassn.org/code-of-conduct.html) C√≥digo de Conduta (criado em 2013)
* [ACM C√≥digo de √âtica e Conduta Profissional](https://www.acm.org/code-of-ethics) (desde 1993)

> üö® Pertence a uma organiza√ß√£o profissional de engenharia ou ci√™ncia de dados? Explore o site para ver se definem um c√≥digo profissional de √©tica. O que isso diz sobre os seus princ√≠pios √©ticos? Como est√£o a "incentivar" os membros a seguir o c√≥digo?

### 2. Checklists de √âtica

Enquanto os c√≥digos profissionais definem o comportamento √©tico _necess√°rio_ dos profissionais, eles [t√™m limita√ß√µes conhecidas](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) na aplica√ß√£o, particularmente em projetos de grande escala. Em vez disso, muitos especialistas em ci√™ncia de dados [defendem checklists](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), que podem **conectar princ√≠pios a pr√°ticas** de forma mais determin√≠stica e acion√°vel.

Os checklists convertem perguntas em tarefas "sim/n√£o" que podem ser operacionalizadas, permitindo que sejam rastreadas como parte dos fluxos de trabalho padr√£o de lan√ßamento de produtos.

Exemplos incluem:
* [Deon](https://deon.drivendata.org/) - um checklist de √©tica em dados de uso geral criado a partir de [recomenda√ß√µes da ind√∫stria](https://deon.drivendata.org/#checklist-citations) com uma ferramenta de linha de comando para f√°cil integra√ß√£o.
* [Checklist de Auditoria de Privacidade](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - fornece orienta√ß√µes gerais para pr√°ticas de manuseio de informa√ß√µes sob perspetivas legais e sociais.
* [Checklist de Justi√ßa na IA](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - criado por profissionais de IA para apoiar a ado√ß√£o e integra√ß√£o de verifica√ß√µes de justi√ßa nos ciclos de desenvolvimento de IA.
* [22 perguntas para √©tica em dados e IA](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - estrutura mais aberta, organizada para explora√ß√£o inicial de quest√µes √©ticas no design, implementa√ß√£o e contextos organizacionais.

### 3. Regulamenta√ß√µes √âticas

A √©tica trata de definir valores compartilhados e fazer o que √© certo _voluntariamente_. **Conformidade** trata de _seguir a lei_ onde e quando definida. **Governan√ßa** cobre amplamente todas as formas como as organiza√ß√µes operam para aplicar princ√≠pios √©ticos e cumprir leis estabelecidas.

Hoje, a governan√ßa assume duas formas dentro das organiza√ß√µes. Primeiro, trata-se de definir princ√≠pios de **IA √©tica** e estabelecer pr√°ticas para operacionalizar a ado√ß√£o em todos os projetos relacionados √† IA na organiza√ß√£o. Segundo, trata-se de cumprir todas as regulamenta√ß√µes de **prote√ß√£o de dados** mandatadas pelo governo nas regi√µes onde opera.

Exemplos de regulamenta√ß√µes de prote√ß√£o de dados e privacidade:

* `1974`, [Lei de Privacidade dos EUA](https://www.justice.gov/opcl/privacy-act-1974) - regula a coleta, uso e divulga√ß√£o de informa√ß√µes pessoais pelo _governo federal_.
* `1996`, [Lei de Portabilidade e Responsabilidade de Seguro de Sa√∫de dos EUA (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - protege dados de sa√∫de pessoais.
* `1998`, [Lei de Prote√ß√£o √† Privacidade Online das Crian√ßas dos EUA (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - protege a privacidade de dados de crian√ßas menores de 13 anos.
* `2018`, [Regulamento Geral de Prote√ß√£o de Dados (GDPR)](https://gdpr-info.eu/) - fornece direitos dos utilizadores, prote√ß√£o de dados e privacidade.
* `2018`, [Lei de Privacidade do Consumidor da Calif√≥rnia (CCPA)](https://www.oag.ca.gov/privacy/ccpa) - d√° aos consumidores mais _direitos_ sobre os seus dados pessoais.
* `2021`, [Lei de Prote√ß√£o de Informa√ß√µes Pessoais da China](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) - recentemente aprovada, criando uma das regulamenta√ß√µes de privacidade de dados online mais fortes do mundo.

> üö® A Uni√£o Europeia definiu o GDPR (Regulamento Geral de Prote√ß√£o de Dados), que continua a ser uma das regulamenta√ß√µes de privacidade de dados mais influentes atualmente. Sabia que tamb√©m define [8 direitos dos utilizadores](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) para proteger a privacidade digital e os dados pessoais dos cidad√£os? Descubra quais s√£o e por que s√£o importantes.

### 4. Cultura √âtica

Note que permanece uma lacuna intang√≠vel entre _conformidade_ (fazer o suficiente para cumprir "a letra da lei") e abordar [quest√µes sist√©micas](https://www.coursera.org/learn/data-science-ethics/home/week/4) (como ossifica√ß√£o, assimetria de informa√ß√£o e injusti√ßa distributiva) que podem acelerar a arma√ß√£o da IA.

A √∫ltima exige [abordagens colaborativas para definir culturas √©ticas](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) que construam conex√µes emocionais e valores compartilhados consistentes _entre organiza√ß√µes_ na ind√∫stria. Isto requer mais [culturas √©ticas formalizadas de dados](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) nas organiza√ß√µes - permitindo que _qualquer pessoa_ [puxe o cord√£o Andon](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (para levantar preocupa√ß√µes √©ticas cedo no processo) e tornando as _avalia√ß√µes √©ticas_ (por exemplo, na contrata√ß√£o) um crit√©rio central na forma√ß√£o de equipas em projetos de IA.

---
## [Question√°rio p√≥s-aula](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## Revis√£o e Autoestudo

Cursos e livros ajudam a compreender os conceitos e desafios √©ticos fundamentais, enquanto estudos de caso e ferramentas ajudam com pr√°ticas √©ticas aplicadas em contextos do mundo real. Aqui est√£o alguns recursos para come√ßar.
* [Machine Learning Para Principiantes](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - aula sobre Justi√ßa, da Microsoft.  
* [Princ√≠pios de IA Respons√°vel](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - percurso de aprendizagem gratuito no Microsoft Learn.  
* [√âtica e Ci√™ncia de Dados](https://resources.oreilly.com/examples/0636920203964) - EBook da O'Reilly (M. Loukides, H. Mason et. al)  
* [√âtica na Ci√™ncia de Dados](https://www.coursera.org/learn/data-science-ethics#syllabus) - curso online da Universidade de Michigan.  
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - estudos de caso da Universidade do Texas.  

# Tarefa  

[Escrever Um Estudo de Caso Sobre √âtica de Dados](assignment.md)  

---

**Aviso**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precis√£o, √© importante notar que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes da utiliza√ß√£o desta tradu√ß√£o.